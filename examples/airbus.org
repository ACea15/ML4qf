* House keeping
#+begin_src elisp :results none
(add-to-list 'org-structure-template-alist
'("sp" . "src python :session py1"))
(add-to-list 'org-structure-template-alist
'("se" . "src elisp"))

(setq org-confirm-babel-evaluate nil)
#+end_src

#+begin_src emacs-lisp  :session py1 :results none
(pyvenv-workon "qfpy")
(require 'org-tempo)
#+end_src

#+begin_src python  :session py1 :results none
  import sys
  file_path = sys.path[1]
  sys.path.append(file_path + "/../")
#+end_src

* Import libraries
#+BEGIN_SRC python :session py1 :results output silent

  import numpy as np
  import yfinance as yf
  import ml4qf
  # from umap import UMAP
  # import matplotlib.pyplot as plt
  # from mpl_toolkits.mplot3d import Axes3D
  # import matplotlib
  # matplotlib.rcParams['figure.dpi'] = 80

  from minisom import MiniSom
  from sklearn.model_selection import train_test_split
  from sklearn.metrics import classification_report

  import plotly.express as px
  import scipy.stats
  import pandas as pd
  import pandas_ta as ta
  import ml4qf.inputs
  import  ml4qf.collectors.financial_features
#+END_SRC

* Introduction
#+begin_src python :session py1 :results none 
df = yf.download("EADSY", start="2014-10-02", end="2019-10-1", interval='1d')
#df.index
#+end_src


* Feature engineering

#+begin_src python :session py1
FEATURES1 = {'momentum_': [1, 2, 5, 8, 15, 23],
            'OC_': None,
            'HL_': None,
            'Ret_': [1,2],
            'RetLog_': list(range(10, 90, 10)),
            'Std_': list(range(10, 90, 10)),
            'MA_': [5, 10, 25, 50],
            'EMA_': [5, 10, 25, 50],
            'sign_return_': [1, 2, 5, 8, 15, 23],
            'volume_':None,
            "percent_return": dict(length=2, append=False),
            "log_return": [1, 2]
            }

data = ml4qf.collectors.financial_features.FinancialData("EADSY", 2019, 10, 1, 365*5, FEATURES1)

#+end_src

#+RESULTS:


** Exploratory data analysis


#+begin_src python :session py1 :results file
fig1_path= './img/stock_Close.png'
fig1 = px.line(df_, y=['momentum_1d', 'momentum_5d', 'momentum_15d'])
fig1.write_image(fig1_path)
fig1_path
#+end_src

#+RESULTS:
[[file:]]


** Feature scaling

#+begin_src python :session py1
np.abs(scipy.stats.zscore(df_)).max()
#+end_src

#+RESULTS:
#+begin_example
momentum_1d         7.405875
momentum_2d         6.255884
momentum_5d         5.205477
momentum_8d         4.774641
momentum_15d        3.323660
momentum_23d        3.477268
OC_                 5.166176
HL_                14.712396
Ret_1d              6.518964
Ret_2d              5.631053
RetLog_10d          3.719721
RetLog_20d          3.213870
RetLog_30d          3.490527
RetLog_40d          3.254908
RetLog_50d          3.165038
RetLog_60d          3.294189
RetLog_70d          3.076192
RetLog_80d          3.314533
Std_10d             5.232700
Std_20d             4.027321
Std_30d             3.280614
Std_40d             2.726653
Std_50d             2.411506
Std_60d             2.149318
Std_70d             2.178102
Std_80d             2.123903
MA_5d               2.106170
MA_10d              2.084862
MA_25d              2.001722
MA_50d              1.974645
EMA_5d              2.046045
EMA_10d             2.013283
EMA_25d             1.959804
EMA_50d             1.954027
sign_return_1d      1.760979
sign_return_2d      1.819713
sign_return_5d      1.785669
sign_return_8d      1.721326
sign_return_15d     1.694612
sign_return_23d     1.709776
volume_            12.346371
PCTRET_2            5.631053
LOGRET_1            6.888361
LOGRET_2            6.028731
target              1.000850
dtype: float64
#+end_example


#+begin_src python :session py1
  import sklearn.preprocessing
  class Transformer:

      def __init__(self,  transformer_name, transformer_settings={}):


          self.transformer_name = transformer_name
          self.transformer_settings = transformer_settings
          self.transformer = None
          self.set_transformer()

      def set_transformer(self):

          self.transformer_type = getattr(sklearn.preprocessing, self.transformer_name)
          self.transformer = self.transformer_type(**self.transformer_settings)


  def build_transformation(df, transformers):

      column_transformers = []
      for k, v in transformers.items():
          name = k.replace('Scaler', '')
          if 'settings' in v.keys():
              transformer_ins = Transformer(k, v['settings'])
          else:
              transformer_ins = Transformer(k)
          transformer = transformer_ins.transformer
          features_list = []
          for fi in v['features']:
              for ci in df.columns:
                  if fi in ci[:len(fi)]:
                      features_list.append(ci)
          column_transformers.append((name, transformer, features_list))
      return column_transformers

#+end_src

#+RESULTS:

#+begin_src python :session py1

  import sklearn.compose

  transformers = {'MinMaxScaler': {'features': ['momentum']},
                  'StandardScaler': {'features': ['EMA', 'MA', 'Std']},
                  'RobustScaler': {'features': ['volume', 'HL']}
                  }

  columns = build_transformation(df_, transformers)
  ct = sklearn.compose.ColumnTransformer(columns, remainder='passthrough')
  ct.fit(df_)
  df_2 = ct.transform(df_)
#+end_src

#+RESULTS:

** Label 

*** Class imbalance

#+begin_src python :session py1
  import scipy.optimize
  def fix_imbalance(x, *args):
      #print(args)
      #print(args.df['returns'].shift(-1))
      bins = np.where(args[0].df.loc[args[1]]['returns'].shift(-1) > x, 1, 0)
      len_bins = len(bins) 
      len1 = sum(bins)
      len0 =  len_bins - len1
      return ((len1 - len0) / len_bins)

  #ymin = scipy.optimize.minimize(fix_imbalance, 0.00068, data)
  #ymin = scipy.optimize.newton(fix_imbalance, 0.000, args=(data,))
#+end_src

#+RESULTS:

#+begin_src python :session py1
  df_  = data.features.df.drop(data.df.columns, axis=1)
  df_.dropna(inplace=True)
  ymin = scipy.optimize.bisect(fix_imbalance, -0.01, 0.01, args=(data, df_.index))
  df_['target'] = np.where(data.df.loc[df_.index]['returns'].shift(-1) > ymin, 1, 0)
  df_.target.value_counts()
#+end_src

#+RESULTS:
: 0    589
: 1    588
: Name: target, dtype: int64

** Split data
#+begin_src python :session py1
X_train, X_test, y_train, y_test = train_test_split(data, labels, shuffle=False)
#+end_src

** SOM

#+begin_src python :session py1
########


som = MiniSom(7, 7, 4, sigma=3, learning_rate=0.5, 
              neighborhood_function='triangle', random_seed=10)
som.pca_weights_init(X_train)
som.train(X_train, 500, verbose=True)

#+end_src

#+begin_src python :session py1
W = som.get_weights()
selected_labels, target_name = model_som.som_feature_selection(W, labels=, target_index = -1, a = 0.04)
#+end_src

* Base line model

#+begin_src python :session py1


  layers_dict = dict()
  layers_dict['LSTM'] = dict(units=5, activation = 'relu', return_sequences=False, name='LSTM')
  layers_dict['Dense'] = dict(units=1, name='Output')
  layers_tuple = dict2tuple(layers_dict)
  base_model = Model_keras(keras_model='Sequential', layers=layers_tuple,
                           optimizer_name='adam', loss_name='mse', metrics=None,
                           optimizer_sett=None, compile_sett=None, loss_sett=None)
  base_model.fit(X, y)

#+end_src
