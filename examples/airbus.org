#+PROPERTY: header-args :tangle ./airbus.py :mkdirp yes
* House keeping
#+begin_src elisp :results none :tangle no :exports none
  (add-to-list 'org-structure-template-alist
  '("sp" . "src python :session py1"))
  (add-to-list 'org-structure-template-alist
  '("se" . "src elisp"))

  (setq org-confirm-babel-evaluate nil)
#+end_src

#+begin_src emacs-lisp  :session py1 :results none :tangle nil :exports none
  (pyvenv-workon "qfpy")
  (require 'org-tempo)
#+end_src

#+begin_src python  :session py1 :results none :exports none
  ##############################################################################
  # Add ml4qf library to python path.                                          #
  #                                                                            #
  # Preferable to do export PYTHONPATH="$PYTHONPATH:{path}" from command line  #
  ##############################################################################

  import sys
  import pathlib
  file_path = sys.path[1]
  sys.path.append(file_path + "/../")
#+end_src

* Import libraries
#+BEGIN_SRC python :session py1 :results output silent :exports none

  ################################
  # Import libraries and modules #
  ################################

  import numpy as np
  import pathlib
  import yfinance as yf
  from tabulate import tabulate

  # from umap import UMAP
  # import matplotlib.pyplot as plt
  # from mpl_toolkits.mplot3d import Axes3D
  # import matplotlib
  # matplotlib.rcParams['figure.dpi'] = 80

  import minisom
  import umap
  from sklearn.model_selection import train_test_split
  import sklearn.metrics
  import scipy.optimize
  import sklearn.compose
  import sklearn.pipeline
  #from scikeras.wrappers import KerasClassifier
  import tensorflow.keras.utils
  import plotly.express as px
  import scipy.stats
  import pandas as pd
  import pandas_ta as ta

  import ml4qf
  import ml4qf.utils
  import ml4qf.predictors.model_som
  import ml4qf.collectors.financial_features
  import ml4qf.transformers
  import ml4qf.predictors.model_keras as model_keras
  import ml4qf.predictors.model_tuning
#+END_SRC

#+begin_src python :session py1 :results none :exports none
  #######################################
  # Set random seed for keras and numpy #
  #######################################
  tensorflow.keras.utils.set_random_seed(42)
#+end_src
* Introduction
* Feature engineering

#+begin_src python :session py1 :results none :exports none
  # 
  ###########################################
  # Get data and create Features            #
  # Ticker: EADSY (Airbus)                  #
  # Ten years from October 2019 backwards   #
  ###########################################

  FEATURES1 = {'Month_': None,
      'momentum_': [1, 2, 5, 8, 15, 23, 30, 40, 65, 75],
      'OC_': None,
      'HL_': None,
      'Ret_': [1, 5, 15, 20, 25, 30, 40, 50, 60, 75],
      #'RetLog_': list(range(10, 90, 10)),
      'Std_': list(range(3, 75, 5)),
      'MA_': [5, 10, 18, 25, 35, 50, 60, 70],
      'EMA_': [5, 10, 18, 25, 35, 50, 60, 70],
      'sign_return_': list(range(1,10)),
      'volume_':None,
      "ema": {"close": "volume", "length": [5, 10, 15, 20, 30], "prefix": "VOLUME"},
      "log_return": {"length": [1, 10, 20, 35, 50], "cumulative": True},
      "bias":{"length": [5, 10, 15, 20, 35, 50, 60, 75]},
      "aroon":{"length": [5, 10, 15, 20, 35, 50, 60, 75]},
      "stoch":{},
      "trix":{"length": [5, 10, 15, 20, 35, 50, 60, 75]},
      "tsi":{},
      #"volume---":{},
      "efi":{},
      "mfi": {"length": [5, 10, 15, 20, 35, 50, 60, 75]},
      "cmf":{"length": [5, 10, 15, 20, 35, 50, 60, 75]},
      "ohlc4":{},
      "volatility":"whole"
      #"momentum":"whole"
      #"percent_return": dict(length=2, append=False),
      #"log_return": [1, 2]
  }

#+end_src

#+begin_src python :session py1 :results none :exports none
  # 
  ###########################################
  # Get data and create Features            #
  # Ticker: EADSY (Airbus)                  #
  # Ten years from October 2019 backwards   #
  ###########################################

  data = ml4qf.collectors.financial_features.FinancialData("EADSY", 2019, 10, 1, 365*16, FEATURES1)
  img_dir = "./img/" + data.label
  pathlib.Path(img_dir).mkdir(parents=True, exist_ok=True)
  df_  = data.features.df.drop(data.df.columns, axis=1)
  df_.dropna(inplace=True)

#+end_src


#+begin_src python :session py1 :results none :exports none

  own_features = list(df_.columns[:df_.columns.get_loc('volume_')+1])
  pa_features = list(df_.columns[df_.columns.get_loc('volume_') + 1:df_.columns.get_loc('OHLC4')+1])
  pa_volfeatures = list(df_.columns[df_.columns.get_loc('OHLC4')+1:])
  total_features = len(own_features) + len(pa_features) + len(pa_volfeatures)
  # print("######################")
  # print(own_features)
  # print("######################")
  # print(pa_features)
  # print("######################")
  # print(pa_volfeatures)
  # print("######################")
  assert total_features == len(df_.columns), "Number of features not matching in dataframe"
#+end_src

** Exploratory data analysis


#+begin_src python :session py1 :results file :exports none
  fig1_path= img_dir +'/stock_Close.png'
  fig1 = px.line(df_, y=['Ret_1d', 'Ret_5d', 'Ret_15d'])
  fig1.write_image(fig1_path)
  fig1_path
#+end_src

#+RESULTS:
[[file:./img/_EADSY_2003-10-05_2019-10-01/stock_Close.png]]

*** Correlation

#+begin_src python :session py1 :results file :exports none
  fig1_path= img_dir +'/correlation.png'
  df_corr = df_.corr().round(2)
  fig1 = px.imshow(np.abs(df_corr))
  fig1.layout.height = 600
  fig1.layout.width = 600
  fig1.write_image(fig1_path)
  fig1_path
#+end_src

#+RESULTS:
[[file:./img/_EADSY_2009-10-03_2019-10-01/correlation.png]]

** Label 

*** Class imbalance

#+begin_src python :session py1
alpha_min = scipy.optimize.bisect(ml4qf.utils.fix_imbalance, -0.01, 0.01, args=(data, df_.index))
df_['target'] = np.where(data.df.loc[df_.index]['returns'].shift(-1) > alpha_min, 1, 0)
df_.target.value_counts()
#+end_src

#+RESULTS:
: 0    1487
: 1    1486
: Name: target, dtype: int64


** Feature scaling

https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02


#+begin_src python :session py1 :results output 
zscores = np.abs(scipy.stats.zscore(df_)).max()
print(zscores)
#+end_src

#+RESULTS:
#+begin_example
Month_               1.618382
momentum_1d          9.797553
momentum_2d          8.325245
momentum_5d          6.923864
momentum_8d          6.349095
                      ...    
THERMOl_20_2_0.5     3.453417
THERMOs_20_2_0.5     1.720094
TRUERANGE_1         11.346320
UI_14                5.719633
target               1.000336
Length: 177, dtype: float64
#+end_example

#+begin_src python :session py1 
tabulate(df_[:10], headers=df_.columns, tablefmt='orgtbl')
#+end_src

#+RESULTS:
#+begin_example
|                     |   Month_ |   momentum_1d |   momentum_2d |   momentum_5d |   momentum_8d |   momentum_15d |   momentum_23d |   momentum_30d |   momentum_40d |   momentum_65d |   momentum_75d |   OC_ |       HL_ |      Ret_1d |      Ret_5d |     Ret_15d |     Ret_20d |    Ret_25d |     Ret_30d |     Ret_40d |     Ret_50d |   Ret_60d |   Ret_75d |     Std_3d |     Std_8d |   Std_13d |   Std_18d |   Std_23d |   Std_28d |   Std_33d |   Std_38d |   Std_43d |   Std_48d |   Std_53d |   Std_58d |   Std_63d |   Std_68d |   Std_73d |   MA_5d |   MA_10d |   MA_18d |   MA_25d |   MA_35d |   MA_50d |   MA_60d |   MA_70d |   EMA_5d |   EMA_10d |   EMA_18d |   EMA_25d |   EMA_35d |   EMA_50d |   EMA_60d |   EMA_70d |   sign_return_1d |   sign_return_2d |   sign_return_3d |   sign_return_4d |   sign_return_5d |   sign_return_6d |   sign_return_7d |   sign_return_8d |   sign_return_9d |   volume_ |   VOLUME_EMA_5 |   VOLUME_EMA_10 |   VOLUME_EMA_15 |   VOLUME_EMA_20 |   VOLUME_EMA_30 |   CUMLOGRET_1 |   CUMLOGRET_10 |   CUMLOGRET_20 |   CUMLOGRET_35 |   CUMLOGRET_50 |   BIAS_SMA_5 |   BIAS_SMA_10 |   BIAS_SMA_15 |   BIAS_SMA_20 |   BIAS_SMA_35 |   BIAS_SMA_50 |   BIAS_SMA_60 |   BIAS_SMA_75 |   AROOND_5 |   AROONU_5 |   AROONOSC_5 |   AROOND_10 |   AROONU_10 |   AROONOSC_10 |   AROOND_15 |   AROONU_15 |   AROONOSC_15 |   AROOND_20 |   AROONU_20 |   AROONOSC_20 |   AROOND_35 |   AROONU_35 |   AROONOSC_35 |   AROOND_50 |   AROONU_50 |   AROONOSC_50 |   AROOND_60 |   AROONU_60 |   AROONOSC_60 |   AROOND_75 |   AROONU_75 |   AROONOSC_75 |   STOCHk_14_3_3 |   STOCHd_14_3_3 |   TRIX_5_9 |   TRIXs_5_9 |   TRIX_10_9 |   TRIXs_10_9 |   TRIX_15_9 |   TRIXs_15_9 |   TRIX_20_9 |   TRIXs_20_9 |   TRIX_35_9 |   TRIXs_35_9 |   TRIX_50_9 |   TRIXs_50_9 |   TRIX_60_9 |   TRIXs_60_9 |   TRIX_75_9 |   TRIXs_75_9 |   TSI_13_25_13 |   TSIs_13_25_13 |   EFI_13 |    MFI_5 |   MFI_10 |   MFI_15 |   MFI_20 |   MFI_35 |   MFI_50 |   MFI_60 |   MFI_75 |     CMF_5 |    CMF_10 |    CMF_15 |    CMF_20 |   CMF_35 |   CMF_50 |   CMF_60 |   CMF_75 |   OHLC4 |   ABER_ZG_5_15 |   ABER_SG_5_15 |   ABER_XG_5_15 |   ABER_ATR_5_15 |   ACCBL_20 |   ACCBM_20 |   ACCBU_20 |   ATRr_14 |   BBL_5_2.0 |   BBM_5_2.0 |   BBU_5_2.0 |   BBB_5_2.0 |   BBP_5_2.0 |   DCL_20_20 |   DCM_20_20 |   DCU_20_20 |     HWM |     HWU |     HWL |   KCLe_20_2 |   KCBe_20_2 |   KCUe_20_2 |   MASSI_9_25 |   NATR_14 |     PDIST |   RVI_14 |   THERMO_20_2_0.5 |   THERMOma_20_2_0.5 |   THERMOl_20_2_0.5 |   THERMOs_20_2_0.5 |   TRUERANGE_1 |   UI_14 |   target |
|---------------------+----------+---------------+---------------+---------------+---------------+----------------+----------------+----------------+----------------+----------------+----------------+-------+-----------+-------------+-------------+-------------+-------------+------------+-------------+-------------+-------------+-----------+-----------+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+---------+----------+----------+----------+----------+----------+----------+----------+----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------+----------------+-----------------+-----------------+-----------------+-----------------+---------------+----------------+----------------+----------------+----------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+------------+------------+--------------+-------------+-------------+---------------+-------------+-------------+---------------+-------------+-------------+---------------+-------------+-------------+---------------+-------------+-------------+---------------+-------------+-------------+---------------+-------------+-------------+---------------+-----------------+-----------------+------------+-------------+-------------+--------------+-------------+--------------+-------------+--------------+-------------+--------------+-------------+--------------+-------------+--------------+-------------+--------------+----------------+-----------------+----------+----------+----------+----------+----------+----------+----------+----------+----------+-----------+-----------+-----------+-----------+----------+----------+----------+----------+---------+----------------+----------------+----------------+-----------------+------------+------------+------------+-----------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+---------+---------+---------+-------------+-------------+-------------+--------------+-----------+-----------+----------+-------------------+---------------------+--------------------+--------------------+---------------+---------+----------|
| 2007-12-07 00:00:00 |       12 |     0.0250001 |     0.0374999 |    -0.0625    |    -0.0250001 |     -0.5125    |     -0.04      |     -0.9625    |     -0.2875    |         0.7125 |       1.05     |     0 | 0.0250001 |  0.00316958 | -0.00783699 | -0.0608309  | -0.0552239  | -0.0341776 | -0.108451   | -0.035061   |  0.0567612  |  0.118375 | 0.153005  | 0.011775   | 0.0148982  | 0.0128057 | 0.0183693 | 0.0252698 | 0.0258942 | 0.0247699 | 0.0242697 | 0.0237404 | 0.0228744 | 0.0224757 | 0.0218453 | 0.0212447 | 0.0210558 | 0.0210242 | 7.86458 |  7.91023 |  8.05145 |  8.07173 |  8.22118 |  8.14157 |  7.97992 |  7.87556 |  7.95143 |   8.02032 |   8.02981 |   7.99969 |   7.95597 |   7.91651 |   7.9051  |   7.90161 |                1 |                0 |                0 |                0 |                0 |                0 |                0 |                0 |                1 |     18800 |        54172.8 |         71748.7 |         92071.3 |        107586   |        123249   |    -0.0538178 |     -0.0538178 |     -0.0538178 |     -0.0538178 |     -0.0538178 |  0.00892572  |  -0.000157957 |  -0.00463352  |  -0.0186959   |   -0.0373502  |  -0.0296947   |   -0.010319   |    0.00915734 |         40 |          0 |          -40 |          70 |          30 |           -40 |     80      |      0      |      -80      |          85 |          20 |           -65 |     91.4286 |    17.1429  |      -74.2857 |           0 |          42 |            42 |     3.33333 |     51.6667 |       48.3333 |     0       |     61.3333 |       61.3333 |         30.6715 |         18.3196 | -0.219191  |  -0.331432  |  -0.263889  |    -0.24678  |  -0.132998  |   -0.064734  |  0.00291774 |  0.0746055   |   0.120718  |    0.142605  |   0.0727711 |    0.0727182 |   0.0389354 |    0.0347315 |   0.0137573 |   0.00933384 |       -9.41426 |        -4.49834 | 3641.84  |  74.7711 |  65.6168 |  52.6879 |  82.4752 |  79.6775 |  60.1079 |  60.2014 |  60.4244 | -0.321094 | -0.249897 | -0.314589 |  0.580214 | 0.510817 | 0.390488 | 0.391695 | 0.389698 | 7.91875 |        7.85433 |        8.01976 |        7.68891 |        0.165423 |    7.79273 |    8.06325 |    8.3496  |  0.164184 |     7.692   |      7.8425 |     7.993   |     3.83804 |    0.732559 |         7.7 |     8.13125 |      8.5625 | 7.54035 | 7.83858 | 7.24212 |     7.69287 |     8.00577 |     8.31867 |      25.1913 |   2.07499 | 0.0750003 |  51.2372 |         0.0599999 |            0.122232 |                  1 |                  0 |     0.0500002 | 7.28912 |        1 |
| 2007-12-10 00:00:00 |       12 |     0.2       |     0.225     |     0.275     |     0.0375004 |     -0.3625    |      0.0950003 |     -0.6875    |     -0.349999  |         0.9125 |       1.05     |     0 | 0.0625    |  0.0252765  |  0.0350877  | -0.0427729  | -0.0211162  | -0.0255255 | -0.078125   | -0.0413589  |  0.0587276  |  0.146643 | 0.148673  | 0.0132439  | 0.0164288  | 0.0144696 | 0.019091  | 0.0257655 | 0.026268  | 0.0247214 | 0.024215  | 0.0239262 | 0.0231452 | 0.0226969 | 0.0218244 | 0.0213545 | 0.0212552 | 0.0211948 | 7.8875  |  7.93182 |  8.04092 |  8.06865 |  8.21653 |  8.15382 |  7.99693 |  7.88665 |  7.97444 |   8.02801 |   8.03394 |   8.00387 |   7.9602  |   7.92028 |   7.90844 |   7.90454 |                1 |                1 |                1 |                0 |                0 |                0 |                0 |                1 |                1 |     99600 |        69315.2 |         76812.6 |         93012.4 |        106826   |        121723   |    -0.0288555 |     -0.0288555 |     -0.0288555 |     -0.0288555 |     -0.0288555 |  0.0272238   |   0.0220473   |   0.0236378   |   0.00720097  |   -0.0116006  |  -0.00626555  |    0.0125069  |    0.0328211  |         20 |        100 |           80 |          60 |          20 |           -40 |     73.3333 |      0      |      -73.3333 |          80 |          15 |           -65 |     88.5714 |    14.2857  |      -74.2857 |           2 |          40 |            38 |     1.66667 |     50      |       48.3333 |    14.6667  |     60      |       45.3333 |         53.1095 |         33.0457 | -0.0585858 |  -0.272784  |  -0.236644  |    -0.249083 |  -0.13621   |   -0.0817979 | -0.00847163 |  0.0567632   |   0.115162  |    0.137513  |   0.0722465 |    0.0729615 |   0.0395747 |    0.0359302 |   0.0146437 |   0.0105128  |       -7.47061 |        -4.92295 | 5967.29  |  85.9995 |  70.8138 |  61.274  |  84.9513 |  79.7498 |  60.6038 |  60.702  |  60.9106 | -0.251315 | -0.242499 | -0.3856   |  0.547336 | 0.497164 | 0.382892 | 0.384033 | 0.382429 | 8.11562 |        7.90517 |        8.07539 |        7.73494 |        0.170228 |    7.80626 |    8.0545  |    8.32876 |  0.169421 |     7.63511 |      7.8975 |     8.15989 |     6.64497 |    0.909691 |         7.7 |     8.13125 |      8.5625 | 7.6042  | 7.90942 | 7.29899 |     7.6876  |     8.01594 |     8.34427 |      24.9549 |   2.08839 | 0.325     |  54.714  |         0.2125    |            0.130829 |                  1 |                  1 |     0.2375    | 7.07451 |        0 |
| 2007-12-11 00:00:00 |       12 |    -0.0375004 |     0.1625    |     0.375     |     0.0124998 |      0.0749998 |      0.3375    |     -0.6425    |     -0.325     |         1.05   |       0.7625   |     0 | 0.125     | -0.00462254 |  0.0487013  |  0.00937498 | -0.0285715  |  0.015404  | -0.0737023  | -0.0386905  |  0.0470016  |  0.151515 | 0.104273  | 0.0155101  | 0.0165245  | 0.0136878 | 0.0186278 | 0.024603  | 0.0262742 | 0.0247125 | 0.0240331 | 0.0237984 | 0.0231519 | 0.0222704 | 0.0218414 | 0.0212241 | 0.0209098 | 0.0208278 | 7.92708 |  7.95    |  8.02184 |  8.05904 |  8.20403 |  8.16191 |  8.01332 |  7.89722 |  7.98881 |   8.03192 |   8.036   |   8.00651 |   7.9633  |   7.92325 |   7.91113 |   7.90691 |                0 |                0 |                0 |                0 |                0 |                0 |                0 |                1 |                0 |     15600 |        51410.1 |         65683   |         83335.8 |         98137.7 |        114877   |    -0.0334887 |     -0.0334887 |     -0.0334887 |     -0.0334887 |     -0.0334887 |  0.0128567   |   0.0155636   |   0.0182636   |   0.0040254   |   -0.0149264  |  -0.0117368   |    0.00560396 |    0.026718   |          0 |        100 |          100 |          50 |         100 |            50 |     66.6667 |    100      |       33.3333 |          75 |          10 |           -65 |     85.7143 |    11.4286  |      -74.2857 |           0 |          38 |            38 |     0       |     48.3333 |       48.3333 |    13.3333  |     58.6667 |       45.3333 |         70.045  |         51.2753 |  0.0721845 |  -0.214838  |  -0.201876  |    -0.245744 |  -0.134492  |   -0.095806  | -0.0170906  |  0.0403463   |   0.109933  |    0.132256  |   0.071698  |    0.0730003 |   0.0401681 |    0.0369841 |   0.0155013 |   0.0116177  |       -6.17579 |        -5.10193 | 5031.25  | 100      |  64.0353 |  66.3585 |  84.7415 |  80.1049 |  60.5963 |  60.7153 |  60.9089 | -0.169189 | -0.266223 | -0.346821 |  0.547772 | 0.493852 | 0.379953 | 0.381331 | 0.37968  | 8.10625 |        7.9835  |        8.15071 |        7.81629 |        0.167213 |    7.79442 |    8.04262 |    8.32317 |  0.166248 |     7.77163 |      7.9725 |     8.17337 |     5.03915 |    0.755136 |         7.7 |     8.13125 |      8.5625 | 7.65866 | 7.97122 | 7.34611 |     7.70069 |     8.02156 |     8.34244 |      24.8445 |   2.0588  | 0.2875    |  50.2934 |         0.0500002 |            0.123131 |                  1 |                  0 |     0.125     | 6.80134 |        1 |
| 2007-12-12 00:00:00 |       12 |     0.1       |     0.0625    |     0.3       |     0.2       |      0.2375    |     -0.2       |     -0.3875    |      0.0124998 |         1.025  |       0.8      |     0 | 0.0874996 |  0.0123839  |  0.0380953  |  0.0299213  | -0.0311111  |  0.0196446 | -0.0452555  |  0.00153137 |  0.0599676  |  0.135811 | 0.108475  | 0.0149966  | 0.0163226  | 0.0138938 | 0.0186689 | 0.0175406 | 0.0251841 | 0.02449   | 0.0241156 | 0.0237786 | 0.0232062 | 0.0222887 | 0.0217643 | 0.0212532 | 0.020938  | 0.020665  | 8.00625 |  7.97159 |  8.00145 |  8.0676  |  8.19674 |  8.17098 |  8.03238 |  7.90722 |  8.01541 |   8.04385 |   8.04295 |   8.01275 |   7.96903 |   7.92809 |   7.91538 |   7.91063 |                0 |                0 |                1 |                1 |                0 |                0 |                0 |                0 |                0 |     64000 |        55606.8 |         65377   |         80918.9 |         94886.5 |        111594   |    -0.0211808 |     -0.0211808 |     -0.0211808 |     -0.0211808 |     -0.0211808 |  0.0177405   |   0.0268488   |   0.0288196   |   0.0181207   |   -0.00102993 |  -0.000629546 |    0.015996   |    0.038025   |          0 |        100 |          100 |          40 |         100 |            60 |     60      |    100      |       40      |          70 |           5 |           -65 |     82.8571 |     8.57143 |      -74.2857 |           4 |          36 |            32 |     3.33333 |     46.6667 |       43.3333 |    12       |     57.3333 |       45.3333 |         82.8779 |         68.6775 |  0.203962  |  -0.158245  |  -0.157286  |    -0.237187 |  -0.126872  |   -0.106686  | -0.0223603  |  0.0254984   |   0.105224  |    0.126913  |   0.071204  |    0.0728631 |   0.0407623 |    0.0379082 |   0.0163536 |   0.0126547  |       -4.19726 |        -4.97269 | 5226.79  | 100      |  64.9895 |  72.648  |  56.3282 |  80.1252 |  60.9167 |  60.9992 |  61.1879 | -0.480324 | -0.377189 | -0.35633  | -0.352348 | 0.485628 | 0.368541 | 0.369302 | 0.367792 | 8.19688 |        8.05017 |        8.21873 |        7.8816  |        0.168565 |    7.78753 |    8.0295  |    8.31316 |  0.167766 |     7.80634 |      8.0325 |     8.25866 |     5.63122 |    0.815037 |         7.7 |     8.13125 |      8.5625 | 7.73495 | 8.07223 | 7.39767 |     7.71014 |     8.03618 |     8.36221 |      24.788  |   2.05218 | 0.275     |  54.9148 |         0.1       |            0.120928 |                  1 |                  1 |     0.1875    | 6.38863 |        0 |
| 2007-12-13 00:00:00 |       12 |    -0.14      |    -0.04      |     0.1475    |     0.1975    |      0.22      |     -0.252501  |     -0.515     |     -0.2775    |         1.01   |       0.8975   |     0 | 0.1375    | -0.0171254  |  0.0187005  |  0.028151   | -0.0616059  |  0.0384491 | -0.060234   | -0.0333835  |  0.0452032  |  0.11636  | 0.125744  | 0.0148119  | 0.0163018  | 0.014969  | 0.0189179 | 0.0177597 | 0.0251893 | 0.0246055 | 0.0237515 | 0.0233912 | 0.0230091 | 0.0222326 | 0.0218962 | 0.0213949 | 0.021066  | 0.0207816 | 8.03292 |  7.96795 |  7.98092 |  8.06827 |  8.17931 |  8.1773  |  8.04611 |  7.91546 |  8.0182  |   8.04311 |   8.04255 |   8.01357 |   7.97081 |   7.93015 |   7.91731 |   7.91236 |                0 |                1 |                0 |                0 |                0 |                0 |                0 |                0 |                1 |     47200 |        52804.5 |         62072.1 |         76704   |         90344.9 |        107440   |    -0.0384546 |     -0.0384546 |     -0.0384546 |     -0.0384546 |     -0.0384546 | -0.00334906  |   0.00961235  |   0.00933756  |   0.0039828   |   -0.0158958  |  -0.018578    |   -0.00313267 |    0.0187005  |          0 |         80 |           80 |          30 |          90 |            60 |     53.3333 |     93.3333 |       40      |          65 |           0 |           -65 |     80      |     5.71429 |      -74.2857 |           2 |          34 |            32 |     1.66667 |     45      |       43.3333 |    10.6667  |     56      |       45.3333 |         73      |         75.3076 |  0.228583  |  -0.107818  |  -0.120627  |    -0.224595 |  -0.118717  |   -0.114641  | -0.0264874  |  0.0122377   |   0.100672  |    0.121541  |   0.0706474 |    0.0725715 |   0.04129   |    0.0387135 |   0.0171663 |   0.0136282  |       -3.75467 |        -4.79869 | 3536.1   |  79.5828 |  65.7752 |  72.6446 |  52.6479 |  79.0518 |  60.5858 |  60.6428 |  60.8497 | -0.577036 | -0.498641 | -0.340604 | -0.400592 | 0.48781  | 0.363498 | 0.364103 | 0.362767 | 8.05187 |        8.08317 |        8.25216 |        7.91417 |        0.168994 |    7.75699 |    8.00313 |    8.29824 |  0.168283 |     7.88635 |      8.062  |     8.23765 |     4.35756 |    0.423144 |         7.7 |     8.1     |      8.5    | 7.77713 | 8.12313 | 7.43113 |     7.70775 |     8.03606 |     8.36438 |      24.6867 |   2.09437 | 0.415     |  49.6158 |         0.175     |            0.126078 |                  1 |                  1 |     0.175     | 6.03484 |        0 |
| 2007-12-14 00:00:00 |       12 |    -0.0599999 |    -0.2       |     0.0625    |     0.275     |      0.0999999 |     -0.3375    |     -0.2175    |     -0.305     |         0.9    |       0.65     |     0 | 0.0499997 | -0.00746732 |  0.00789889 |  0.0126984  | -0.0534125  | -0.0477612 | -0.0265487  | -0.0368357  |  0.0373984  |  0.124427 | 0.0887372 | 0.0150452  | 0.0147861  | 0.0150395 | 0.0135865 | 0.0177813 | 0.0237326 | 0.024591  | 0.0237273 | 0.0233917 | 0.0229829 | 0.0222505 | 0.0219319 | 0.0213842 | 0.0208521 | 0.0207011 | 8.0475  |  7.96    |  7.95461 |  8.0774  |  8.15951 |  8.18294 |  8.05885 |  7.92602 |  8.01203 |   8.03743 |   8.03917 |   8.01214 |   7.97092 |   7.93101 |   7.91824 |   7.91323 |                0 |                0 |                1 |                0 |                0 |                0 |                0 |                0 |                0 |     33600 |        46403   |         56895.4 |         71316   |         84940.6 |        102676   |    -0.0459499 |     -0.0459499 |     -0.0459499 |     -0.0459499 |     -0.0459499 | -0.0123228   |   0.00207325  |   0.000962257 |  -0.000704842 |   -0.0201585  |  -0.0265903   |   -0.0123788  |    0.00998374 |          0 |         60 |           60 |          20 |          80 |            60 |     46.6667 |     86.6667 |       40      |          60 |           0 |           -60 |     77.1429 |     2.85714 |      -74.2857 |           0 |          32 |            32 |     0       |     43.3333 |       43.3333 |     9.33333 |     54.6667 |       45.3333 |         64.2963 |         73.3914 |  0.178744  |  -0.0613001 |  -0.0952733 |    -0.208421 |  -0.111762  |   -0.119712  | -0.0302799  |  0.000640234 |   0.0961438 |    0.116204  |   0.0699869 |    0.0721519 |   0.0417273 |    0.0394144 |   0.0179262 |   0.0145443  |       -3.90698 |        -4.6713  | 2742.95  |  67.2785 |  71.1258 |  66.1218 |  55.8192 |  73.5159 |  60.3365 |  60.4045 |  60.5952 | -0.536497 | -0.427486 | -0.339417 | -0.363885 | 0.472404 | 0.359786 | 0.360474 | 0.359181 | 7.98125 |        8.09567 |        8.25823 |        7.93311 |        0.162561 |    7.73709 |    7.98062 |    8.27084 |  0.161441 |     7.93884 |      8.0745 |     8.21016 |     3.36025 |    0.133278 |         7.7 |     8.0875  |      8.475  | 7.80643 | 8.16295 | 7.44991 |     7.71939 |     8.03025 |     8.3411  |      24.5146 |   2.02434 | 0.159999  |  44.7399 |         0.125     |            0.125975 |                  1 |                  1 |     0.0724998 | 5.6782  |        1 |
| 2007-12-17 00:00:00 |       12 |     0.0125003 |    -0.0474997 |    -0.125     |     0.1125    |      0.1125    |     -0.45      |     -0.3375    |     -0.537499  |         0.9125 |       0.6625   |     0 | 0.0249996 |  0.00156743 | -0.0154083  |  0.0142857  | -0.0575221  | -0.0361991 | -0.0405405  | -0.0630498  |  0.010117   |  0.115183 | 0.0904437 | 0.00934816 | 0.0128198  | 0.0141781 | 0.013452  | 0.017425  | 0.0236849 | 0.0244431 | 0.0232753 | 0.0230009 | 0.0228723 | 0.0222503 | 0.0219302 | 0.0211602 | 0.0207622 | 0.0206978 | 8.06    |  7.96114 |  7.95395 |  8.0625  |  8.13486 |  8.18882 |  8.07352 |  7.93711 |  8.00853 |   8.03327 |   8.03659 |   8.01123 |   7.97137 |   7.9321  |   7.91936 |   7.91426 |                0 |                0 |                0 |                0 |                0 |                0 |                0 |                0 |                0 |     12800 |        35202   |         48878   |         64001.5 |         78070.1 |         96877.4 |    -0.0443837 |     -0.0443837 |     -0.0443837 |     -0.0443837 |     -0.0443837 | -0.00770232  |   0.00175584  |   0.00158836  |   0.00392776  |   -0.0158156  |  -0.0252549   |   -0.0125123  |    0.0104365  |         80 |         40 |          -40 |          10 |          70 |            60 |     40      |     80      |       40      |          55 |           0 |           -55 |     74.2857 |     0       |      -74.2857 |          82 |          30 |           -52 |     0       |     41.6667 |       41.6667 |     8       |     53.3333 |       45.3333 |         53.1852 |         63.4939 |  0.119174  |  -0.010963  |  -0.0768362 |    -0.188088 |  -0.105392  |   -0.121642  | -0.0335697  | -0.00909267  |   0.0916879 |    0.110997  |   0.069243  |    0.0716415 |   0.0420855 |    0.0400317 |   0.0186385 |   0.0154128  |       -3.91427 |        -4.56315 | 2373.95  |  49.1716 |  74.3531 |  66.6452 |  59.5257 |  74.5246 |  60.3393 |  60.4526 |  60.6563 | -0.764268 | -0.421774 | -0.349421 | -0.443991 | 0.482152 | 0.356544 | 0.358381 | 0.356989 | 7.99375 |        8.0715  |        8.22572 |        7.91728 |        0.154224 |    7.73259 |    7.95625 |    8.23509 |  0.152588 |     7.90526 |      8.0495 |     8.19374 |     3.58373 |    0.285075 |         7.7 |     7.98125 |      8.2625 | 7.83993 | 8.18785 | 7.49201 |     7.73778 |     8.02618 |     8.31457 |      24.1519 |   1.91034 | 0.0624995 |  50.3517 |         0.0250001 |            0.116358 |                  1 |                  0 |     0.0374999 | 5.36768 |        1 |
| 2007-12-18 00:00:00 |       12 |     0.0374994 |     0.0499997 |    -0.0500002 |     0.1375    |      0.0874996 |     -0.5375    |      0.0724998 |     -0.4125    |         1.0125 |       0.559999 |     0 | 0.150001  |  0.00469476 | -0.00619197 |  0.0110236  |  0.00312495 | -0.0345865 |  0.0091166  | -0.0488889  |  0.0255591  |  0.120028 | 0.0750167 | 0.00631563 | 0.0128578  | 0.0142542 | 0.0128798 | 0.0170997 | 0.022675  | 0.0244694 | 0.0232871 | 0.0228311 | 0.0227457 | 0.0222486 | 0.0215018 | 0.0211618 | 0.0206279 | 0.0203636 | 8.04542 |  7.97818 |  7.95855 |  8.0524  |  8.11333 |  8.19113 |  8.08766 |  7.94873 |  8.01088 |   8.03258 |   8.03601 |   8.01174 |   7.97282 |   7.93388 |   7.92106 |   7.9158  |                1 |                0 |                0 |                0 |                0 |                0 |                1 |                1 |                0 |     86400 |        52268   |         55700.2 |         66801.3 |         78863.4 |         96201.5 |    -0.0397    |     -0.0397    |     -0.0397    |     -0.0397    |     -0.0397    | -0.00180364  |   0.00237318  |   0.00555505  |   0.00848252  |   -0.00877856 |  -0.0211565   |   -0.0096312  |    0.0142222  |         60 |         20 |          -40 |           0 |          60 |            60 |     33.3333 |     73.3333 |       40      |          50 |          80 |            30 |     71.4286 |     0       |      -71.4286 |          80 |          28 |           -52 |     1.66667 |     40      |       38.3333 |     6.66667 |     52      |       45.3333 |         52.5926 |         56.6914 |  0.085196  |   0.0356698 |  -0.060535  |    -0.164865 |  -0.0985817 |   -0.120749  | -0.0359753  | -0.0170141   |   0.0873988 |    0.105963  |   0.0684535 |    0.0710616 |   0.0423868 |    0.0405776 |   0.0193138 |   0.0162392  |       -3.55648 |        -4.41934 | 2497.67  |  67.1393 |  85.0778 |  65.0055 |  66.5307 |  76.2579 |  60.7968 |  60.8613 |  60.9067 | -0.832669 | -0.459521 | -0.437958 | -0.451029 | 0.446343 | 0.34128  | 0.343679 | 0.341799 | 8.0625  |        8.06317 |        8.21961 |        7.90672 |        0.156442 |    7.73138 |    7.9575  |    8.24013 |  0.155082 |     7.8968  |      8.0395 |     8.1822  |     3.55004 |    0.449194 |         7.7 |     7.98125 |      8.2625 | 7.8822  | 8.21654 | 7.54786 |     7.72942 |     8.02606 |     8.32271 |      24.0409 |   1.93248 | 0.337501  |  55.4743 |         0.1625    |            0.120753 |                  1 |                  1 |     0.1875    | 5.17079 |        0 |
| 2007-12-19 00:00:00 |       12 |    -0.0249996 |     0.0124998 |    -0.175     |     0.0875001 |     -0.0749998 |     -0.425     |     -0.0174999 |     -0.6625    |         0.8025 |       0.55     |     0 | 0.150001  | -0.00311522 | -0.0214068  | -0.0092879  |  0.00787402 | -0.0518519 | -0.00218272 | -0.0764791  |  0.00628933 |  0.116539 | 0.0738255 | 0.00393072 | 0.0129839  | 0.0139332 | 0.012829  | 0.0168503 | 0.0162243 | 0.0233868 | 0.0229586 | 0.0228287 | 0.0226582 | 0.0222556 | 0.02148   | 0.0210663 | 0.0206286 | 0.0203624 | 8.03292 |  8.00545 |  7.96829 |  8.04038 |  8.0934  |  8.19456 |  8.10135 |  7.96246 |  8.00933 |   8.02987 |   8.03421 |   8.01131 |   7.97355 |   7.93516 |   7.92234 |   7.91697 |                0 |                0 |                0 |                0 |                0 |                0 |                0 |                0 |                0 |     57600 |        54045.3 |         56045.6 |         65651.1 |         76838.3 |         93711.1 |    -0.04282   |     -0.04282   |     -0.04282   |     -0.04282   |     -0.04282   | -0.000562173 |  -0.00230716  |   0.00305095  |   0.00494623  |   -0.00990099 |  -0.0243248   |   -0.0144091  |    0.0101265  |        100 |          0 |         -100 |           0 |          50 |            50 |     26.6667 |     66.6667 |       40      |          45 |          75 |            30 |     68.5714 |     0       |      -68.5714 |          78 |          26 |           -52 |     0       |     38.3333 |       38.3333 |     5.33333 |     50.6667 |       45.3333 |         54.0741 |         53.284  |  0.0528574 |   0.0736584 |  -0.0483318 |    -0.140144 |  -0.092176  |   -0.117467  | -0.0379006  | -0.0232464   |   0.0832191 |    0.101129  |   0.0676036 |    0.0704282 |   0.0426226 |    0.0410614 |   0.0199474 |   0.0170276  |       -3.48889 |        -4.28642 | 1935.15  |  41.9314 |  72.2559 |  58.0794 |  66.1394 |  76.3869 |  60.3281 |  60.4414 |  60.4394 | -0.464526 | -0.472992 | -0.402745 | -0.378793 | 0.453672 | 0.341601 | 0.344806 | 0.344805 | 7.98125 |        8.01733 |        8.17335 |        7.86132 |        0.156013 |    7.73255 |    7.96062 |    8.23193 |  0.154719 |     7.95951 |      8.0045 |     8.04949 |     1.12408 |    0.449988 |         7.7 |     7.98125 |      8.2625 | 7.9179  | 8.2385  | 7.5973  |     7.72662 |     8.02358 |     8.32054 |      24.091  |   1.93399 | 0.325001  |  49.6725 |         0.1375    |            0.122348 |                  1 |                  1 |     0.150001  | 4.96274 |        0 |
| 2007-12-20 00:00:00 |       12 |    -0.0374999 |    -0.0624995 |    -0.0724998 |    -0.15      |     -0.0999999 |     -0.5125    |      0.225     |     -0.725     |         0.765  |       0.7375   |     0 | 0.150001  | -0.00468749 | -0.00902299 | -0.0124031  |  0.018874   | -0.070073  |  0.0290791  | -0.0834532  | -0.0154559  |  0.074199 | 0.102076  | 0.00502485 | 0.00876483 | 0.0130189 | 0.0128969 | 0.0167652 | 0.0161431 | 0.0232044 | 0.0229402 | 0.0223446 | 0.0221596 | 0.0219284 | 0.0212909 | 0.0210599 | 0.0206433 | 0.0203751 | 7.9975  |  8.01341 |  7.97289 |  8.02212 |  8.07674 |  8.1948  |  8.11443 |  7.97391 |  8.00264 |   8.02425 |   8.03062 |   8.0095  |   7.97326 |   7.93568 |   7.92299 |   7.9176  |                1 |                0 |                0 |                0 |                0 |                0 |                1 |                0 |                0 |    128400 |        78830.2 |         69201   |         73494.8 |         81749   |         95949.1 |    -0.0475186 |     -0.0475186 |     -0.0475186 |     -0.0475186 |     -0.0475186 | -0.00344179  |  -0.00791177  |  -0.000815643 |  -0.000690244 |   -0.0124906  |  -0.0286021   |   -0.0201357  |    0.0041448  |         80 |         60 |          -20 |           0 |          40 |            40 |     20      |     60      |       40      |          40 |          70 |            30 |     65.7143 |    28.5714  |      -37.1429 |          76 |          24 |           -52 |     0       |     36.6667 |       36.6667 |     4       |     49.3333 |       45.3333 |         52.5926 |         53.0864 |  0.0100901 |   0.0991341 |  -0.0419587 |    -0.115485 |  -0.0870421 |   -0.112361  | -0.0397888  | -0.0279916   |   0.0790736 |    0.0965017 |   0.0666709 |    0.0697504 |   0.04278   |    0.0414886 |   0.0205319 |   0.0177803  |       -3.79173 |        -4.21575 |  970.842 |  71.5265 |  74.9231 |  67.8866 |  72.3486 |  78.2947 |  60.8481 |  61.0325 |  61.054  | -0.609159 | -0.595193 | -0.53839  | -0.410528 | 0.408082 | 0.321369 | 0.325978 | 0.326655 | 7.99375 |        8.00667 |        8.16228 |        7.85105 |        0.155612 |    7.73882 |    7.968   |    8.24132 |  0.154382 |     7.94699 |      7.99   |     8.03301 |     1.07663 |    0.180318 |         7.7 |     7.98125 |      8.2625 | 7.94451 | 8.252   | 7.63703 |     7.72051 |     8.01776 |     8.31502 |      24.2701 |   1.93886 | 0.337501  |  44.3236 |         0.0625    |            0.116648 |                  1 |                  1 |     0.150001  | 4.66355 |        1 |
#+end_example


#+begin_src python :session py1 :results output
  zscore5 = zscores[np.where(zscores>5)[0]]
  zscore6 = zscores[np.where(zscores>6)[0]]
  momentum = ['momentum_%sd'%i for i in FEATURES1['momentum_']]
  robust_scaler = set(zscore6.keys()).union(momentum) - set(['Ret_1d', 'Ret_5d','Std_3d', 'Std_8d'])
  #robustscaler += momentum
  print(zscore6)
#+end_src

#+RESULTS:
#+begin_example
momentum_1d         9.797553
momentum_2d         8.325245
momentum_5d         6.923864
momentum_8d         6.349095
OC_                 7.111603
HL_                17.900512
Ret_1d              8.650474
Ret_5d              7.179704
Std_3d              7.467710
Std_8d              6.306172
volume_            25.036073
VOLUME_EMA_5       17.933932
VOLUME_EMA_10      13.547161
VOLUME_EMA_15      10.946547
VOLUME_EMA_20       9.275857
VOLUME_EMA_30       7.269476
BIAS_SMA_5          6.245809
EFI_13              8.728109
BBB_5_2.0           7.599345
PDIST              15.234062
THERMO_20_2_0.5    12.317750
TRUERANGE_1        11.346320
dtype: float64
#+end_example

#+begin_src python :session py1 :results none

  transformers = {'SeasonTransformer':{'features': ['Month_']},
                  'MinMaxScaler': {'features': ['sign_return']},
                  'RobustScaler': {'features': robust_scaler},
                  'StandardScaler_1': {'features': ['EMA', 'MA', 'Std', 'Ret']},
                  'StandardScaler_2': {'features': list(set(pa_volfeatures) - robust_scaler)},
                  'StandardScaler_3': {'features': list(set(pa_features) - robust_scaler)}
                  }

  columns = ml4qf.transformers.build_transformation(df_, transformers)
  columns_validation = ml4qf.transformers.build_transformation(df_, transformers)
  ct = sklearn.compose.ColumnTransformer(columns, remainder='passthrough')
  #ct_validation = sklearn.compose.ColumnTransformer(columns, remainder='passthrough')

#+end_src
*** Split data
#+begin_src python :session py1 :results output
  Xtrain, Xtest = train_test_split(df_.to_numpy(), train_size=0.8, shuffle=False)
  len_train = len(Xtrain)
  len_test = len(Xtest)
  df_train = df_.iloc[:len_train, :]
  df_test = df_.iloc[len_train:, :]
  Xtrain_scaled = ct.fit_transform(df_train)
  Xtrain_scaled = ml4qf.transformers.swap_features(Xtrain_scaled, df_train, ct)
  Xtest_scaled = ct.transform(df_test)
  Xtest_scaled = ml4qf.transformers.swap_features(Xtest_scaled, df_test, ct)
  df_train_scaled = ml4qf.transformers.scale_df(df_train, columns_validation)
  assert (Xtrain_scaled == df_train_scaled.to_numpy()).all(), "scaling failed"
  #Xtrain_scaled = ct.transform(Xtrain)
  assert len([i for i in ct.get_feature_names_out() if i[:9]=='remainder']) == 1, "some scaling missing"
#+end_src

#+RESULTS:

** SOM

#+begin_src python :session py1 :results output
  som_labels = None
  #som_labels = ['volume_', 'THERMOma_20_2_0.5', 'momentum_1d', 'VOLUME_EMA_20', 'VOLUME_EMA_10', 'Std_73d', 'Std_48d', 'TRUERANGE_1', 'sign_return_1d', 'EMA_5d', 'TRIX_20_9', 'Month_', 'target', 'Std_58d', 'Std_8d', 'HWL', 'sign_return_7d', 'NATR_14', 'Std_28d', 'momentum_23d', 'TRIXs_50_9', 'BBB_5_2.0', 'sign_return_4d', 'Std_38d', 'TRIX_35_9', 'THERMOl_20_2_0.5', 'sign_return_3d', 'AROOND_35', 'VOLUME_EMA_15', 'VOLUME_EMA_5', 'THERMO_20_2_0.5']
  if som_labels is None:
    som_size = 50
    som_obj = ml4qf.predictors.model_som.Model(som_size, som_size, Xtrain_scaled, sigma=1.5, learning_rate=0.1, 
                                               neighborhood_function='gaussian', num_iter=10000, random_seed=42)
    som_labels = som_obj.iterate_som_selection(min_num_features=30, labels=list(df_train.columns), a_range=[0.01, 0.03, 0.05, 0.08, 0.1, 0.2], num_iterations=30)
  print(som_labels)
#+end_src

#+RESULTS:
: /home/acea/anaconda3/envs/qfpy/lib/python3.10/site-packages/minisom.py:379: ComplexWarning: Casting complex values to real discards the imaginary part
:   self._weights[i, j] = c1*pc[pc_order[0]] + c2*pc[pc_order[1]]
: Total number of iterations: 8
: ['volume_', 'VOLUME_EMA_20', 'VOLUME_EMA_10', 'EFI_13', 'momentum_1d', 'BIAS_SMA_10', 'Std_68d', 'BBU_5_2.0', 'HL_', 'momentum_2d', 'TRIXs_50_9', 'AROONU_5', 'momentum_75d', 'sign_return_3d', 'VOLUME_EMA_30', 'TRIXs_15_9', 'CMF_5', 'Ret_60d', 'Ret_50d', 'BIAS_SMA_35', 'VOLUME_EMA_5', 'Std_23d', 'CMF_50', 'sign_return_2d', 'THERMO_20_2_0.5', 'Std_28d', 'THERMOl_20_2_0.5', 'ATRr_14', 'NATR_14', 'THERMOs_20_2_0.5', 'TRIXs_5_9', 'UI_14', 'sign_return_6d']


#+begin_src python :session py1 :results file
  # for i, f in enumerate(feature_names):
  #     plt.subplot(3, 3, i+1)
  #     plt.title(f)
  #     plt.pcolor(W[:,:,i].T, cmap='coolwarm')
  #     plt.xticks(np.arange(size+1))
  #     plt.yticks(np.arange(size+1))
  # plt.tight_layout()
  # plt.show()
  fig1_path= img_dir +'/som.png'
  fig1 = px.imshow(som_obj.W[:20,30:,20].T)
  fig1.layout.height = 1000
  fig1.layout.width = 1000
  fig1.write_image(fig1_path)
  fig1_path
#+end_src

#+RESULTS:
[[file:./img/_EADSY_2009-10-03_2019-10-01/som.png]]

#+end_src

*** Reduced model

#+begin_src python :session py1 :results output
  index_reducedlabels = [df_train.columns.get_loc(i) for i in som_labels]
  dftrain_reduced = df_train[som_labels]
  dftest_reduced = df_test[som_labels]
  assert (dftrain_reduced.to_numpy() == Xtrain[:, index_reducedlabels]).all(), "Reduced matrix not maching dimensions"
  Xtrain_reduced = Xtrain_scaled[:, index_reducedlabels]
  Xtest_reduced = Xtest_scaled[:, index_reducedlabels]
  #Xtest_reduced = Xtest_scaled[:, index_reducedlabels]
#+end_src

#+RESULTS:

* LSTM design
#+begin_src python :session py1
  SEQ_LEN = 15
  y_train = df_train['target'].to_numpy()
  y_test  = df_test.target.to_numpy()

#+end_src

#+RESULTS:

** COMMENT Base line model
#+begin_src python :session py1
  layers_dict = dict()
  ############
  # layers_dict['LSTM'] = dict(units=5, activation = 'relu', return_sequences=False, name='LSTM')
  # layers_dict['Dense'] = dict(units=1, name='Output')
  ############
  # layers_dict['LSTM_1'] = dict(units=100*2, activation = 'elu', return_sequences=True, name='LSTM1')
  # layers_dict['Dropout_1'] = dict(rate=0.4, name='Drouput1')
  # layers_dict['LSTM_2'] = dict(units=100, activation = 'elu', return_sequences=True, name='LSTM2')
  # layers_dict['Dropout_2'] = dict(rate=0.4, name='Drouput2')
  # layers_dict['LSTM_3'] = dict(units=100, activation = 'elu', return_sequences=False, name='LSTM3')
  # layers_dict['Dense_1'] = dict(units=1, activation='sigmoid', name='Output')
  #####################
  ############
  layers_dict['LSTM_1'] = dict(units=100, activation = 'elu', return_sequences=True, name='LSTM1')
  layers_dict['LSTM_2'] = dict(units=100, activation = 'elu', return_sequences=False, name='LSTM2')
  layers_dict['Dense_1'] = dict(units=1, activation='sigmoid', name='Output')
  #####################
  # layers_dict['LSTM_1'] = dict(units=50, activation = 'elu', name='LSTM1')
  # layers_dict['Dense_1'] = dict(units=1, activation='sigmoid', name='Output')
  #####################
  winner = {'batch_size': 16, 'layers': (('LSTM_1', (('units', 70), ('activation', 'relu'), ('return_sequences', True), ('name', 'LSTM1'))), ('Dropout_1', (('rate', 0.5), ('name', 'Drouput1'))), ('LSTM_2', (('units', 50), ('activation', 'relu'), ('return_sequences', False), ('name', 'LSTM2'))), ('Dense_1', (('units', 1), ('activation', 'sigmoid'), ('name', 'Output')))), 'optimizer_name': 'adam', 'seqlen': 30}
  ####################
  layers_tuple = ml4qf.utils.dict2tuple(layers_dict)
  #######################
  base_model = model_keras.Model_binary(keras_model='Sequential', layers=layers_tuple,
                                        seqlen=SEQ_LEN, optimizer_name='adam',
                                        loss_name='binary_crossentropy',
                                        metrics=['accuracy','binary_accuracy', 'mse'],
                                        optimizer_sett=None, compile_sett=None, loss_sett=None)
  base_model.set_params(**winner)
  base_model.fit(Xtrain_reduced, y_train, epochs=100, shuffle=False, verbose=1)

  # summary
  #base_model._model.summary()

#+end_src

*** Classification
#+begin_src python :session py1
  ypred_basemodel = base_model.predict(Xtest_reduced, y_test)#.reshape(len(y_test[SEQ_LEN-1:]))
  test_report = sklearn.metrics.classification_report(base_model.ypred_generated_, 
                                                      ypred_basemodel, output_dict=True)
  dftest_report = pd.DataFrame(test_report).transpose()
  print(dftest_report)

#+end_src

#+RESULTS:


#+begin_src python :session py1
  ypred_basemodeltrain = base_model.predict(Xtrain_reduced, y_train)#.reshape(len(y_train[SEQ_LEN-1:]))
  train_report = sklearn.metrics.classification_report(base_model.ypred_generated_,
                                                       ypred_basemodeltrain, output_dict=True)
  dftrain_report = pd.DataFrame(train_report).transpose()
  print(dftrain_report)

#+end_src

#+RESULTS:

* Cross validation
** LSTM model design
*** Searcher
#+begin_src python :session py1
  lstm_model = model_keras.Model_binary(keras_model='Sequential',
                                        seqlen=SEQ_LEN, optimizer_name='adam',
                                        loss_name='binary_crossentropy',
                                        metrics=['accuracy','binary_accuracy'],
                                        optimizer_sett=None, compile_sett=None, loss_sett=None)

#+end_src

#+RESULTS:
| memory | : | hline | steps | : | ((lstm Model_binary (metrics= (accuracy binary_accuracy mse) seqlen=30))) | verbose | : | False | lstm | : | Model_binary | (metrics= (accuracy binary_accuracy mse) seqlen=30) | lstm__keras_model | : | Sequential | lstm__layers | : | nil | lstm__seqlen | : | 30 | lstm__optimizer_name | : | adam | lstm__optimizer_sett | : | hline | lstm__compile_sett | : | hline | lstm__loss_sett | : | hline | lstm__loss_name | : | binary_crossentropy | lstm__metrics | : | (accuracy binary_accuracy mse) | lstm__timeseries_sett | : | hline |


#+begin_src python :session py1 
  searcher_name = 'GridSearchCV'
  layers_hyper = []
  ###########
  layers_dict = dict()
  layers_dict['LSTM_1'] = dict(units=120, activation = 'relu', name='LSTM1')
  layers_dict['Dropout_1'] = dict(rate=0.5, name='Drouput1')
  layers_dict['Dense_1'] = dict(units=1, activation='sigmoid', name='Output')
  layers_tuple = ml4qf.utils.dict2tuple(layers_dict)
  layers_hyper.append(layers_tuple)
  #####################
  layers_dict = dict()
  layers_dict['LSTM_1'] = dict(units=50, activation = 'elu', return_sequences=True, name='LSTM1')
  layers_dict['LSTM_2'] = dict(units=50, activation = 'elu', return_sequences=False, name='LSTM2')
  layers_dict['Dense_1'] = dict(units=1, activation='sigmoid', name='Output')
  layers_tuple = ml4qf.utils.dict2tuple(layers_dict)
  layers_hyper.append(layers_tuple)
  #####################
  layers_dict = dict()
  layers_dict['LSTM_1'] = dict(units=70, activation = 'relu', return_sequences=True, name='LSTM1')
  layers_dict['Dropout_1'] = dict(rate=0.5, name='Drouput1')
  layers_dict['LSTM_2'] = dict(units=50, activation = 'relu', return_sequences=False, name='LSTM2')
  layers_dict['Dense_1'] = dict(units=1, activation='sigmoid', name='Output')
  layers_tuple = ml4qf.utils.dict2tuple(layers_dict)
  layers_hyper.append(layers_tuple)
  ############
  layers_dict = dict()
  layers_dict['LSTM_1'] = dict(units=60, activation = 'elu', return_sequences=True, name='LSTM1')
  layers_dict['LSTM_2'] = dict(units=40, activation = 'relu', return_sequences=True, name='LSTM2')
  layers_dict['LSTM_3'] = dict(units=20, activation = 'elu', return_sequences=False, name='LSTM3')
  layers_dict['Dense_1'] = dict(units=1, activation='sigmoid', name='Output')
  layers_tuple = ml4qf.utils.dict2tuple(layers_dict)
  layers_hyper.append(layers_tuple)

  ############
  layers_dict = dict()
  layers_dict['LSTM_1'] = dict(units=50, activation = 'elu', return_sequences=True, name='LSTM1')
  layers_dict['Dropout_1'] = dict(rate=0.5, name='Drouput1')
  layers_dict['LSTM_2'] = dict(units=40, activation = 'relu', return_sequences=True, name='LSTM2')
  layers_dict['LSTM_3'] = dict(units=30, activation = 'elu', return_sequences=False, name='LSTM3')
  layers_dict['Dense_1'] = dict(units=1, activation='sigmoid', name='Output')
  layers_tuple = ml4qf.utils.dict2tuple(layers_dict)
  layers_hyper.append(layers_tuple)
  #####################
  layers_dict = dict()
  layers_dict['LSTM_1'] = dict(units=50, activation = 'elu', return_sequences=True, name='LSTM1')
  layers_dict['Dropout_1'] = dict(rate=0.35, name='Drouput1')
  layers_dict['LSTM_2'] = dict(units=25, activation = 'elu', return_sequences=True, name='LSTM2')
  layers_dict['Dropout_2'] = dict(rate=0.35, name='Drouput2')
  layers_dict['LSTM_3'] = dict(units=25, activation = 'elu', return_sequences=False, name='LSTM3')
  layers_dict['Dense_1'] = dict(units=1, activation='sigmoid', name='Output')
  layers_tuple = ml4qf.utils.dict2tuple(layers_dict)
  layers_hyper.append(layers_tuple)
  #####################

  ###########
  hyper_grid = {'seqlen':[15, 25, 35, 45, 60],
                'layers':layers_hyper,
                'optimizer_name':['adam', 'adamax'],
                'batch_size': [8, 16, 32,64,128]
                }
  searcher_settings = {#'scoring':'f1',
                       #'n_iter':25,
                       'n_jobs':7,
                       'verbose': False}
  cv_name = 'TimeSeriesSplit'
  cv_settings = {'n_splits': 2}
  _hypertuning1 = ml4qf.predictors.model_tuning.HyperTuning(lstm_model, searcher_name, searcher_settings,
                                                            hyper_grid, cv_name, cv_settings)
  hypertuning1 = _hypertuning1()
  hypertuning1.fit(Xtrain_reduced, y_train, epochs=85, verbose=False, shuffle=False)

#+end_src

*** COMMENT grid itertools
#+begin_src python :session py1 
  import tensorflow.keras.backend
  import itertools
  umap_model = umap.UMAP()
  lstm_model = model_keras.Model_binary(keras_model='Sequential',
                                        seqlen=SEQ_LEN, optimizer_name='adam',
                                        loss_name='binary_crossentropy',
                                        metrics=['accuracy','binary_accuracy', 'mse'],
                                        optimizer_sett=None, compile_sett=None, loss_sett=None)
  pipe = sklearn.pipeline.Pipeline([('umap', umap_model),
                                    ('lstm', lstm_model)])

  searcher_name = 'RandomizedSearchCV'
  layers_hyper = []
  ###########
  layers_dict = dict()
  layers_dict['LSTM_1'] = dict(units=100, activation = 'elu', name='LSTM1')
  layers_dict['Dense_1'] = dict(units=1, activation='sigmoid', name='Output')
  layers_tuple = ml4qf.utils.dict2tuple(layers_dict)
  layers_hyper.append(layers_tuple)
  #####################
  layers_dict = dict()
  layers_dict['LSTM_1'] = dict(units=50, activation = 'elu', return_sequences=True, name='LSTM1')
  layers_dict['LSTM_2'] = dict(units=50, activation = 'elu', return_sequences=False, name='LSTM2')
  layers_dict['Dense_1'] = dict(units=1, activation='sigmoid', name='Output')
  layers_tuple = ml4qf.utils.dict2tuple(layers_dict)
  layers_hyper.append(layers_tuple)
  ############
  layers_dict = dict()
  layers_dict['LSTM_1'] = dict(units=50, activation = 'elu', return_sequences=True, name='LSTM1')
  layers_dict['Dropout_1'] = dict(rate=0.3, name='Drouput1')
  layers_dict['LSTM_2'] = dict(units=25, activation = 'elu', return_sequences=True, name='LSTM2')
  layers_dict['Dropout_2'] = dict(rate=0.3, name='Drouput2')
  layers_dict['LSTM_3'] = dict(units=25, activation = 'elu', return_sequences=False, name='LSTM3')
  layers_dict['Dense_1'] = dict(units=1, activation='sigmoid', name='Output')
  layers_tuple = ml4qf.utils.dict2tuple(layers_dict)
  layers_hyper.append(layers_tuple)
  #####################
  def product_dict(**kwargs):
    keys = kwargs.keys()
    vals = kwargs.values()
    for instance in itertools.product(*vals):
        yield dict(zip(keys, instance))

  ###########
  hyper_grid = {#'umap':dict(n_neighbors=[5, 15, 30, 50, 100],
                #            n_components=[3, 8, 15, 30],
                #            min_dist=[0.05, 0.1, 0.4, 0.75],
                #            random_state=42),
                'umap__n_neighbors':[30],    
                'umap__n_components':[18],         
                'umap__min_dist':[0.05],     
                'umap__random_state':[42],                    
                #'lstm__seqlen':[10, 25],
                'lstm__layers':[layers_hyper[0]],
                'lstm__optimizer_name':['adam']
                }
  searcher_settings = {'scoring':'f1',
                       'n_iter':25,
                       'verbose': True}
  fit_settings = {'lstm__epochs':150, 'lstm__shuffle':False}
  cv_name = 'TimeSeriesSplit'
  cv_settings = {'n_splits': 3}
  _hypertuning1 = ml4qf.predictors.model_tuning.HyperTuning(pipe, searcher_name, searcher_settings,
                                                            hyper_grid, cv_name, cv_settings)
  hypertuning1 = _hypertuning1()
  hyperspace = list(product_dict(**hyper_grid))
  
#+end_src

#+RESULTS:

#+begin_src python :session py1
  def do_hyper():
    score = []
    for hi in hyperspace:
        tensorflow.keras.backend.clear_session()
        pipe.set_params(**hi)
        score_hi = []
        for cvi in hypertuning1.cv.split(Xtrain_reduced):
            index_train, index_test = cvi
            Xtrain_i = Xtrain_reduced[index_train]
            ytrain_i = y_train[index_train]
            Xtest_i = Xtrain_reduced[index_test]
            pipe.fit(Xtrain_i, ytrain_i, **fit_settings)
            ypred = pipe.predict(Xtest_i)
            score_i = sklearn.metrics.f1_score(y_train[index_test][SEQ_LEN-1:], ypred)
            score_hi.append(score_i)
 
        score.append(np.average(score_hi))
 
    return score

  score1 = do_hyper()
#+end_src

*** COMMENT spacing investigation
#+begin_src python :session py1
def fun1():
  a=[]
  for i in range(5):
    for j in range(3):
      a.append(j)
 
    a.append(i)
 
  return a

a = fun1()
#+end_src

#+RESULTS:

#+begin_src python :session py1
  a=[]
  for i in range(5):
    for j in range(3):
      a.append(j)
   
    a.append(i)
  
#+end_src

#+RESULTS:

** COMMENT UMAP and LSTM model
#+begin_src python :session py1
  umap_model = umap.UMAP(n_components=3)
  layers_dict = dict()
  #####################
  layers_dict['LSTM_1'] = dict(units=50, activation = 'elu', name='LSTM1')
  layers_dict['Dense_1'] = dict(units=1, activation='sigmoid', name='Output')
  #####################
  layers_tuple = ml4qf.utils.dict2tuple(layers_dict)
  #######################
  lstm_model = model_keras.Model_binary(keras_model='Sequential', layers=layers_tuple,
                                        seqlen=SEQ_LEN, optimizer_name='adam',
                                        loss_name='binary_crossentropy',
                                        metrics=['accuracy','binary_accuracy', 'mse'],
                                        optimizer_sett=None, compile_sett=None, loss_sett=None)

  pipe = sklearn.pipeline.Pipeline([('umap', umap_model),
                                    ('lstm', lstm_model)])

  pipe.fit(Xtrain_reduced, y_train, lstm__epochs=70, lstm__shuffle=False)

  # summary
  
#+end_src

*** DONE Classification
#+begin_src python :session py1
  y_test  = df_test.target.to_numpy()
  ypred_basemodel = pipe.predict(Xtest_reduced)#.reshape(len(y_test[SEQ_LEN-1:]))
  test_report = sklearn.metrics.classification_report(y_test[SEQ_LEN-1:], 
                                                      ypred_basemodel, output_dict=True)
  dftest_report = pd.DataFrame(test_report).transpose()
  print(dftest_report)

#+end_src


#+begin_src python :session py1
  ypred_basemodeltrain = pipe.predict(Xtrain_reduced)#.reshape(len(y_train[SEQ_LEN-1:]))
  train_report = sklearn.metrics.classification_report(y_train[SEQ_LEN-1:],
                                                       ypred_basemodeltrain, output_dict=True)
  dftrain_report = pd.DataFrame(train_report).transpose()
  print(dftrain_report)

#+end_src

* COMMENT Implementation

| Name | Description | Value |
|      |             |       |


['Std_23d', 'TSIs_13_25_13', 'MFI_60', 'sign_return_8d', 'VOLUME_EMA_10', 'VOLUME_EMA_15', 'STOCHd_14_3_3', 'BIAS_SMA_50', 'sign_return_3d', 'MFI_35', 'TRIXs_35_9', 'BIAS_SMA_75', 'TRUERANGE_1', 'momentum_1d', 'BBB_5_2.0', 'Ret_40d', 'UI_14', 'TRIXs_75_9', 'AROONU_60', 'TRIXs_20_9', 'volume_', 'Month_', 'HWL', 'sign_return_7d', 'Std_13d', 'Std_48d', 'PDIST', 'sign_return_1d', 'VOLUME_EMA_5', 'NATR_14', 'AROONU_10', 'CMF_60', 'Std_53d', 'THERMOl_20_2_0.5', 'Std_8d']
#+begin_src python :session py1
  import pickle
  with open("./data/hypertuning1.pickle", 'rb') as f1:
      ht1 = pickle.load(f1)
#+end_src
