#+PROPERTY: header-args :tangle ./airbus.py :mkdirp yes
* House keeping
#+begin_src elisp :results none :tangle no
  (add-to-list 'org-structure-template-alist
  '("sp" . "src python :session py1"))
  (add-to-list 'org-structure-template-alist
  '("se" . "src elisp"))

  (setq org-confirm-babel-evaluate nil)
#+end_src

#+begin_src emacs-lisp  :session py1 :results none :tangle nil
  (pyvenv-workon "qfpy")
  (require 'org-tempo)
#+end_src

#+begin_src python  :session py1 :results none
  ##############################################################################
  # Add ml4qf library to python path.                                          #
  #                                                                            #
  # Preferable to do export PYTHONPATH="$PYTHONPATH:{path}" from command line  #
  ##############################################################################

  import sys
  import pathlib
  file_path = sys.path[1]
  sys.path.append(file_path + "/../")
#+end_src

* Import libraries
#+BEGIN_SRC python :session py1 :results output silent

  ################################
  # Import libraries and modules #
  ################################

  import numpy as np
  import pathlib
  import yfinance as yf
  from tabulate import tabulate

  # from umap import UMAP
  # import matplotlib.pyplot as plt
  # from mpl_toolkits.mplot3d import Axes3D
  # import matplotlib
  # matplotlib.rcParams['figure.dpi'] = 80

  import minisom
  import umap
  from sklearn.model_selection import train_test_split
  import sklearn.metrics
  import scipy.optimize
  import sklearn.compose
  import sklearn.pipeline
  from scikeras.wrappers import KerasClassifier
  import tensorflow.keras.utils
  import plotly.express as px
  import scipy.stats
  import pandas as pd
  import pandas_ta as ta

  import ml4qf
  import ml4qf.utils
  import ml4qf.predictors.model_som
  import ml4qf.collectors.financial_features
  import ml4qf.transformers
  import ml4qf.predictors.model_keras as model_keras
  import ml4qf.predictors.model_tuning
#+END_SRC

#+begin_src python :session py1 :results none
  #######################################
  # Set random seed for keras and numpy #
  #######################################
  tensorflow.keras.utils.set_random_seed(42)
#+end_src
* Introduction

* Feature engineering

#+begin_src python :session py1 :tangle none
  FEATURES1 = {'momentum_': [1, 2, 5, 8, 15, 23],
               'OC_': None,
               'HL_': None,
               'Ret_': [1, 5, 30, 35],
               #'RetLog_': list(range(10, 90, 10)),
               'Std_': list(range(10, 90, 10)),
               'MA_': [5, 10, 25, 50],
               'EMA_': [5, 10, 25, 50],
               'sign_return_': list(range(1,10)),
               'volume_':None,
               "ema": {"close": "volume", "length": [5, 10, 20], "prefix": "VOLUME"},
               "log_return": {"length": [1, 20], "cumulative": True},
               "ohlc4":{},
               "volatility":"whole"
               #"momentum":"whole"
               #"percent_return": dict(length=2, append=False),
               #"log_return": [1, 2]
               }

#+end_src

#+RESULTS:

#+begin_src python :session py1 :results none 
  # 
  ###########################################
  # Get data and create Features            #
  # Ticker: EADSY (Airbus)                  #
  # Ten years from October 2019 backwards   #
  ###########################################

  FEATURES1 = {'Month_': None,
      'momentum_': [1, 2, 5, 8, 15, 23, 30, 40, 65, 75],
      'OC_': None,
      'HL_': None,
      'Ret_': [1, 5, 15, 20, 25, 30, 40, 50, 60, 75],
      #'RetLog_': list(range(10, 90, 10)),
      'Std_': list(range(3, 75, 5)),
      'MA_': [5, 10, 18, 25, 35, 50, 60, 70],
      'EMA_': [5, 10, 18, 25, 35, 50, 60, 70],
      'sign_return_': list(range(1,10)),
      'volume_':None,
      "ema": {"close": "volume", "length": [5, 10, 15, 20, 30], "prefix": "VOLUME"},
      "log_return": {"length": [1, 10, 20, 35, 50], "cumulative": True},
      "bias":{"length": [5, 10, 15, 20, 35, 50, 60, 75]},
      "aroon":{"length": [5, 10, 15, 20, 35, 50, 60, 75]},
      "stoch":{},
      "trix":{"length": [5, 10, 15, 20, 35, 50, 60, 75]},
      "tsi":{},
      #"volume---":{},
      "efi":{},
      "mfi": {"length": [5, 10, 15, 20, 35, 50, 60, 75]},
      "cmf":{"length": [5, 10, 15, 20, 35, 50, 60, 75]},
      "ohlc4":{},
      "volatility":"whole"
      #"momentum":"whole"
      #"percent_return": dict(length=2, append=False),
      #"log_return": [1, 2]
  }

#+end_src

#+begin_src python :session py1 :results none 
  # 
  ###########################################
  # Get data and create Features            #
  # Ticker: EADSY (Airbus)                  #
  # Ten years from October 2019 backwards   #
  ###########################################

  data = ml4qf.collectors.financial_features.FinancialData("EADSY", 2019, 10, 1, 365*16, FEATURES1)
  img_dir = "./img/" + data.label
  pathlib.Path(img_dir).mkdir(parents=True, exist_ok=True)
  df_  = data.features.df.drop(data.df.columns, axis=1)
  df_.dropna(inplace=True)

#+end_src


#+begin_src python :session py1 :results output

  own_features = list(df_.columns[:df_.columns.get_loc('volume_')+1])
  pa_features = list(df_.columns[df_.columns.get_loc('volume_') + 1:df_.columns.get_loc('OHLC4')+1])
  pa_volfeatures = list(df_.columns[df_.columns.get_loc('OHLC4')+1:])
  total_features = len(own_features) + len(pa_features) + len(pa_volfeatures)
  print("######################")
  print(own_features)
  print("######################")
  print(pa_features)
  print("######################")
  print(pa_volfeatures)
  print("######################")
  assert total_features == len(df_.columns), "Number of features not matching in dataframe"
#+end_src

#+RESULTS:
: ######################
: ['Month_', 'momentum_1d', 'momentum_2d', 'momentum_5d', 'momentum_8d', 'momentum_15d', 'momentum_23d', 'momentum_30d', 'momentum_40d', 'momentum_65d', 'momentum_75d', 'OC_', 'HL_', 'Ret_1d', 'Ret_5d', 'Ret_15d', 'Ret_20d', 'Ret_25d', 'Ret_30d', 'Ret_40d', 'Ret_50d', 'Ret_60d', 'Ret_75d', 'Std_3d', 'Std_8d', 'Std_13d', 'Std_18d', 'Std_23d', 'Std_28d', 'Std_33d', 'Std_38d', 'Std_43d', 'Std_48d', 'Std_53d', 'Std_58d', 'Std_63d', 'Std_68d', 'Std_73d', 'MA_5d', 'MA_10d', 'MA_18d', 'MA_25d', 'MA_35d', 'MA_50d', 'MA_60d', 'MA_70d', 'EMA_5d', 'EMA_10d', 'EMA_18d', 'EMA_25d', 'EMA_35d', 'EMA_50d', 'EMA_60d', 'EMA_70d', 'sign_return_1d', 'sign_return_2d', 'sign_return_3d', 'sign_return_4d', 'sign_return_5d', 'sign_return_6d', 'sign_return_7d', 'sign_return_8d', 'sign_return_9d', 'volume_']
: ######################
: ['VOLUME_EMA_5', 'VOLUME_EMA_10', 'VOLUME_EMA_15', 'VOLUME_EMA_20', 'VOLUME_EMA_30', 'CUMLOGRET_1', 'CUMLOGRET_10', 'CUMLOGRET_20', 'CUMLOGRET_35', 'CUMLOGRET_50', 'BIAS_SMA_5', 'BIAS_SMA_10', 'BIAS_SMA_15', 'BIAS_SMA_20', 'BIAS_SMA_35', 'BIAS_SMA_50', 'BIAS_SMA_60', 'BIAS_SMA_75', 'AROOND_5', 'AROONU_5', 'AROONOSC_5', 'AROOND_10', 'AROONU_10', 'AROONOSC_10', 'AROOND_15', 'AROONU_15', 'AROONOSC_15', 'AROOND_20', 'AROONU_20', 'AROONOSC_20', 'AROOND_35', 'AROONU_35', 'AROONOSC_35', 'AROOND_50', 'AROONU_50', 'AROONOSC_50', 'AROOND_60', 'AROONU_60', 'AROONOSC_60', 'AROOND_75', 'AROONU_75', 'AROONOSC_75', 'STOCHk_14_3_3', 'STOCHd_14_3_3', 'TRIX_5_9', 'TRIXs_5_9', 'TRIX_10_9', 'TRIXs_10_9', 'TRIX_15_9', 'TRIXs_15_9', 'TRIX_20_9', 'TRIXs_20_9', 'TRIX_35_9', 'TRIXs_35_9', 'TRIX_50_9', 'TRIXs_50_9', 'TRIX_60_9', 'TRIXs_60_9', 'TRIX_75_9', 'TRIXs_75_9', 'TSI_13_25_13', 'TSIs_13_25_13', 'EFI_13', 'MFI_5', 'MFI_10', 'MFI_15', 'MFI_20', 'MFI_35', 'MFI_50', 'MFI_60', 'MFI_75', 'CMF_5', 'CMF_10', 'CMF_15', 'CMF_20', 'CMF_35', 'CMF_50', 'CMF_60', 'CMF_75', 'OHLC4']
: ######################
: ['ABER_ZG_5_15', 'ABER_SG_5_15', 'ABER_XG_5_15', 'ABER_ATR_5_15', 'ACCBL_20', 'ACCBM_20', 'ACCBU_20', 'ATRr_14', 'BBL_5_2.0', 'BBM_5_2.0', 'BBU_5_2.0', 'BBB_5_2.0', 'BBP_5_2.0', 'DCL_20_20', 'DCM_20_20', 'DCU_20_20', 'HWM', 'HWU', 'HWL', 'KCLe_20_2', 'KCBe_20_2', 'KCUe_20_2', 'MASSI_9_25', 'NATR_14', 'PDIST', 'RVI_14', 'THERMO_20_2_0.5', 'THERMOma_20_2_0.5', 'THERMOl_20_2_0.5', 'THERMOs_20_2_0.5', 'TRUERANGE_1', 'UI_14']
: ######################

** Exploratory data analysis


#+begin_src python :session py1 :results file
  fig1_path= img_dir +'/stock_Close.png'
  fig1 = px.line(df_, y=['Ret_1d', 'Ret_5d', 'Ret_15d'])
  fig1.write_image(fig1_path)
  fig1_path
#+end_src

#+RESULTS:
[[file:./img/_EADSY_2003-10-05_2019-10-01/stock_Close.png]]

*** Correlation

#+begin_src python :session py1 :results file
  fig1_path= img_dir +'/correlation.png'
  df_corr = df_.corr().round(2)
  fig1 = px.imshow(np.abs(df_corr))
  fig1.layout.height = 600
  fig1.layout.width = 600
  fig1.write_image(fig1_path)
  fig1_path
#+end_src

#+RESULTS:
[[file:./img/_EADSY_2009-10-03_2019-10-01/correlation.png]]

** Label 

*** Class imbalance

#+begin_src python :session py1
alpha_min = scipy.optimize.bisect(ml4qf.utils.fix_imbalance, -0.01, 0.01, args=(data, df_.index))
df_['target'] = np.where(data.df.loc[df_.index]['returns'].shift(-1) > alpha_min, 1, 0)
df_.target.value_counts()
#+end_src

#+RESULTS:
: 0    1487
: 1    1486
: Name: target, dtype: int64


** Feature scaling

https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02


#+begin_src python :session py1 :results output 
zscores = np.abs(scipy.stats.zscore(df_)).max()
print(zscores)
#+end_src

#+RESULTS:
#+begin_example
Month_               1.618382
momentum_1d          9.797553
momentum_2d          8.325245
momentum_5d          6.923864
momentum_8d          6.349095
                      ...    
THERMOl_20_2_0.5     3.453417
THERMOs_20_2_0.5     1.720094
TRUERANGE_1         11.346320
UI_14                5.719633
target               1.000336
Length: 177, dtype: float64
#+end_example

    #+begin_src python :session py1:var tbl=zscore6 :colnames no :results output
    import pandas as pd
    df = pd.DataFrame(tbl[1:], columns=tbl[0])
    print(df, "\n")
    print(df["Patients"].mean())
    #+end_src

    #+RESULTS:
    : Python 3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:23:14) [GCC 10.4.0] on linux
    : Type "help", "copyright", "credits" or "license" for more information.
    : Traceback (most recent call last):
    :   File "<stdin>", line 1, in <module>
    :   File "/tmp/babel-E0LHzc/python-62CCU3", line 2, in <module>
    :     df = pd.DataFrame(tbl[1:], columns=tbl[0])
    : NameError: name 'tbl' is not defined
    : python.el: native completion setup loaded

#+begin_src python :session py1 :results output
  zscore5 = zscores[np.where(zscores>5)[0]]
  zscore6 = zscores[np.where(zscores>6)[0]]
  momentum = ['momentum_%sd'%i for i in FEATURES1['momentum_']]
  robust_scaler = set(zscore7.keys()).union(momentum) - set(['Ret_1d', 'Ret_5d','Std_3d', 'Std_8d'])
  #robustscaler += momentum
  print(zscore6)
#+end_src

#+RESULTS:
#+begin_example
momentum_1d         9.797553
momentum_2d         8.325245
momentum_5d         6.923864
momentum_8d         6.349095
OC_                 7.111603
HL_                17.900512
Ret_1d              8.650474
Ret_5d              7.179704
Std_3d              7.467710
Std_8d              6.306172
volume_            25.036073
VOLUME_EMA_5       17.933932
VOLUME_EMA_10      13.547161
VOLUME_EMA_15      10.946547
VOLUME_EMA_20       9.275857
VOLUME_EMA_30       7.269476
BIAS_SMA_5          6.245809
EFI_13              8.728109
BBB_5_2.0           7.599345
PDIST              15.234062
THERMO_20_2_0.5    12.317750
TRUERANGE_1        11.346320
dtype: float64
#+end_example

#+begin_src python :session py1 :results none

  transformers = {'SeasonTransformer':{'features': ['Month_']},
                  'MinMaxScaler': {'features': ['sign_return']},
                  'RobustScaler': {'features': robust_scaler},
                  'StandardScaler_1': {'features': ['EMA', 'MA', 'Std', 'Ret']},
                  'StandardScaler_2': {'features': list(set(pa_volfeatures) - robust_scaler)},
                  'StandardScaler_3': {'features': list(set(pa_features) - robust_scaler)}
                  }

  columns = ml4qf.transformers.build_transformation(df_, transformers)
  columns_validation = ml4qf.transformers.build_transformation(df_, transformers)
  ct = sklearn.compose.ColumnTransformer(columns, remainder='passthrough')
  #ct_validation = sklearn.compose.ColumnTransformer(columns, remainder='passthrough')

#+end_src

#+begin_src python :results value raw :session py1 :return tabulate(df, headers=df.columns, tablefmt='orgtbl')
df = pd.DataFrame({
    "a": [1,2,3],
    "b": [4,5,6]
})
#+end_src

#+RESULTS:


*** Split data
#+begin_src python :session py1 :results output
  Xtrain, Xtest = train_test_split(df_.to_numpy(), train_size=0.8, shuffle=False)
  len_train = len(Xtrain)
  len_test = len(Xtest)
  df_train = df_.iloc[:len_train, :]
  df_test = df_.iloc[len_train:, :]
  Xtrain_scaled = ct.fit_transform(df_train)
  Xtrain_scaled = ml4qf.transformers.swap_features(Xtrain_scaled, df_train, ct)
  Xtest_scaled = ct.transform(df_test)
  Xtest_scaled = ml4qf.transformers.swap_features(Xtest_scaled, df_test, ct)
  df_train_scaled = ml4qf.transformers.scale_df(df_train, columns_validation)
  assert (Xtrain_scaled == df_train_scaled.to_numpy()).all(), "scaling failed"
  #Xtrain_scaled = ct.transform(Xtrain)
  assert len([i for i in ct.get_feature_names_out() if i[:9]=='remainder']) == 1, "some scaling missing"
#+end_src

#+RESULTS:

** SOM

#+begin_src python :session py1 :results output
  som_labels = None
  #som_labels = ['volume_', 'THERMOma_20_2_0.5', 'momentum_1d', 'VOLUME_EMA_20', 'VOLUME_EMA_10', 'Std_73d', 'Std_48d', 'TRUERANGE_1', 'sign_return_1d', 'EMA_5d', 'TRIX_20_9', 'Month_', 'target', 'Std_58d', 'Std_8d', 'HWL', 'sign_return_7d', 'NATR_14', 'Std_28d', 'momentum_23d', 'TRIXs_50_9', 'BBB_5_2.0', 'sign_return_4d', 'Std_38d', 'TRIX_35_9', 'THERMOl_20_2_0.5', 'sign_return_3d', 'AROOND_35', 'VOLUME_EMA_15', 'VOLUME_EMA_5', 'THERMO_20_2_0.5']
  if som_labels is None:
    som_size = 50
    som_obj = ml4qf.predictors.model_som.Model(som_size, som_size, Xtrain_scaled, sigma=1.5, learning_rate=0.1, 
                                               neighborhood_function='gaussian', num_iter=10000, random_seed=42)
    som_labels = som_obj.iterate_som_selection(min_num_features=30, labels=list(df_train.columns), a_range=[0.01, 0.03, 0.05, 0.08, 0.1, 0.2], num_iterations=30)
  print(som_labels)
#+end_src

#+RESULTS:
: /home/acea/anaconda3/envs/qfpy/lib/python3.10/site-packages/minisom.py:379: ComplexWarning: Casting complex values to real discards the imaginary part
:   self._weights[i, j] = c1*pc[pc_order[0]] + c2*pc[pc_order[1]]
: Total number of iterations: 8
: ['volume_', 'VOLUME_EMA_20', 'VOLUME_EMA_10', 'EFI_13', 'momentum_1d', 'BIAS_SMA_10', 'Std_68d', 'BBU_5_2.0', 'HL_', 'momentum_2d', 'TRIXs_50_9', 'AROONU_5', 'momentum_75d', 'sign_return_3d', 'VOLUME_EMA_30', 'TRIXs_15_9', 'CMF_5', 'Ret_60d', 'Ret_50d', 'BIAS_SMA_35', 'VOLUME_EMA_5', 'Std_23d', 'CMF_50', 'sign_return_2d', 'THERMO_20_2_0.5', 'Std_28d', 'THERMOl_20_2_0.5', 'ATRr_14', 'NATR_14', 'THERMOs_20_2_0.5', 'TRIXs_5_9', 'UI_14', 'sign_return_6d']


#+begin_src python :session py1 :results file
  # for i, f in enumerate(feature_names):
  #     plt.subplot(3, 3, i+1)
  #     plt.title(f)
  #     plt.pcolor(W[:,:,i].T, cmap='coolwarm')
  #     plt.xticks(np.arange(size+1))
  #     plt.yticks(np.arange(size+1))
  # plt.tight_layout()
  # plt.show()
  fig1_path= img_dir +'/som.png'
  fig1 = px.imshow(som_obj.W[:20,30:,20].T)
  fig1.layout.height = 1000
  fig1.layout.width = 1000
  fig1.write_image(fig1_path)
  fig1_path
#+end_src

#+RESULTS:
[[file:./img/_EADSY_2009-10-03_2019-10-01/som.png]]

#+end_src

*** Reduced model

#+begin_src python :session py1 :results output
  index_reducedlabels = [df_train.columns.get_loc(i) for i in som_labels]
  dftrain_reduced = df_train[som_labels]
  dftest_reduced = df_test[som_labels]
  assert (dftrain_reduced.to_numpy() == Xtrain[:, index_reducedlabels]).all(), "Reduced matrix not maching dimensions"
  Xtrain_reduced = Xtrain_scaled[:, index_reducedlabels]
  Xtest_reduced = Xtest_scaled[:, index_reducedlabels]
  #Xtest_reduced = Xtest_scaled[:, index_reducedlabels]
#+end_src

#+RESULTS:

* LSTM design
#+begin_src python :session py1
  SEQ_LEN = 15
  y_train = df_train['target'].to_numpy()
  y_test  = df_test.target.to_numpy()

#+end_src

#+RESULTS:

** COMMENT Base line model
#+begin_src python :session py1
  layers_dict = dict()
  ############
  # layers_dict['LSTM'] = dict(units=5, activation = 'relu', return_sequences=False, name='LSTM')
  # layers_dict['Dense'] = dict(units=1, name='Output')
  ############
  # layers_dict['LSTM_1'] = dict(units=100*2, activation = 'elu', return_sequences=True, name='LSTM1')
  # layers_dict['Dropout_1'] = dict(rate=0.4, name='Drouput1')
  # layers_dict['LSTM_2'] = dict(units=100, activation = 'elu', return_sequences=True, name='LSTM2')
  # layers_dict['Dropout_2'] = dict(rate=0.4, name='Drouput2')
  # layers_dict['LSTM_3'] = dict(units=100, activation = 'elu', return_sequences=False, name='LSTM3')
  # layers_dict['Dense_1'] = dict(units=1, activation='sigmoid', name='Output')
  #####################
  ############
  layers_dict['LSTM_1'] = dict(units=100, activation = 'elu', return_sequences=True, name='LSTM1')
  layers_dict['LSTM_2'] = dict(units=100, activation = 'elu', return_sequences=False, name='LSTM2')
  layers_dict['Dense_1'] = dict(units=1, activation='sigmoid', name='Output')
  #####################
  # layers_dict['LSTM_1'] = dict(units=50, activation = 'elu', name='LSTM1')
  # layers_dict['Dense_1'] = dict(units=1, activation='sigmoid', name='Output')
  #####################
  winner = {'batch_size': 16, 'layers': (('LSTM_1', (('units', 70), ('activation', 'relu'), ('return_sequences', True), ('name', 'LSTM1'))), ('Dropout_1', (('rate', 0.5), ('name', 'Drouput1'))), ('LSTM_2', (('units', 50), ('activation', 'relu'), ('return_sequences', False), ('name', 'LSTM2'))), ('Dense_1', (('units', 1), ('activation', 'sigmoid'), ('name', 'Output')))), 'optimizer_name': 'adam', 'seqlen': 30}
  ####################
  layers_tuple = ml4qf.utils.dict2tuple(layers_dict)
  #######################
  base_model = model_keras.Model_binary(keras_model='Sequential', layers=layers_tuple,
                                        seqlen=SEQ_LEN, optimizer_name='adam',
                                        loss_name='binary_crossentropy',
                                        metrics=['accuracy','binary_accuracy', 'mse'],
                                        optimizer_sett=None, compile_sett=None, loss_sett=None)
  base_model.set_params(**winner)
  base_model.fit(Xtrain_reduced, y_train, epochs=100, shuffle=False, verbose=1)

  # summary
  #base_model._model.summary()

#+end_src

#+RESULTS:
#+begin_example
Model_binary(batch_size=16,
             layers=(('LSTM_1',
                      (('units', 70), ('activation', 'relu'),
                       ('return_sequences', True), ('name', 'LSTM1'))),
                     ('Dropout_1', (('rate', 0.5), ('name', 'Drouput1'))),
                     ('LSTM_2',
                      (('units', 50), ('activation', 'relu'),
                       ('return_sequences', False), ('name', 'LSTM2'))),
                     ('Dense_1',
                      (('units', 1), ('activation', 'sigmoid'),
                       ('name', 'Output')))),
             metrics=['accuracy', 'binary_accuracy', 'mse'], seqlen=30)
#+end_example



*** Classification
#+begin_src python :session py1
  ypred_basemodel = base_model.predict(Xtest_reduced, y_test)#.reshape(len(y_test[SEQ_LEN-1:]))
  test_report = sklearn.metrics.classification_report(base_model.ypred_generated_, 
                                                      ypred_basemodel, output_dict=True)
  dftest_report = pd.DataFrame(test_report).transpose()
  print(dftest_report)

#+end_src

#+RESULTS:


#+begin_src python :session py1
  ypred_basemodeltrain = base_model.predict(Xtrain_reduced, y_train)#.reshape(len(y_train[SEQ_LEN-1:]))
  train_report = sklearn.metrics.classification_report(base_model.ypred_generated_,
                                                       ypred_basemodeltrain, output_dict=True)
  dftrain_report = pd.DataFrame(train_report).transpose()
  print(dftrain_report)

#+end_src

#+RESULTS:

* Cross validation

** LSTM model design

*** Searcher
#+begin_src python :session py1
  lstm_model = model_keras.Model_binary(keras_model='Sequential',
                                        seqlen=SEQ_LEN, optimizer_name='adam',
                                        loss_name='binary_crossentropy',
                                        metrics=['accuracy','binary_accuracy'],
                                        optimizer_sett=None, compile_sett=None, loss_sett=None)

#+end_src

#+RESULTS:
| memory | : | hline | steps | : | ((lstm Model_binary (metrics= (accuracy binary_accuracy mse) seqlen=30))) | verbose | : | False | lstm | : | Model_binary | (metrics= (accuracy binary_accuracy mse) seqlen=30) | lstm__keras_model | : | Sequential | lstm__layers | : | nil | lstm__seqlen | : | 30 | lstm__optimizer_name | : | adam | lstm__optimizer_sett | : | hline | lstm__compile_sett | : | hline | lstm__loss_sett | : | hline | lstm__loss_name | : | binary_crossentropy | lstm__metrics | : | (accuracy binary_accuracy mse) | lstm__timeseries_sett | : | hline |


#+begin_src python :session py1 
  searcher_name = 'GridSearchCV'
  layers_hyper = []
  ###########
  layers_dict = dict()
  layers_dict['LSTM_1'] = dict(units=100, activation = 'relu', name='LSTM1')
  layers_dict['Dense_1'] = dict(units=1, activation='sigmoid', name='Output')
  layers_tuple = ml4qf.utils.dict2tuple(layers_dict)
  layers_hyper.append(layers_tuple)
  #####################
  layers_dict = dict()
  layers_dict['LSTM_1'] = dict(units=50, activation = 'elu', return_sequences=True, name='LSTM1')
  layers_dict['LSTM_2'] = dict(units=50, activation = 'elu', return_sequences=False, name='LSTM2')
  layers_dict['Dense_1'] = dict(units=1, activation='sigmoid', name='Output')
  layers_tuple = ml4qf.utils.dict2tuple(layers_dict)
  layers_hyper.append(layers_tuple)
  #####################
  layers_dict = dict()
  layers_dict['LSTM_1'] = dict(units=70, activation = 'relu', return_sequences=True, name='LSTM1')
  layers_dict['Dropout_1'] = dict(rate=0.5, name='Drouput1')
  layers_dict['LSTM_2'] = dict(units=50, activation = 'relu', return_sequences=False, name='LSTM2')
  layers_dict['Dense_1'] = dict(units=1, activation='sigmoid', name='Output')
  layers_tuple = ml4qf.utils.dict2tuple(layers_dict)
  layers_hyper.append(layers_tuple)
  ############
  layers_dict = dict()
  layers_dict['LSTM_1'] = dict(units=50, activation = 'elu', return_sequences=True, name='LSTM1')
  layers_dict['LSTM_2'] = dict(units=40, activation = 'relu', return_sequences=True, name='LSTM2')
  layers_dict['LSTM_3'] = dict(units=30, activation = 'elu', return_sequences=False, name='LSTM3')
  layers_dict['Dense_1'] = dict(units=1, activation='sigmoid', name='Output')
  layers_tuple = ml4qf.utils.dict2tuple(layers_dict)
  layers_hyper.append(layers_tuple)
  #####################
  layers_dict = dict()
  layers_dict['LSTM_1'] = dict(units=50, activation = 'elu', return_sequences=True, name='LSTM1')
  layers_dict['Dropout_1'] = dict(rate=0.35, name='Drouput1')
  layers_dict['LSTM_2'] = dict(units=25, activation = 'elu', return_sequences=True, name='LSTM2')
  layers_dict['Dropout_2'] = dict(rate=0.35, name='Drouput2')
  layers_dict['LSTM_3'] = dict(units=25, activation = 'elu', return_sequences=False, name='LSTM3')
  layers_dict['Dense_1'] = dict(units=1, activation='sigmoid', name='Output')
  layers_tuple = ml4qf.utils.dict2tuple(layers_dict)
  layers_hyper.append(layers_tuple)
  #####################

  ###########
  hyper_grid = {'seqlen':[5, 15, 30, 45, 75],
                'layers':layers_hyper,
                'optimizer_name':['adam', 'adamax'],
                'batch_size': [8, 16, 32,64,128]
                }
  searcher_settings = {#'scoring':'f1',
                       #'n_iter':25,
                       'n_jobs':7,
                       'verbose': False}
  cv_name = 'TimeSeriesSplit'
  cv_settings = {'n_splits': 2}
  _hypertuning1 = ml4qf.predictors.model_tuning.HyperTuning(lstm_model, searcher_name, searcher_settings,
                                                            hyper_grid, cv_name, cv_settings)
  hypertuning1 = _hypertuning1()
  hypertuning1.fit(Xtrain_reduced, y_train, epochs=85, shuffle=False)

#+end_src

*** COMMENT grid itertools
#+begin_src python :session py1 
  import tensorflow.keras.backend
  import itertools
  umap_model = umap.UMAP()
  lstm_model = model_keras.Model_binary(keras_model='Sequential',
                                        seqlen=SEQ_LEN, optimizer_name='adam',
                                        loss_name='binary_crossentropy',
                                        metrics=['accuracy','binary_accuracy', 'mse'],
                                        optimizer_sett=None, compile_sett=None, loss_sett=None)
  pipe = sklearn.pipeline.Pipeline([('umap', umap_model),
                                    ('lstm', lstm_model)])

  searcher_name = 'RandomizedSearchCV'
  layers_hyper = []
  ###########
  layers_dict = dict()
  layers_dict['LSTM_1'] = dict(units=100, activation = 'elu', name='LSTM1')
  layers_dict['Dense_1'] = dict(units=1, activation='sigmoid', name='Output')
  layers_tuple = ml4qf.utils.dict2tuple(layers_dict)
  layers_hyper.append(layers_tuple)
  #####################
  layers_dict = dict()
  layers_dict['LSTM_1'] = dict(units=50, activation = 'elu', return_sequences=True, name='LSTM1')
  layers_dict['LSTM_2'] = dict(units=50, activation = 'elu', return_sequences=False, name='LSTM2')
  layers_dict['Dense_1'] = dict(units=1, activation='sigmoid', name='Output')
  layers_tuple = ml4qf.utils.dict2tuple(layers_dict)
  layers_hyper.append(layers_tuple)
  ############
  layers_dict = dict()
  layers_dict['LSTM_1'] = dict(units=50, activation = 'elu', return_sequences=True, name='LSTM1')
  layers_dict['Dropout_1'] = dict(rate=0.3, name='Drouput1')
  layers_dict['LSTM_2'] = dict(units=25, activation = 'elu', return_sequences=True, name='LSTM2')
  layers_dict['Dropout_2'] = dict(rate=0.3, name='Drouput2')
  layers_dict['LSTM_3'] = dict(units=25, activation = 'elu', return_sequences=False, name='LSTM3')
  layers_dict['Dense_1'] = dict(units=1, activation='sigmoid', name='Output')
  layers_tuple = ml4qf.utils.dict2tuple(layers_dict)
  layers_hyper.append(layers_tuple)
  #####################
  def product_dict(**kwargs):
    keys = kwargs.keys()
    vals = kwargs.values()
    for instance in itertools.product(*vals):
        yield dict(zip(keys, instance))

  ###########
  hyper_grid = {#'umap':dict(n_neighbors=[5, 15, 30, 50, 100],
                #            n_components=[3, 8, 15, 30],
                #            min_dist=[0.05, 0.1, 0.4, 0.75],
                #            random_state=42),
                'umap__n_neighbors':[30],    
                'umap__n_components':[18],         
                'umap__min_dist':[0.05],     
                'umap__random_state':[42],                    
                #'lstm__seqlen':[10, 25],
                'lstm__layers':[layers_hyper[0]],
                'lstm__optimizer_name':['adam']
                }
  searcher_settings = {'scoring':'f1',
                       'n_iter':25,
                       'verbose': True}
  fit_settings = {'lstm__epochs':150, 'lstm__shuffle':False}
  cv_name = 'TimeSeriesSplit'
  cv_settings = {'n_splits': 3}
  _hypertuning1 = ml4qf.predictors.model_tuning.HyperTuning(pipe, searcher_name, searcher_settings,
                                                            hyper_grid, cv_name, cv_settings)
  hypertuning1 = _hypertuning1()
  hyperspace = list(product_dict(**hyper_grid))
  
#+end_src

#+RESULTS:

#+begin_src python :session py1
  def do_hyper():
    score = []
    for hi in hyperspace:
        tensorflow.keras.backend.clear_session()
        pipe.set_params(**hi)
        score_hi = []
        for cvi in hypertuning1.cv.split(Xtrain_reduced):
            index_train, index_test = cvi
            Xtrain_i = Xtrain_reduced[index_train]
            ytrain_i = y_train[index_train]
            Xtest_i = Xtrain_reduced[index_test]
            pipe.fit(Xtrain_i, ytrain_i, **fit_settings)
            ypred = pipe.predict(Xtest_i)
            score_i = sklearn.metrics.f1_score(y_train[index_test][SEQ_LEN-1:], ypred)
            score_hi.append(score_i)
 
        score.append(np.average(score_hi))
 
    return score

  score1 = do_hyper()
#+end_src

*** COMMENT spacing investigation
#+begin_src python :session py1
def fun1():
  a=[]
  for i in range(5):
    for j in range(3):
      a.append(j)
 
    a.append(i)
 
  return a

a = fun1()
#+end_src

#+RESULTS:

#+begin_src python :session py1
  a=[]
  for i in range(5):
    for j in range(3):
      a.append(j)
   
    a.append(i)
  
#+end_src

#+RESULTS:

** COMMENT UMAP and LSTM model
#+begin_src python :session py1
  umap_model = umap.UMAP(n_components=3)
  layers_dict = dict()
  #####################
  layers_dict['LSTM_1'] = dict(units=50, activation = 'elu', name='LSTM1')
  layers_dict['Dense_1'] = dict(units=1, activation='sigmoid', name='Output')
  #####################
  layers_tuple = ml4qf.utils.dict2tuple(layers_dict)
  #######################
  lstm_model = model_keras.Model_binary(keras_model='Sequential', layers=layers_tuple,
                                        seqlen=SEQ_LEN, optimizer_name='adam',
                                        loss_name='binary_crossentropy',
                                        metrics=['accuracy','binary_accuracy', 'mse'],
                                        optimizer_sett=None, compile_sett=None, loss_sett=None)

  pipe = sklearn.pipeline.Pipeline([('umap', umap_model),
                                    ('lstm', lstm_model)])

  pipe.fit(Xtrain_reduced, y_train, lstm__epochs=70, lstm__shuffle=False)

  # summary
  
#+end_src

*** DONE Classification
#+begin_src python :session py1
  y_test  = df_test.target.to_numpy()
  ypred_basemodel = pipe.predict(Xtest_reduced)#.reshape(len(y_test[SEQ_LEN-1:]))
  test_report = sklearn.metrics.classification_report(y_test[SEQ_LEN-1:], 
                                                      ypred_basemodel, output_dict=True)
  dftest_report = pd.DataFrame(test_report).transpose()
  print(dftest_report)

#+end_src


#+begin_src python :session py1
  ypred_basemodeltrain = pipe.predict(Xtrain_reduced)#.reshape(len(y_train[SEQ_LEN-1:]))
  train_report = sklearn.metrics.classification_report(y_train[SEQ_LEN-1:],
                                                       ypred_basemodeltrain, output_dict=True)
  dftrain_report = pd.DataFrame(train_report).transpose()
  print(dftrain_report)

#+end_src

* COMMENT Implementation

| Name | Description | Value |
|      |             |       |


['Std_23d', 'TSIs_13_25_13', 'MFI_60', 'sign_return_8d', 'VOLUME_EMA_10', 'VOLUME_EMA_15', 'STOCHd_14_3_3', 'BIAS_SMA_50', 'sign_return_3d', 'MFI_35', 'TRIXs_35_9', 'BIAS_SMA_75', 'TRUERANGE_1', 'momentum_1d', 'BBB_5_2.0', 'Ret_40d', 'UI_14', 'TRIXs_75_9', 'AROONU_60', 'TRIXs_20_9', 'volume_', 'Month_', 'HWL', 'sign_return_7d', 'Std_13d', 'Std_48d', 'PDIST', 'sign_return_1d', 'VOLUME_EMA_5', 'NATR_14', 'AROONU_10', 'CMF_60', 'Std_53d', 'THERMOl_20_2_0.5', 'Std_8d']
