#+TITLE: Portfolio Construction using Black-Litterman Model and Factors
#+AUTHOR: Alvaro Cea
#+PROPERTY: header-args :tangle ./main.py :mkdirp yes
#+LATEX_HEADER: \usepackage[margin=1in]{geometry}
#+LATEX_HEADER: \usepackage{mathtools}
#+OPTIONS: broken-links:tc
#+begin_comment
#+OPTIONS: toc:nil
#+LATEX_HEADER: \let\oldsection\section
#+LATEX_HEADER: \renewcommand{\section}{\clearpage\oldsection}
#+LATEX_HEADER: \let\oldsubsection\subsection
#+LATEX_HEADER: \renewcommand{\subsection}{\clearpage\oldsubsection}
#+end_comment

* House keeping :noexport:
#+begin_src elisp :results none :exports none
  (add-to-list 'org-structure-template-alist
  '("sp" . "src python :session py1"))
  (add-to-list 'org-structure-template-alist
  '("se" . "src elisp"))

  (setq org-confirm-babel-evaluate nil)
  (pyvenv-workon "ml4qf")
  (require 'org-tempo)
  (setq org-format-latex-options (plist-put org-format-latex-options :scale 2.0))
  (setq org-latex-pdf-process (list "latexmk -f -synctex=1 -pdf %f"))
  ;; (setq org-latex-pdf-process (list "latexmk -f -pdf -interaction=nonstopmode -output-directory=%o %f"))

#+end_src

#+begin_src python :session py1 :tangle yes :results none :exports none
  import pandas as pd
  import numpy as np
  import yfinance as yf
  import statsmodels.api as sm
  import getFamaFrenchFactors as gff
  import pathlib
  import datetime
  import importlib
  import ml4qf
  import ml4qf.collectors.financial_features as financial_features
  import ml4qf.collectors.financial_factors as financial_factors
  import ml4qf.collectors.financial_markets as financial_markets
  from ml4qf.predictors.model_stats import regression_OLS
  import ml4qf.predictors.model_stats as model_stats
  import ml4qf.portfolios.blacklitterman as bl
  import ml4qf.portfolios.optimization as optimization  
  from tabulate import tabulate
  import plotly.express as px
  import plotly.graph_objects as go
  import matplotlib.pyplot as plt
  import collections
  from pandas.plotting import autocorrelation_plot
  import config
  importlib.reload(config)
  img_dir = pathlib.Path("./img/")
  #img_dir = img_dir0.absolute()
  img_dir.mkdir(parents=True, exist_ok=True)
  import warnings
  warnings.filterwarnings("ignore")
#+end_src

* Introduction
The goal of this project is to implement a Black-Litterman portfolio optimization framework and leverage factor analysis in the portfolio construction. In this section the project specifications are outlined together with the implemented computational tools to fulfil them.
** Specifications:
From a high level view these were the requirements in the assignment, all of which have been completed:
- Pick a portfolio of assets that yields good diversification.
- Collect a good range of factors, compute alphas and betas for each of the assets and backtesting the resulting factor model.
- Implement Black-Litterman portfolio model for the posterior returns and covariance.
- Generate absolute and relative views for the portfolio from historical data.
- Compute at least two types of optimisation using the posterior information from the Black-Litterman model and with three levels of risk aversion.
- Assessment of the implications and effects of incorporating the views into the portfolio optimisation. 
- Compare performance of constructed portfolios to the market and backtesting.  
** Implemented computational tools and external libraries
In order to fulfil the specifications laid above, a range of tools have been implemented in Python which are summarised as:
- Functionality to automatically retrieve data from YahooFinance, select a diversified porfolio, create features such as returns, volatilities, moving averages etc.
- Similarly, a front-end to get factors and risk-free rate data and construct a factor model via Ordinary Least squares (OLS).
- A wrapper for the creation of an ARIMA  model and hyperparameter search of the underlying model  parameters. A forecasting capability based of this statistical to estimate short-term future returns and from those extract meaningful views into the assets performance.
- A portfolio optimization framework with constraints and leveraging Scipy optimization engine.
- A model of Black-Litterman portfolio approach to introduce the asset's manager views.
- A set of utilities to ...
- A testing framework to validate the codes, including the OLS for factors model building, the Black-Litterman implementation and the portfolio optimization. 
Outside the general-purpose Python ecosystem (numpy, pandas, etc.), the following libraries have been employed  to complete this work:
- *getFamaFrenchFactors*: Collects data for fame french factors from the Kenneth French library
  https://pypi.org/project/getFamaFrenchFactors/
- *statsmodels*: statistical software used to deploy an ARIMA process on the asset returns and also to fit the factors to data using an Ordinary Least Squares (OLS) approach. 
  https://www.statsmodels.org/stable/index.html
- *yahoofinance*: employed to get financial data such companies prices or market capitalization.
  https://python-yahoofinance.readthedocs.io/en/latest/
* Theory
** Black-Litterman model


$$
\mu_{eq} = \lambda \Sigma w_{mkt}
$$

- $\mu_{eq}$ is the Implied Excess Equilibrium Return Vector 
- $\lambda$ is the risk aversion coefficient, $\lambda = \frac{E(\mu) - \mu_r}{\sigma^2}$
- $\Sigma$ is the covariance matrix of excess returns
- $w_{mkt}$ are market capitalization weights

$$
E[\mu_{bl}] = \left[(\tau \Sigma)^{-1} + P'\Omega^{-1}P\right]^{-1} \left[(\tau \Sigma)^{-1}\mu_{eq} + P'\Omega^{-1}Q\right]  
$$

- $E[\mu_{bl}]$: Posterior combined return vector ($N\times a$)
- $\tau$: error in views
- $P$: Matrix identifying assets involved in the views ($K\times N$)
- $Q$: View vector ($K\times 1$)
- $\Omega$: Diagonal covariance matrix with the errors expressed in the views ($N\times N$). 

Normal distributions:

- Prior equilibrium distribution: $N(\mu_{eq}, \tau \Sigma)$
- Views distribution: $N(Q, \Omega)$
- New combined return distribution: $N\left(E[\mu_{bl}], \left[(\tau \Sigma)^{-1} + P'\Omega^{-1}P\right]^{-1} \right)$
  
** Fama-French factors

It is one of the multi-factor models which is widely used in both academia and industry to estimate the excess return of an investment asset. It is an extension to Capital Asset Pricing Model (CAPM) by adding two additional factors apart from the market risk when estimating the excess returns of an asset. The three factors considered in this model are:

    - Market factor (MKT): Excess market return
    - Size factor (SMB): Excess return with a small market cap over those with a large market cap
    - Value factor (HML): Excess return of value stocks over growth stocks.
    - Profitability factor (RMW): Excess returns of stocks with strong and weak profitability
    - Investment factor (CMA): Excess returns between high and low investment firms.
      
The Fama-French model is widely known as a stock market benchmark to evaluate investment performance.

$$
E[r_a] = \mu_a + \beta E[r_f]  + \epsilon_a
$$

$$
Var[r_a] = \mu_a + \beta r_f  + \epsilon_a
$$

$$
\Pi = w_a^{\top} r_a
$$

$$
Var(\Pi) = Var(r_a^{\top} w_a) = Var(r_a^{\top} w_a)
$$

$$
\Sigma = B^{\top} \Sigma_f B 
$$

** ARIMA model for time series
AutoRegressive Integrated Moving Average (ARIMA) statistical models are used 
AR: Autoregression. A model that uses the dependent relationship between an observation and some number of lagged observations.
I: Integrated. The use of differencing of raw observations (e.g. subtracting an observation from an observation at the previous time step) in order to make the time series stationary.
MA: Moving Average. A model that uses the dependency between an observation and a residual error from a moving average model applied to lagged observations.

Each of these components are explicitly specified in the model as a parameter. A standard notation is used of ARIMA(p,d,q) where the parameters are substituted with integer values to quickly indicate the specific ARIMA model being used.

The parameters of the ARIMA model are defined as follows:

- p: Number of lags in the observations that included in the model.
- d: Number of times differencing is applied to the observations.
- q: Size of moving average window.

** Optimisation

- Minimise Mean variance
- Maximize Sharpe ratio
- Hierarchical Risk Parity (HRP)   

* Results
The analysis herein is divided in three major sections: a portfolio selection of 10 assets from the S&P500 and the factor analysis of those assets; a statistical analysis using an ARIMA process in order to project the assets onto the future and generate the views input to the portfolio optimisation; and finally the Black-Litterman portfolio optimization with backtesting.  
** Portfolio and Factor analysis
:PROPERTIES:
:header-args: :session py1 :tangle yes :exports none
:END:
There are many approaches for picking a basket of assets and herein the adoption is a simple yet general and automatic strategy that guarantees diversification. The companies gathered are first presented together with their performance and correlations, then the factor analysis on these companies is shown.  
*** Asset selection
The S&P 500 is considered a better reflection of the marketâ€™s performance across all sectors compared to the Nasdaq Composite and the Down, therefore ten assets from this index are chosen using a tailored approach to attain a good diversified basket.  
The selection is based on a random and automatic generation of the tickers with these constraints: no two assets could belong to the same sector; 1 company is chosen among the top 5% in terms of market cap, 2 among the next 20%, 4 among the next 50%, 2 in the following 20% and the final one picked among the 5% smallest; the correlation among assets should also be kept small. 
#+NAME: Load index SP500
#+begin_src python :results none
  # Load index SP500
  # sp500 = financial_features.FinancialData("^GSPC",
  #                                          config.start_date_assets,
  #                                          config.end_date_assets,
  #                                          DATA_FOLDER="./data")
  # df_sp500 = sp500.df[['returns']].dropna()
#+end_src

#+NAME: Load portfolio and calculate market weights
#+begin_src python :results none
  # Load portfolio and calculate market weights
  tickers_sp500 = ml4qf.collectors.scrap_tickers_index(config.index_weblist)
  df_tickers_sp500 = ml4qf.collectors.get_tickers_info(tickers_sp500,
                                                       config.info_sp500,
                                                       data_folder="./data",
                                                       name_family="sp500")
  df_tickers_sp500.dropna(inplace=True)
  df_tickers_filtered = ml4qf.utils.date_filter_lower(df_tickers_sp500,
                                                      'first_date',
                                                      date_lower=config.start_date_assets)
  df_tickers_filtered =  df_tickers_filtered.sort_values('marketCap',ascending=False)
  df_selected_tickers = ml4qf.collectors.select_assets(df_tickers_filtered,
                                                       config.ASSET_SELECTION_PCT,
                                                       config.ASSET_SELECTION_NAMES)
  # Market cap equilibrium weights
  w_mkt = df_selected_tickers.marketCap / df_selected_tickers.marketCap.sum()
  num_assets = len(df_selected_tickers)
  portfolios_path = pathlib.Path("./data/portfolios/")
  portfolios_path.mkdir(parents=True, exist_ok=True)
  portfolios_file = portfolios_path / ("_".join(df_selected_tickers.index))
  if not portfolios_file.is_file():
      df_selected_tickers.to_csv(portfolios_file)
  w_mkt = w_mkt.to_numpy()

  # Load assets returns
  fdc = financial_features.FinancialDataContainer(df_selected_tickers.index,
                                                  config.start_date_assets,
                                                  config.end_date_assets,
                                                  '1mo',
                                                  './data')
  df_assets = fdc.df.dropna()
  df_assets_train, df_assets_test = ml4qf.utils.split_df_date(
      df_assets,
      split_index=config.split_data_idx)
  sp500x = financial_features.FinancialDataContainer(["^GSPC"],
                                                  config.start_date_assets,
                                                  config.end_date_assets,
                                                  '1mo',
                                                  './data')
  df_sp500 = sp500x.df.dropna()
  df_sp500_train, df_sp500_test = ml4qf.utils.split_df_date(
      df_sp500,
      split_index=config.split_data_idx)
  asset_names = list(df_assets.columns)

#+end_src
The resulting portfolio is shown in Table  [[df_portfolio_summary]].
#+NAME: Compute and show Data Frame, df_portfolio_summary
#+begin_src python :results raw :exports results :tangle no
  # Compute Data Frame df_portfolio_summary
  df_portfolio_summary = df_selected_tickers.copy()
  #df_portfolio_summary = df_portfolio_summary.drop('first_date', axis=1)
  df_portfolio_summary['marketWeights'] = w_mkt
  df_portfolio_summary = df_portfolio_summary[['marketCap',
                                               'marketWeights',
                                               'sector']]
  tabulate(df_portfolio_summary,
           headers=df_portfolio_summary.columns,
           showindex=True,
           tablefmt='orgtbl')
#+end_src
#+NAME: df_portfolio_summary
#+CAPTION: Portfolio selected assets
#+ATTR_LATEX: :width 0.7\textwidth :environment longtable :caption  
#+RESULTS: Compute and show Data Frame, df_portfolio_summary
|      |   marketCap | marketWeights | sector                 |
|------+-------------+---------------+------------------------|
| JPM  | 4.46929e+11 |      0.544416 | Financial Services     |
| CVS  | 9.56394e+10 |      0.116501 | Healthcare             |
| ATVI | 7.18864e+10 |     0.0875667 | Communication Services |
| PH   | 5.41743e+10 |     0.0659911 | Industrials            |
| WELL | 4.24812e+10 |     0.0517475 | Real Estate            |
| YUM  |   3.733e+10 |     0.0454727 | Consumer Cyclical      |
| KR   | 3.53562e+10 |     0.0430683 | Consumer Defensive     |
| ATO  | 1.69743e+10 |     0.0206769 | Utilities              |
| EQT  | 1.59304e+10 |     0.0194052 | Energy                 |
| DXC  | 4.23124e+09 |    0.00515418 | Technology             |
JP Morgan is the flagship of the porfolio and DXC Technology sits at the bottom of the portfolio in terms of size. 
*** Assets exploratory analysis
Monthly returns are used for the analysis as a better metric for a portfolio that is not going to be rebalanced for long periods of time. A period of over 20 years is taken for both the analysis and the backtesting as to make sure a reasonable amount of data is utilised in the study. The data is divided into a training and testing set with the forming comprising roughly 20 years and the latter over 2 years. The returns and dates of the assets for the first and last five days are shown in Table [[df_assets_training]] for the training set and in Table[[ df_assets_testing]] for the testing set.
#+NAME: df_assets_training
#+begin_src python :session py1 :results raw :exports results :tangle no
  df_assets2show = pd.concat([df_assets_train.iloc[:5],df_assets_train.iloc[-5:]])
  df_assets2show.index = df_assets2show.index.date
  tabulate(df_assets2show.round(decimals=3),
           headers=asset_names,
           showindex=True,
           tablefmt='orgtbl')
#+end_src
#+NAME: df_assets_training
#+CAPTION: Portfolio monthly returns training data
#+ATTR_LATEX: :width 0.7\textwidth :environment longtable :caption  
#+RESULTS: df_assets_training
|            |    JPM |    CVS |   ATVI |     PH |   WELL |    YUM |     KR |    ATO |    EQT |    DXC |
|------------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------|
| 2000-03-01 |  0.095 |  0.073 | -0.049 |   0.14 |   -0.1 |  0.167 |  0.176 | -0.044 |  0.187 |  0.004 |
| 2000-04-01 | -0.173 |  0.158 | -0.482 |  0.126 |  0.138 |  0.091 |  0.057 | -0.031 |  0.035 |  0.031 |
| 2000-05-01 |  0.036 |      0 |  -0.01 | -0.103 |  0.008 | -0.135 |  0.071 |  0.154 |  0.073 |  0.175 |
| 2000-06-01 | -0.075 |  -0.08 |  0.051 | -0.178 |  0.013 | -0.036 |   0.11 | -0.044 |  -0.03 |  -0.22 |
| 2000-07-01 |  0.081 | -0.012 |  0.346 |  0.038 |  0.107 | -0.142 | -0.062 |  0.178 |  0.079 | -0.172 |
| 2020-04-01 |  0.064 |  0.037 |  0.071 |  0.219 |  0.119 |  0.261 |  0.049 |  0.028 |  1.064 |  0.389 |
| 2020-05-01 |  0.016 |  0.065 |  0.129 |  0.138 | -0.011 |  0.038 |  0.032 |  0.008 | -0.086 | -0.216 |
| 2020-06-01 | -0.033 | -0.009 |  0.054 |  0.018 |  0.021 | -0.031 |  0.038 | -0.031 | -0.108 |  0.161 |
| 2020-07-01 |  0.027 | -0.031 |  0.089 | -0.024 |  0.035 |  0.048 |  0.028 |  0.064 |   0.22 |  0.085 |
| 2020-08-01 |  0.037 | -0.013 |  0.011 |  0.151 |  0.074 |  0.053 |  0.026 | -0.058 |  0.093 |  0.116 |

#+NAME: df_assets_testing
#+begin_src python :session py1 :results raw :exports results :tangle no
  df_assets2show = pd.concat([df_assets_test.iloc[:5],df_assets_test.iloc[-5:]])
  df_assets2show.index = df_assets2show.index.date
  tabulate(df_assets2show.round(decimals=3),
           headers=asset_names,
           showindex=True,
           tablefmt='orgtbl')
#+end_src
#+NAME: df_assets_testing
#+CAPTION: Portfolio monthly returns testing data
#+ATTR_LATEX: :width 0.7\textwidth :environment longtable :caption  
#+RESULTS: df_assets_testing
|            |    JPM |    CVS |   ATVI |     PH |   WELL |    YUM |     KR |    ATO |    EQT |    DXC |
|------------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------|
| 2020-09-01 | -0.039 |  -0.06 | -0.031 | -0.018 | -0.042 | -0.047 |  -0.05 | -0.042 | -0.185 | -0.107 |
| 2020-10-01 |  0.018 |  -0.04 | -0.064 |   0.03 | -0.024 |  0.022 |  -0.05 | -0.041 |  0.171 |  0.032 |
| 2020-11-01 |  0.202 |  0.209 |   0.05 |  0.283 |  0.171 |  0.134 |  0.025 |  0.046 | -0.017 |  0.189 |
| 2020-12-01 |  0.078 |  0.008 |  0.168 |  0.019 |  0.026 |  0.026 | -0.038 | -0.005 | -0.146 |  0.175 |
| 2021-01-01 |  0.013 |  0.049 |  -0.02 | -0.029 | -0.062 | -0.065 |  0.086 | -0.067 |  0.283 |  0.095 |
| 2022-07-01 |  0.024 |  0.033 |  0.027 |  0.175 |  0.048 |   0.08 | -0.019 |  0.083 |   0.28 |  0.043 |
| 2022-08-01 | -0.014 |  0.026 | -0.018 | -0.083 | -0.112 | -0.092 |  0.032 | -0.066 |  0.086 | -0.216 |
| 2022-09-01 | -0.081 | -0.028 | -0.053 | -0.086 | -0.161 | -0.044 | -0.087 | -0.102 | -0.147 | -0.012 |
| 2022-10-01 |  0.205 | -0.007 | -0.021 |  0.199 | -0.051 |  0.112 |  0.081 |  0.046 |  0.027 |  0.174 |
| 2022-11-01 |  0.098 |  0.076 |  0.016 |  0.029 |  0.164 |  0.088 |   0.04 |  0.128 |  0.014 |  0.032 |

#
The same data in the tables can be visualised in Fig. [[basket_returns]] for the whole period of analysis.
#+NAME: basket_returns
#+begin_src python :results value file  :exports results :var name=(org-element-property  :name (org-element-context))
  # Plot basket_returns
  fig1_path= img_dir / f'{name}.png'
  fig1 = px.line(df_assets, y=df_assets.keys(),
                 labels={'value':'Monthly returns'})
  fig1.update_layout(margin_b=3, margin_t=5)
  fig1.write_image(fig1_path)
  fig1_path #
#+end_src
#+NAME: basket_returns
#+CAPTION:  Asset's basket returns over the studied period 
#+ATTR_LATEX: :width 0.75\textwidth 
#+RESULTS: basket_returns
[[file:img/basket_returns.png]]

The correlation between the returns is a good indication of how well diversified our portfolio is and it can be seen that a low correlation is reflected among most of the assets. It is important to keep in mind this is not the ultimate proof of diversification since it does not capture nonlinear relations between the pairs.
#+NAME: AssetsCorrelation
#+begin_src python :results value file  :exports results :var name=(org-element-property :name (org-element-context))
  # Plot AssetsCorrelation
  fig1_path= img_dir / f'{name}.png'
  df_corr = df_assets.corr().round(2)
  fig1 = px.imshow(np.abs(df_corr), text_auto = True, color_continuous_scale='blues')
  #fig1.layout.height = 600
  #fig1.layout.width = 600
  fig1.update_layout(margin_l=0,margin_b=3, margin_t=5)
  fig1.write_image(fig1_path)
  fig1_path #
#+end_src
#+CAPTION: Assets correlation
#+NAME: AssetsCorrelation
#+ATTR_LATEX: :width 0.9\textwidth 
#+RESULTS: AssetsCorrelation
[[file:img/AssetsCorrelation.png]]
\newpage
*** Factor collection
The 5 Fama-French factors are retrieved for the dates of interest together with the momentum factor and the risk-free interest rate. As with the returns, monthly periods are considered. Fig. [[Factors_evolution]] shows the evolution of the six factors studied and Fig. [[RFrate_evolution]] presents the annualised interest rates.
#+NAME: Load Fama and French 5 factors and Momentum factor  
#+begin_src python  :results none
  # Load Fama and French 5 factors and Momentum factor
  factor_names = financial_factors.get_factor_names(config.FACTORS)  
  df_factors0 = financial_factors.get_factors(config.FACTORS.keys(), 'm')
  df_factors =  ml4qf.utils.trim_df_date(df_factors0, start_date=config.start_date_factors,
                                         end_date=config.end_date_factors)
  df_factors_train, df_factors_test = ml4qf.utils.split_df_date(df_factors,
                                          split_index=config.split_data_idx)
#+end_src

#+NAME: Factors_evolution 
#+begin_src python :results value file  :exports results :var name=(org-element-property :name (org-element-context))
  # Plot monthly Factors evolution 
  fig1_path= img_dir / f'{name}.png'
  fig1 = px.line(df_factors, y=factor_names)
  fig1.update_layout(margin_b=3, margin_t=5)
  fig1.write_image(fig1_path)
  fig1_path # 
#+end_src
#+NAME: Factors_evolution 
#+CAPTION: 5 Fama-French and Momentun factors evolution
#+ATTR_LATEX: :width 0.75\textwidth 
#+RESULTS: Factors_evolution
[[file:img/Factors_evolution.png]]

#+NAME: RFrate_evolution
#+begin_src python :results value file  :exports results :var name=(org-element-property  :name (org-element-context)) 
  fig1_path= img_dir / f'{name}.png'
  fig1 = px.line(df_factors*12, y='RF')
  fig1.update_layout(margin_r=12, margin_l=1,
                     margin_b=3, margin_t=5)
  fig1.write_image(fig1_path)
  fig1_path #
#+end_src
#+NAME: RFrate_evolution
#+CAPTION: (Annualised) risk-free rate evolution
#+ATTR_LATEX: :width 0.75\textwidth 
#+RESULTS: RFrate_evolution
[[file:img/RFrate_evolution.png]]

*** Factor regression
With the asset returns, the factors and the risk-free rate, the $\alpha$ vector and the $\beta$ matrix are calculated using an OLS regression. The results of this regression are shown in Table [[df_train_factors]].
#+NAME: Compute regression on assets returns vs factors
#+begin_src python :results none
  # Compute regression on assets returns vs factors
  factor_models = financial_factors.factors_regression(factor_names,
                                                       df_factors_train,
                                                       df_assets_train,
                                                       regression_kernel=regression_OLS)
  alpha, beta = financial_factors.compute_factors_coeff(factor_models)
  factor_model = financial_factors.factor_lin_generator(alpha, beta)
#+end_src

Note the regression is computed based on the training data only, such that we can backtest how well it performs on the testing data.
#+NAME: Data Frame df_train_factors with alphas and betas
#+begin_src python  :results raw :exports results :tangle no
  albe = np.vstack([alpha, beta]).T
  df_index = asset_names
  df_columns = ['alpha'] + factor_names
  df_train_factors = pd.DataFrame(albe, columns=df_columns, index=df_index)
  tabulate(df_train_factors, headers=df_columns, showindex=True, tablefmt='orgtbl')
#+end_src
#+NAME: df_train_factors
#+CAPTION: Factor analysis alphas and betas.  
#+ATTR_LATEX: :width 0.7\textwidth :environment longtable :caption
#+RESULTS: Data Frame df_train_factors with alphas and betas
|      |        alpha |   Mkt-RF |       SMB |       HML |      RMW |        CMA |        MOM |
|------+--------------+----------+-----------+-----------+----------+------------+------------|
| JPM  |   0.00647651 | 0.922232 |  -0.28544 |   1.18559 | -1.07209 |  -0.493312 |  -0.273228 |
| CVS  |  -0.00148262 | 0.868157 | -0.215823 |  0.012167 |  0.25336 |    1.07393 |   0.026691 |
| ATVI |    0.0197562 | 0.889834 |  0.242885 | -0.206069 | -0.47066 |   0.212356 |    0.39213 |
| PH   |  -0.00382804 |  1.49288 |  0.446137 | -0.124334 |  1.14579 |   0.475606 |  -0.236351 |
| WELL |  0.000788633 |  0.58505 |  0.356454 |  0.135987 | 0.250443 |   0.247888 | -0.0239137 |
| YUM  |   0.00341028 | 0.876246 |  0.410095 | -0.205616 |  1.00998 |  0.0557652 |  -0.120782 |
| KR   | -7.80218e-05 | 0.758796 | -0.304005 | -0.211342 | 0.349079 |   0.986875 |   0.149481 |
| ATO  |   0.00152634 | 0.401037 |  0.208798 |  -0.14717 | 0.374291 |   0.563191 |  0.0033396 |
| EQT  |  -0.00322677 |  1.09022 |  0.140145 | -0.317918 | 0.955749 |   0.672032 | -0.0193002 |
| DXC  |  -0.00517583 |  1.47021 | -0.098542 | 0.0172014 | 0.263557 | 0.00627073 |  -0.250473 |

With the /alpha/ and /betas/ approximated, one can create a model to approximate the asset returns based on the factors information of the markets.
#+NAME: Compute factor model prediction
#+begin_src python :results none
  # Compute factor model prediction
  # prediction on test data
  returns_pred = factor_model(df_factors_test[factor_names].to_numpy())
  df_returns_pred = pd.DataFrame(returns_pred,
                                 columns=asset_names,
                                 index=df_assets_test.index)
  # prediction on training data
  returns_predt = factor_model(df_factors_train[factor_names].to_numpy())
  df_returns_predt = pd.DataFrame(returns_predt,
                                 columns=asset_names,
                                 index=df_assets_train.index)

#+end_src

*** Factors backtesting
#+begin_comment
#+NAME: predicted_returns
#+begin_src python :var i_asset=0 name=(org-element-property :name (org-element-context))
  i_asset = i_asset
  i_name = asset_names[i_asset]
  fig1_path= img_dir / f'{name}{i_name}.png'
  fig1 = go.Figure()
  fig1.add_trace(go.Scatter(
      x=df_assets_test.index,
      y=df_assets_test.iloc[:, i_asset] - df_factors_test.RF.to_numpy(),
      mode='lines+markers',
      name=f"{i_name} real"))
  fig1.add_trace(go.Scatter(
      x=df_assets_test.index,
      y=df_returns_pred[i_name],
      mode='lines',
      name=f"{i_name} pred."))

  #px.line(df_returns_pred['GOOGL'], y=df_returns_pred.keys()[0])

  fig1.write_image(fig1_path)
  str(fig1_path)
#+end_src

#+NAME: predicted_returns0
#+begin_src python :noweb eval :results value file  :exports results 
  fig_path = "<<predicted_returns(i_asset=0, name="predicted_returns_")>>"
  fig_path
#+end_src
#+CAPTION:  Backtesting factor approximation on Google asset
#+ATTR_LATEX: :width 0.75\textwidth 
#+RESULTS: predicted_returns0
#+end_comment
For the factor backtesting, the predicted returns given the 6-factor data are compared to the actual returns of each asset.  
#+NAME: Function to plot returns from factor model
#+begin_src python :results none 
  def plot_rets_fromfactors(df_assets,
                            df_factors,
                            df_returns_pred,
                            i_asset,
                            name):
      """Function to plot returns from factor model"""

      i_name = asset_names[i_asset]
      fig1_path= img_dir / f'{name}.png'
      fig1 = go.Figure()
      fig1.add_trace(go.Scatter(
          x=df_assets.index,
          y=df_assets.iloc[:, i_asset] - df_factors.RF.to_numpy(),
          mode='lines+markers',
          name=f"{i_name} real"))
      fig1.add_trace(go.Scatter(
          x=df_assets.index,
          y=df_returns_pred[i_name],
          mode='lines',
          name=f"{i_name} pred."))
      fig1.update_layout(margin_b=3, margin_l=2, margin_t=5,
                         yaxis_title='Monthly returns')
      fig1.write_image(fig1_path)
      return str(fig1_path)

#+end_src
Some of the companies are better correlated with the factors and so the approximation is more accurate, as is the case for example for JPM and PH, whose returns from the factor model are depicted in Figures [[predicted_factorreturns_JPM]] and [[predicted_factorreturns_PH]] respectively.
#+NAME: predicted_factorreturns_JPM
#+begin_src python :noweb eval :results value file  :exports results :var name=(org-element-property :name (org-element-context)) 
  fig1_path = plot_rets_fromfactors(df_assets_test,
                                   df_factors_test,
                                   df_returns_pred,
                                   i_asset=0, name=name)
  fig1_path #
#+end_src
#+NAME: predicted_factorreturns_JPM
#+CAPTION:  Backtesting factor approaximation on JPM
#+ATTR_LATEX: :width 0.75\textwidth 
#+RESULTS: predicted_factorreturns_JPM

#+NAME: predicted_factorreturns_PH
#+begin_src python :noweb eval :results value file  :exports results :var name=(org-element-property :name (org-element-context)) 
  fig1_path = plot_rets_fromfactors(df_assets_test,
                                   df_factors_test,
                                   df_returns_pred,
                                   i_asset=3, name=name)
  fig1_path #
#+end_src
#+NAME: predicted_factorreturns_PH
#+CAPTION:  Backtesting factor approximation on PH
#+ATTR_LATEX: :width 0.75\textwidth 
#+RESULTS: predicted_factorreturns_PH
[[file:img/predicted_factorreturns_PH.png]]

On the contrary, CVS's returns are captured by the factor model but not as well as JPM and PH, as shown in Fig. [[predicted_factorreturns_CVS]]. And for ATVI the approximation is even less accurate. Note that these results are in line with the matrix of correlations in Fig. [[AssetsCorrelation]]: JPM and PH show higher correlations with the rest of the assets than CVS, and ATVI represents one with smallest correlations.  
#+NAME: predicted_factorreturns_CVS
#+begin_src python :noweb eval :results value file  :exports results :var name=(org-element-property :name (org-element-context)) 
  fig1_path = plot_rets_fromfactors(df_assets_test,
                                   df_factors_test,
                                   df_returns_pred,
                                   i_asset=1, name=name)
  fig1_path #
#+end_src
#+NAME: predicted_factorreturns_CVS
#+CAPTION:  Backtesting factor approaximation on CVS
#+ATTR_LATEX: :width 0.75\textwidth 
#+RESULTS: predicted_factorreturns_CVS
[[file:img/predicted_factorreturns_CVS.png]]

#+NAME: predicted_factorreturns_ATVI
#+begin_src python :noweb eval :results value file  :exports results :var name=(org-element-property :name (org-element-context)) 
  fig1_path = plot_rets_fromfactors(df_assets_test,
                                   df_factors_test,
                                   df_returns_pred,
                                   i_asset=2, name=name)
  fig1_path #
#+end_src
#+NAME: predicted_factorreturns_ATVI
#+CAPTION:  Backtesting factor approaximation on ATVI
#+ATTR_LATEX: :width 0.75\textwidth 
#+RESULTS: predicted_factorreturns_ATVI
[[file:img/predicted_factorreturns_ATVI.png]]

In Appendix [[appx_factors]] a summary of the OLS computation output to approximate the factors can be found for JPM, PH, CVS and ATVI. 
#+NAME: SummaryfactorsOLS_JPM
#+begin_src python :results output :exports results :tangle no
  print(factor_models[asset_names[0]].summary())
#+end_src

#+NAME: SummaryfactorsOLS_CVS
#+begin_src python :results output :exports results :tangle no
  print(factor_models[asset_names[1]].summary())
#+end_src

#+NAME: SummaryfactorsOLS_ATVI
#+begin_src python :results output :exports results :tangle no
  print(factor_models[asset_names[2]].summary())
#+end_src

#+NAME: SummaryfactorsOLS_PH
#+begin_src python :results output :exports results :tangle no
  print(factor_models[asset_names[3]].summary())
#+end_src


#+BEGIN_COMMENT
#+NAME: predicted_factorreturns_train
#+begin_src python :noweb eval :results value file  :exports results :var name=(org-element-property :name (org-element-context))
  fig1_path = plot_rets_fromfactors(df_assets_train,
                                    df_factors_train,
                                    df_returns_predt,
                                    i_asset=0, name=name)
  fig1_path #
#+end_src
#+CAPTION:  Backtesting factor approaximation on Google asset
#+ATTR_LATEX: :width 0.75\textwidth 
#+RESULTS: predicted_factorreturns_train
[[file:img/predicted_factorreturns_trainJPM.png]]
#+END_COMMENT
** ARIMA model for the the generation of asset views
:PROPERTIES:
:header-args: :session py1 :tangle yes :exports none
:END:
Firstly an ARIMA process is constructed with a hyperparameter search for the p, d and q parameters. Then a backtest of the asset's returns predictions is carried out and based on those predictions a set of views are imposed for the portfolio construction in next section. 
*** ARIMA model construction
#+BEGIN_COMMENT
#+NAME: arima_autocorrelation
#+begin_src python :results value file  :exports results :var name=(org-element-property :name  (org-element-context)) 
  fig1_path= img_dir / f'{name}.png'
  fig = plt.figure()
  ax = autocorrelation_plot(df_train_factors['SMB'])
  #ax.set_title("bleh")
  #ax.set_xlabel("xlabel")
  #ax.plot(x, y, 'r--')
  fig.savefig(fig1_path)
  fig1_path
#+end_src
#+RESULTS: arima_autocorrelation
[[file:img/arima_autocorrelation.png]]
#+END_COMMENT

#+begin_src python :results none
  def pick_arimahyper(errs):
      """ Builds ARIMA parameters and errors from hyperparameter search"""
      arima_parameters = dict()
      derrors = collections.defaultdict(list)
      derrorsind = collections.defaultdict(list)
      minvalue = collections.defaultdict(list)
      index = collections.defaultdict(list)  
      for k, v in errs.items():
          conv = k.split('_')
          derrors[conv[0]].append(v)
          derrorsind[conv[0]].append(tuple(int(i) for i in conv[1:]))
      for k, v in derrors.items():
          index[k] = v.index(min(v))
          minvalue[k] = min(v)
          arima_parameters[k] = derrorsind[k][index[k]]
      return arima_parameters, derrors, derrorsind, minvalue
#+end_src
The /statsmodels/ library is used to build an AIRIMA model training on the companies returns and predicting in the testing data.  
#+begin_src python :results none
  if config.compute_arima_parameters:
      errs_train, errs_test = model_stats.arima_hyperparameters(
          df_factors_train, # 
          df_factors_test,
          factor_names,
          [0, 2, 4, 6, 8, 11, 15, 19, 23, 29, 35],
          [0, 1, 2, 3, 4, 5],
          [0, 1, 3, 4, 5, 6, 15, 19, 23],
          model_stats.err_mse,
          dict(enforce_stationarity=False,
               enforce_invertibility=False)
      )
      arima_parameters, derrors, derrorsind, minvalue = pick_arimahyper(errs_test)
  else:
      arima_parameters = config.arima_parameters
  df_arima_parameters = pd.DataFrame(arima_parameters, index=['p', 'd', 'q'])
#+end_src
Table [[df_arima_parameters]] shows the parameters used in the ARIMA model after a hyperparameter search to minimise the error in the approximation. 
#+NAME: df_arima_parameters
#+begin_src python  :results raw :exports results :tangle no
  tabulate(df_arima_parameters, headers=df_arima_parameters, showindex=True, tablefmt='orgtbl')
#+end_src
#+NAME: df_arima_parameters
#+CAPTION: Hyperparameters in ARIMA process for each factor
#+ATTR_LATEX: :width 0.7\textwidth :environment longtable :caption
#+RESULTS: df_arima_parameters
|   | Mkt-RF | SMB | HML | RMW | CMA | MOM |
|---+--------+-----+-----+-----+-----+-----|
| p |     15 |  15 |   6 |  15 |   6 |   9 |
| d |      0 |   0 |   0 |   0 |   4 |   0 |
| q |     15 |   9 |   3 |   6 |  12 |   6 |

*** ARIMA factors prediction
Using the hyperparameters above the best model is built and here the performance of the model on the factors time-series is checked. Figures  [[ARIMA_Mkt-RF_train]] and [[ARIMA_Mkt-RF_test]] present the results of the market premium factor on the training and testing data respectively, and similarly Figures [[ARIMA_CMA_train]] and [[ARIMA_CMA_test]] for the investment input factors. 
#+begin_src python :results none 
  Xtrain = df_factors_train[factor_names].to_numpy()
  Xtest = df_factors_test[factor_names].to_numpy()
  index_train = df_factors_train.index
  index_test = df_factors_test.index
  # arima_parameters = {'Mkt-RF': (3,0,21),
  #                     'SMB': (15,0,9),
  #                     'HML': (6,0,3),
  #                     'RMW': (15,0,6),
  #                     'CMA': (21,3,12),
  #                     'MOM': (12,0,21)
  #                     }
  #arima_parameters = config.arima_parameters
  model_sett = dict(enforce_stationarity=False,
                    enforce_invertibility=False) 
  arima_train_models = model_stats.arima_fit(Xtrain,
                                             factor_names,
                                             arima_parameters,
                                             model_sett=model_sett)
  df_arimatrain, df_arimatest = model_stats.arima_build_pred(arima_train_models,
                                                             Xtrain,
                                                             Xtest,
                                                             factor_names,
                                                             index_train,
                                                             index_test)  
#+end_src

#+NAME: ARIMA_Mkt-RF_train
#+begin_src python :results value file :exports results :var name=(org-element-property :name (org-element-context)) 
  fig1_path= img_dir / f'{name}.png'
  fig1 = px.line(df_arimatrain, y=['Mkt-RF','Mkt-RF_pred'])
  fig1.write_image(fig1_path)
  fig1_path #

#+end_src
#+CAPTION: ARIMA model on Market excess returns factor (training data).
#+NAME: ARIMA_Mkt-RF_train
#+ATTR_LATEX: :width 0.75\textwidth 
#+RESULTS: ARIMA_Mkt-RF_train

#+NAME: ARIMA_Mkt-RF_test
#+begin_src python :results value file :exports results :var name=(org-element-property :name  (org-element-context)) 
  fig1_path= img_dir / f'{name}.png'
  fig1 = px.line(df_arimatest, y=['Mkt-RF','Mkt-RF_pred'])
  fig1.write_image(fig1_path)
  fig1_path #

#+end_src
#+CAPTION: ARIMA model on Market excess returns factor (testing data).
#+NAME: ARIMA_Mkt-RF_test
#+ATTR_LATEX: :width 0.75\textwidth 
#+RESULTS: ARIMA_Mkt-RF_test

#+NAME: ARIMA_CMA_train
#+begin_src python :results value file :exports results :var name=(org-element-property :name  (org-element-context)) 
  fig1_path= img_dir / f'{name}.png'
  fig1 = px.line(df_arimatrain, y=['CMA','CMA_pred'])
  fig1.write_image(fig1_path)
  fig1_path #

#+end_src
#+CAPTION: ARIMA model on CMA factor (training data).
#+NAME: ARIMA_CMA_train
#+ATTR_LATEX: :width 0.75\textwidth 
#+RESULTS: ARIMA_CMA_train

#+NAME: ARIMA_CMA_test
#+begin_src python :results value file :exports results :var name=(org-element-property :name  (org-element-context)) 
  fig1_path= img_dir / f'{name}.png'
  fig1 = px.line(df_arimatest, y=['CMA','CMA_pred'])
  fig1.write_image(fig1_path)
  fig1_path #

#+end_src
#+CAPTION: ARIMA model on CMA factor (testing data).
#+NAME: ARIMA_CMA_test
#+ATTR_LATEX: :width 0.75\textwidth 
#+RESULTS: ARIMA_CMA_test

We can see how moving into the future the ARIMA model loses the variance and therefore it would be necessary to introduce a GARCH model to correctly estimate the volatility component of the assets. This improvement is not pursue herein since the interest in this exercise is to have an overall prediction on the growth of the returns to generate the views.
The summary of the ARIMA model built for MKT-RK and CMA can be found in Appendix [[sec:appx_arima]] (reports for other factors can be generated within the main code).  
#+NAME: ARIMA_summary_mkt
#+begin_src python :results output :exports results :tangle no  
  print(arima_train_models['Mkt-RF'].summary())
#+end_src

#+NAME: ARIMA_summary_cma
#+begin_src python :results output :exports results :tangle no
  print(arima_train_models['CMA'].summary())
#+end_src

*** Backtesting of returns and views from model prediction
Using the ARIMA model built on the training data set, predicted returns are projected onto the "future", i.e. the testing set, for over two years. The cumulative returns are then compared so that the views for the Black-Litterman model can be proposed. 
#+begin_src python :results none 
  # prediction on train data
  fnames_prediction = [k for k in df_arimatrain.columns if "_pred" in k]
  asset_names_pred = [k + '_pred' for k in asset_names]
  returns_arimapred_train = factor_model(df_arimatrain[fnames_prediction].to_numpy())
  df_arimapred_train = pd.DataFrame(returns_arimapred_train,
                                    columns=asset_names_pred,
                                    index=df_assets_train.index[:-1])
  df_arimaasset_train = df_arimapred_train.join(df_assets_train)
  # prediction on test data
  returns_arimapred_test = factor_model(df_arimatest[fnames_prediction].to_numpy())
  df_arimapred_test = pd.DataFrame(returns_arimapred_test,
                                   columns=asset_names_pred,
                                   index=df_assets_test.index[:-1])
  df_arimaasset_test = df_arimapred_test.join(df_assets_test)
  # # prediction on training data
  df_arimatest_profits = ml4qf.utils.profit_portfolio(
     df_arimaasset_test,
     {k: 1. for k in df_arimaasset_test.columns})
#+end_src

#+NAME: ARIMA_returnsbacktest
#+begin_src python :results value file :exports none :var name=(org-element-property :name  (org-element-context))
  # no exporting, just for experimenting
  fig1_path= img_dir / f'{name}.png'
  #fig1 = px.line(df_arimaasset_test, y=['JPM_pred', 'JPM','EQT','EQT_pred'])
  fig1 = px.line(df_arimaasset_test, y=['JPM', 'JPM_pred', 'PH','PH_pred'])
  fig1.write_image(fig1_path)
  fig1_path #

#+end_src
#+ATTR_LATEX: :width 0.75\textwidth 
#+RESULTS: ARIMA_returnsbacktest
[[file:img/ARIMA_returnsbacktest.png]]

The first assets we compare are JPM and PH, which showed one of the strongest correlations. Fig. [[ARIMA_returnsbacktestJPM_PH]] shows PH outperforming JPM by nearly 11% over the period of 26 months.
#+NAME: ARIMA_returnsbacktestJPM_PH
#+begin_src python :results value file :exports results :var name=(org-element-property :name  (org-element-context)) 
  fig1_path= img_dir / f'{name}.png'
  fig1 = px.line(df_arimatest_profits, y=['JPM','JPM_pred','PH','PH_pred'],
                 labels={'value':'Cumulative returns'})
  #fig1 = px.line(df_arimaasset_test, y=['JPM', 'JPM_pred'])
  fig1.update_layout(margin_l=0,margin_b=3, margin_t=5)
  fig1.write_image(fig1_path)
  fig1_path #
#+end_src
#+CAPTION: Prediction and real Cumulative returs for JPM and PH
#+NAME:ARIMA_returnsbacktestJPM_PH
#+ATTR_LATEX: :width 0.75\textwidth 
#+RESULTS: ARIMA_returnsbacktestJPM_PH
[[file:img/ARIMA_returnsbacktestJPM_PH.png]]
Looking at an absolute view, we can see in Fig. [[ARIMA_returnsbacktestEQT]] the forecast of the energy company EQT is better than many of the others --although it is not close to the actual growth the asset underwent over this period. A 29%  increment over the curse of the testing is predicted and it is what will be taken for the view. 
#+NAME: ARIMA_returnsbacktestEQT
#+begin_src python :results value file :exports results :var name=(org-element-property :name  (org-element-context)) 
  fig1_path= img_dir / f'{name}.png'
  fig1 = px.line(df_arimatest_profits,
                 y=['EQT', 'EQT_pred', 'WELL', 'WELL_pred', 'DXC', 'DXC_pred'],
                 labels={'value':'Comulative returns'})
  #fig1 = px.line(df_arimaasset_test, y=['JPM', 'JPM_pred'])
  fig1.update_layout(margin_l=0,margin_b=3, margin_t=5)
  fig1.write_image(fig1_path)
  fig1_path #

#+end_src
#+CAPTION: Prediction and real Cumulative returs for WELL, DXC and EQT
#+NAME: ARIMA_returnsbacktestEQT
#+ATTR_LATEX: :width 0.75\textwidth 
#+RESULTS: ARIMA_returnsbacktestEQT
[[file:img/ARIMA_returnsbacktestEQT.png]]

Finally an interesting comparison appears for the ATO and KR tickers, where the increasing spread is rather well captured by the forecasting factor model. 
#+NAME: ARIMA_returnsbacktestATO
#+begin_src python :results value file :exports results :var name=(org-element-property :name  (org-element-context)) 
  fig1_path= img_dir / f'{name}.png'
  fig1 = px.line(df_arimatest_profits,
                 y=['ATO', 'ATO_pred', 'KR','KR_pred'],
                 labels={'value':'Comulative returns'})
  fig1.update_layout(margin_l=0,margin_b=3, margin_t=5)
  fig1.write_image(fig1_path)
  fig1_path #

#+end_src
#+CAPTION: Prediction and real Cumulative returs for ATO and KR
#+NAME: ARIMA_returnsbacktestATO
#+ATTR_LATEX: :width 0.75\textwidth 
#+RESULTS: #+NAME: ARIMA_returnsbacktestATO
[[file:img/ARIMA_returnsbacktestATO.png]]


Given these results on the testing data set, the following views are proposed: 
- EQT to rise 12% annually
- PH to outperform JPM by 6% annually
- KR to outperform ATO by 5% annually
The matrix and vector views, $P$ and $Q$, are then:
$$
P = \begin{bmatrix}
    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0  \\
   -1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0  \\
    0 & 0 & 0 & 0 & 0 & 0 & 1 &-1 & 0 & 0  
\end{bmatrix}
$$
$$
Q = \begin{bmatrix}
    0.12 & 0.06 & 0.05
\end{bmatrix}
$$

** Black-Litterman based portfolio
:PROPERTIES:
:header-args: :session py1 :exports none :tangle yes
:END:
*** COMMENT Covariance treatment
*** Prior and posterior returns construction
The first step in the portfolio construction is to compute the covariance between the assets. One could use the data forecast in the test data but it would have entailed the combination of a GARCH model with the ARIMA model to accurately predict the variances, which was not a requirement in this work. Thus historical data (from the training batch) is relied upon. Two options are possible, either calculate the covariance directly on the excess returns, or on the factors and derive the subsequent covariance on the returns using the matrix of betas. It was found the former was more robust so all the following calculations use that.
#+NAME: Calculate Covariance of excess returns
#+begin_src python :results none
  # Calculate Covariance from assets and from factors
  df_Sigma_factors = df_factors[factor_names].cov()
  df_Sigma_factors_train = df_factors_train[factor_names].cov()
  df_Sigma_factors_test = df_factors_test[factor_names].cov()
  Sigma_factors = df_Sigma_factors.to_numpy()
  Sigmainv_factors = np.linalg.inv(Sigma_factors)
  Sigma_factors_train = df_Sigma_factors_train.to_numpy()
  Sigmainv_factors_train = np.linalg.inv(Sigma_factors_train)
  Sigma_factors_test = df_Sigma_factors_test.to_numpy()
  Sigmainv_factors_test = np.linalg.inv(Sigma_factors_test)
  #####
  Sigma_4mfactors = beta.T @ Sigma_factors @ beta
  Sigmainv_4mfactors = np.linalg.inv(Sigma_4mfactors)
  Sigma_4mfactors_train = beta.T @ Sigma_factors_train @ beta
  Sigmainv_4mfactors_train = np.linalg.inv(Sigma_4mfactors_train)
  Sigma_4mfactors_test = beta.T @ Sigma_factors_test @ beta
  Sigmainv_4mfactors_test = np.linalg.inv(Sigma_4mfactors_test)
  ####################
  df_Sigma_assets = df_assets.cov()
  df_Sigma_assets_train = df_assets_train.cov()
  df_Sigma_assets_test = df_assets_test.cov()
  #Sigma_assets = df_Sigma_assets.to_numpy()
  Sigma_assets = np.cov((df_assets.to_numpy() - df_factors.RF.to_numpy().reshape(len(df_factors.RF), 1)).T)
  Sigmainv_assets = np.linalg.inv(Sigma_assets)
  #Sigma_assets_train = df_Sigma_assets_train.to_numpy()
  Sigma_assets_train = np.cov((df_assets_train.to_numpy() - df_factors_train.RF.to_numpy().reshape(len(df_factors_train.RF), 1)).T)
  Sigmainv_assets_train = np.linalg.inv(Sigma_assets_train)
  #Sigma_assets_test = df_Sigma_assets_test.to_numpy()
  Sigma_assets_test = np.cov((df_assets_test.to_numpy() - df_factors_test.RF.to_numpy().reshape(len(df_factors_test.RF), 1)).T)
  Sigmainv_assets_test = np.linalg.inv(Sigma_assets_test)

#+end_src
Covariance matrix of the excess returns is shown in Table [[df_Sigma_train]]. 
#+NAME: df_Sigma_train
#+begin_src python  :results raw :exports results :tangle no
  tabulate(df_Sigma_assets_train.round(decimals=4),
           headers=df_Sigma_assets_train.columns,
           showindex=True, tablefmt='orgtbl')
#+end_src
#+NAME: df_Sigma_train
#+CAPTION: Covariance matrix of excess returns on training data
#+ATTR_LATEX: :width 0.7\textwidth :environment longtable :caption
#+RESULTS: df_Sigma_train
|      |    JPM |    CVS |   ATVI |     PH |   WELL |    YUM |     KR |    ATO |    EQT |    DXC |
|------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------|
| JPM  | 0.0075 | 0.0015 | 0.0027 | 0.0032 | 0.0017 | 0.0015 | 0.0011 | 0.0006 | 0.0011 | 0.0039 |
| CVS  | 0.0015 | 0.0055 |      0 | 0.0028 | 0.0008 | 0.0009 | 0.0016 | 0.0006 | 0.0013 | 0.0018 |
| ATVI | 0.0027 |      0 | 0.0134 | 0.0013 | 0.0005 | 0.0015 | 0.0007 | 0.0008 |  0.002 | 0.0024 |
| PH   | 0.0032 | 0.0028 | 0.0013 | 0.0074 | 0.0017 | 0.0026 | 0.0009 | 0.0008 | 0.0029 | 0.0044 |
| WELL | 0.0017 | 0.0008 | 0.0005 | 0.0017 | 0.0044 | 0.0015 | 0.0004 | 0.0011 | 0.0006 | 0.0011 |
| YUM  | 0.0015 | 0.0009 | 0.0015 | 0.0026 | 0.0015 | 0.0052 | 0.0008 | 0.0003 | 0.0018 | 0.0027 |
| KR   | 0.0011 | 0.0016 | 0.0007 | 0.0009 | 0.0004 | 0.0008 |  0.005 | 0.0005 | 0.0008 | 0.0009 |
| ATO  | 0.0006 | 0.0006 | 0.0008 | 0.0008 | 0.0011 | 0.0003 | 0.0005 | 0.0023 | 0.0009 | 0.0005 |
| EQT  | 0.0011 | 0.0013 |  0.002 | 0.0029 | 0.0006 | 0.0018 | 0.0008 | 0.0009 | 0.0124 | 0.0034 |
| DXC  | 0.0039 | 0.0018 | 0.0024 | 0.0044 | 0.0011 | 0.0027 | 0.0009 | 0.0005 | 0.0034 | 0.0141 |
Next the market equilibrium returns are calculated from the market weights presented in Table [[df_portfolio_summary]].
#+NAME: Black-Litterman initialisation
#+begin_src python :results none 
  bl_model_Sassets = bl.BlackLitterman(Sigma_assets_train, w_mkt,
                                       config.lambda_mkt)
  bl_model_Sassets.set_portfolio_inputs(config.tau, config.P, config.Q)
  bl_model_Sfactors = bl.BlackLitterman(Sigma_4mfactors_train, w_mkt,
                                        config.lambda_mkt)
  bl_model_Sfactors.set_portfolio_inputs(config.tau, config.P, config.Q)
  w_bl = f_w(config.lambda_mkt,
             Sigmainv_assets_train,
             bl_model_Sassets.mu_bl)
  returns_weights_eq = {"mu_bl": bl_model_Sassets.mu_bl,
                        "mu_mkt": bl_model_Sassets.mu_mkt,
                        "mu_bl - mu_mkt": bl_model_Sassets.mu_bl -
                        bl_model_Sassets.mu_mkt,
                        "w_bl": w_bl,
                        "w_mkt": w_mkt,
                        "w_bl - w_mkt": w_bl - w_mkt
                        }
  df_returns_weights_eq = pd.DataFrame(returns_weights_eq,
                                       index=asset_names,)
#+end_src
#
The implied equilibrium returns, Black-Litterman returns and their respective weights can be found in Table [[df_returns_weights_eq]].
The difference represents the so called active risk
The way the views affect the allocations is clear: 
#+NAME: df_returns_weights_eq
#+begin_src python  :results raw :exports results :tangle no
  tabulate(df_returns_weights_eq.round(decimals=5),
           headers=df_returns_weights_eq.columns,
           showindex=True, tablefmt='orgtbl')
#+end_src
#+NAME: df_returns_weights_eq
#+CAPTION: Equilibrium return and portfolio weights
#+ATTR_LATEX: :width 0.7\textwidth :environment longtable :caption
#+RESULTS: df_returns_weights_eq
|      |   mu_bl |  mu_mkt | mu_bl - mu_mkt |     w_bl |   w_mkt | w_bl - w_mkt |
|------+---------+---------+----------------+----------+---------+--------------|
| JPM  | 0.00926 | 0.01121 |       -0.00195 |  0.30334 | 0.54442 |     -0.24107 |
| CVS  |  0.0054 | 0.00417 |        0.00122 |   0.1165 |  0.1165 |            0 |
| ATVI | 0.00618 |  0.0065 |       -0.00032 |  0.08757 | 0.08757 |           -0 |
| PH   | 0.00969 | 0.00677 |        0.00292 |  0.30706 | 0.06599 |      0.24107 |
| WELL | 0.00338 | 0.00345 |         -6e-05 |  0.05175 | 0.05175 |            0 |
| YUM  | 0.00478 | 0.00366 |        0.00111 |  0.04547 | 0.04547 |            0 |
| KR   | 0.00396 | 0.00276 |        0.00119 |  0.15388 | 0.04307 |      0.11082 |
| ATO  | 0.00135 | 0.00151 |       -0.00017 | -0.09014 | 0.02068 |     -0.11082 |
| EQT  | 0.00721 |  0.0035 |        0.00371 |  0.12009 | 0.01941 |      0.10069 |
| DXC  | 0.00835 | 0.00719 |        0.00116 |  0.00515 | 0.00515 |           -0 |
|      |         |         |                |          |         |              |

*** Portfolio  optimisation
#+NAME: Function to build portfolios weights
#+begin_src python :results none 
  def build_portfolio_weights(mu_targetlist: list,
                              x0: np.array,
                              mu_portfolio: np.array,
                              Sigma_portfolio: np.array,
                              cons_sett: dict,
                              annualise:int =12) -> list:
      """Builds an array of optimal portfolios

      Given a list of target returns, builds a variance minimization
      with constraints on the target returns

      Parameters
      ----------
      mu_targetlist : list
          Array of target returns
      x0 : np.array
          initial optimization point
      mu_portfolio : np.array
          excess portfolio returns
      Sigma_portfolio : np.array
          Portfolio covariance
      cons_sett : dict
          Constraint inputs
      annualise : int
          period to annualise (12 if data on mu and Sigma were
          calculated monthly)

      Returns
      -------
      list
          List of objects with optimization results

      """

      res_list = list()
      for mu_i in mu_targetlist:
          args = (mu_portfolio, #* annualise, # annualised
                  Sigma_portfolio,
                  mu_i / annualise)
          res = optimization.scipy_minimize("variance",
                                             x0,
                                             method_name='SLSQP',
                                             args=args,
                                             cons_sett=cons_sett,
                                             options=dict(maxiter=200,
                                                          ftol=1e-12))
          res_list.append(res)
      return res_list

#+end_src
#+NAME: Function to compute weights vs volatility for target returns
#+begin_src python :results none 

  def build_df_weightsvol(assets: list,
                          mu_targetlist: list,
                          x0: np.array,
                          mu_portfolio,
                          Sigma_portfolio,
                          annualise=12):
      """Builds an array of optimal portfolios

      Given a list of target returns, builds a variance minimization
      with constraints on the target returns

      Parameters
      ----------
      assets : list
          Array of assets composing the portfolio    
      mu_targetlist : list
          Array of target returns
      x0 : np.array
          initial optimization point
      mu_portfolio : np.array
          excess portfolio returns
      Sigma_portfolio : np.array
          Portfolio covariance
      annualise : int
          period to annualise (12 if data on mu and Sigma were
          calculated monthly)

      Returns
      -------
      pd.DataFrame
          DataFrame relating portfolio weights and volatilities

      """


      # constraints: returns equal to a number given in mu_targetlist,
      # weights equal to 1, and all weights greater than 0
      cons_sett = dict(eq_rets=dict(type="eq"),
                       eq_weights1=dict(type="eq"),
                       ieq_weights0=dict(type="ineq")
                       )

      res_list = build_portfolio_weights(mu_targetlist,
                                         x0,
                                         mu_portfolio,
                                         Sigma_portfolio,
                                         cons_sett,
                                         annualise
                                         )


      weights = np.array([ri.x for ri in res_list])
      Weights = weights.flatten()
      # anualise vols
      vols=[((ri.fun) * annualise)**0.5 for ri in res_list]
      Vols = [vi for vi in vols for i in range(len(assets))]
      Assets = [k for i in range(len(vols)) for k in assets]
      df_weights_vols = pd.DataFrame(dict(weights=Weights,
                                          vols=Vols,
                                          assets=Assets
                                          ))
      return df_weights_vols
#+end_src
Three types of optimisation are explored, namely *mean-variance* without constraints, for which analytical solutions are available; *variance* optimisation with constraints of weights and *Sharpe ratio* also with constraints. The optimisation is carried out using a Sequential Least Squares Programming (SLSQP) algorithm as implemented in Scipy. This algorithm allows for both nonlinear objective and constraints functions.  
**** Objective: Mean variance
The first optimization performed is a mean-variance without constraints and since there is a close form solution, a comparison is made for verification of the optimization framework. Three types of risk aversion are considered with $\lambda_{portfolio} = [0.1, 2.24, 6]$ (w1, w2, w3 respectively). The results are presented in Table [[df_meanvariance_weights]], illustrating a very good comparison between theoretical and optimised weights after setting the appropriate tolerances to the optimiser. It is worth highlighting the views have shifted allocation from JPM to PH, over a sixfold increase in the KR weight and a shorting position of ATO in favour of KR.     
#+NAME: Compute equilibrium returns 
#+begin_src python :results none 
  f_mu = lambda l, S, w: l * S @ w
  f_w = lambda l, Sinv, mu: 1/ l * Sinv @ mu
  mu_mkt_assets = f_mu(config.lambda_mkt, Sigma_assets_train, w_mkt)
  w1_mkt_theoretical = optimization.mean_variance_opt(mu_mkt_assets, Sigmainv_assets_train, config.lambda_portfolio[0])
  w2_mkt_theoretical = optimization.mean_variance_opt(mu_mkt_assets, Sigmainv_assets_train, config.lambda_portfolio[1])
  w3_mkt_theoretical = optimization.mean_variance_opt(mu_mkt_assets, Sigmainv_assets_train, config.lambda_portfolio[2])
  w1_bl_theoretical = optimization.mean_variance_opt(bl_model_Sassets.mu_bl, Sigmainv_assets_train, config.lambda_portfolio[0])
  w2_bl_theoretical = optimization.mean_variance_opt(bl_model_Sassets.mu_bl, Sigmainv_assets_train, config.lambda_portfolio[1])
  w3_bl_theoretical = optimization.mean_variance_opt(bl_model_Sassets.mu_bl, Sigmainv_assets_train, config.lambda_portfolio[2])

#+end_src

#+NAME: Calculate mean variance optimization
#+begin_src python :results none 
  res_portfolios_bl = []
  res_portfolios_mkt = []
  for lmb_p in config.lambda_portfolio:
      x0 = 1. / num_assets * np.ones(num_assets)
      args = (bl_model_Sassets.mu_bl,            
              bl_model_Sassets.Sigma,          
              lmb_p)
      res = optimization.scipy_minimize("mean_variance",
                                        x0,
                                        method_name='SLSQP',
                                        args=args,
                                        options=dict(maxiter=200,
                                                     ftol=1e-12))
      res_portfolios_bl.append(res)
      args = (bl_model_Sassets.mu_mkt,            
              bl_model_Sassets.Sigma,          
              lmb_p)
      res = optimization.scipy_minimize("mean_variance",
                                        x0,
                                        method_name='SLSQP',
                                        args=args,
                                        options=dict(maxiter=200,
                                                     ftol=1e-12))
      res_portfolios_mkt.append(res)
    
  #res1 = res_portfolios_bl[2]  
#+end_src

#+NAME: df_meanvariance_weights
#+begin_src python  :results raw :exports results :tangle no
  meanvariance_weights = {}
  meanvariance_weights['w1_{mkt-opt}'] = res_portfolios_mkt[0].x
  meanvariance_weights['w1_{mkt-theo}'] = w1_mkt_theoretical
  meanvariance_weights['w2_{mkt-opt}'] = res_portfolios_mkt[1].x
  meanvariance_weights['w2_{mkt-theo}'] = w2_mkt_theoretical
  meanvariance_weights['w3_{mkt-opt}'] = res_portfolios_mkt[2].x
  meanvariance_weights['w3_{mkt-theo}'] = w3_mkt_theoretical
  meanvariance_weights['w1_{bl-opt}'] = res_portfolios_bl[0].x
  meanvariance_weights['w1_{bl-theo}'] = w1_bl_theoretical
  meanvariance_weights['w2_{bl-opt}'] = res_portfolios_bl[1].x
  meanvariance_weights['w2_{bl-theo}'] = w2_bl_theoretical
  meanvariance_weights['w3_{bl-opt}'] = res_portfolios_bl[2].x
  meanvariance_weights['w3_{bl-theo}'] = w3_bl_theoretical
  df_meanvariance_weights = pd.DataFrame(meanvariance_weights,
                                         index=asset_names).transpose()
  tabulate(df_meanvariance_weights.round(decimals=4),
           headers=df_meanvariance_weights.columns,
           showindex=True, tablefmt='orgtbl')
#+end_src
#+NAME: df_meanvariance_weights
#+CAPTION: Mean variance optimization weights
#+ATTR_LATEX: :width 1\textwidth :environment longtable :caption
#+RESULTS: df_meanvariance_weights
|               |     JPM |    CVS |   ATVI |     PH |   WELL |    YUM |     KR |     ATO |    EQT |    DXC |
|---------------+---------+--------+--------+--------+--------+--------+--------+---------+--------+--------|
| w1_{mkt-opt}  | 12.1949 | 2.6095 | 1.9614 | 1.4779 | 1.1595 | 1.0187 |  0.965 |  0.4632 | 0.4347 | 0.1155 |
| w1_{mkt-theo} | 12.1949 | 2.6096 | 1.9615 | 1.4782 | 1.1591 | 1.0186 | 0.9647 |  0.4632 | 0.4347 | 0.1155 |
| w2_{mkt-opt}  |  0.5444 | 0.1165 | 0.0876 |  0.066 | 0.0517 | 0.0455 | 0.0431 |  0.0207 | 0.0194 | 0.0052 |
| w2_{mkt-theo} |  0.5444 | 0.1165 | 0.0876 |  0.066 | 0.0517 | 0.0455 | 0.0431 |  0.0207 | 0.0194 | 0.0052 |
| w3_{mkt-opt}  |  0.2032 | 0.0435 | 0.0327 | 0.0246 | 0.0193 |  0.017 | 0.0161 |  0.0077 | 0.0072 | 0.0019 |
| w3_{mkt-theo} |  0.2032 | 0.0435 | 0.0327 | 0.0246 | 0.0193 |  0.017 | 0.0161 |  0.0077 | 0.0072 | 0.0019 |
| w1_{bl-opt}   |  6.7948 | 2.6097 | 1.9615 | 6.8783 | 1.1591 | 1.0185 | 3.4471 | -2.0191 |   2.69 | 0.1155 |
| w1_{bl-theo}  |  6.7949 | 2.6096 | 1.9615 | 6.8782 | 1.1591 | 1.0186 |  3.447 | -2.0191 | 2.6901 | 0.1155 |
| w2_{bl-opt}   |  0.3033 | 0.1165 | 0.0876 | 0.3071 | 0.0518 | 0.0455 | 0.1539 | -0.0901 | 0.1201 | 0.0052 |
| w2_{bl-theo}  |  0.3033 | 0.1165 | 0.0876 | 0.3071 | 0.0517 | 0.0455 | 0.1539 | -0.0901 | 0.1201 | 0.0052 |
| w3_{bl-opt}   |  0.1132 | 0.0435 | 0.0327 | 0.1146 | 0.0193 |  0.017 | 0.0574 | -0.0337 | 0.0448 | 0.0019 |
| w3_{bl-theo}  |  0.1132 | 0.0435 | 0.0327 | 0.1146 | 0.0193 |  0.017 | 0.0574 | -0.0337 | 0.0448 | 0.0019 |

For backtesting the performance of the computed portfolios, the cumulative returns are compared on the testing data against the Naive porfolio with constant weights of 0.1 across the 10 assets. Although both optimised portfolios perform better than the Naive one, the results in Fig. [[P&L_plot_meanvariance]] remark the importance of the addition on the views to keep a positive trend in the returns when there is perhaps a period of less growth.
#+begin_src python :results none 
  weights_sol_mkt = {k: res_portfolios_mkt[1].x[i] for i, k in enumerate(asset_names)}
  weights_sol_bl = {k: res_portfolios_bl[1].x[i] for i, k in enumerate(asset_names)}
  #weights_sol = {k: w1_assets_opt[i] for i, k in enumerate(asset_names)}
  df_profits_sol_mkt = ml4qf.utils.profit_portfolio(df_assets_test, weights_sol_mkt)
  df_profits_sol_bl = ml4qf.utils.profit_portfolio(df_assets_test, weights_sol_bl)
  weights_naive = {k: x0[i] for i, k in enumerate(asset_names)}
  df_profits_naive = ml4qf.utils.profit_portfolio(df_assets_test, weights_naive)
  weights_sp500 = {'GSPC': 1.}
  df_profits_sp500 = ml4qf.utils.profit_portfolio(df_sp500_test, weights_sp500)

  df_rf = ml4qf.utils.profit_portfolio(df_factors_test[['RF']], {'RF':(sum(res1.x)-1)})
  df_profits_meanvariance = pd.DataFrame(np.array([
      np.hstack([1,df_profits_sol_bl.sum(axis=1).to_numpy()-df_rf.to_numpy()[:,0]]),
      np.hstack([1,df_profits_sol_mkt.sum(axis=1).to_numpy()-df_rf.to_numpy()[:,0]]),
      np.hstack([1,df_profits_naive.sum(axis=1).to_numpy()]),
      np.hstack([1,df_profits_sp500.sum(axis=1).to_numpy()])
  ]).T,
                                     columns=['Opt-BL', 'Opt-mkt', 'Naive', 'SP500'],
                                     index=df_profits_sol.index.insert(
                                         0,df_assets_train.index[-1]))

#+end_src

#+NAME: P&L_plot_meanvariance
#+begin_src python :results value file  :exports results :var name=(org-element-property :name  (org-element-context)) 
  fig1_path= img_dir / f'{name}.png'
  fig1 = px.line(df_profits_meanvariance,
                 y=['Opt-BL', 'Opt-mkt', 'Naive', 'SP500'],
                 labels={'value':'Comulative returns'},
                 markers=True)
  fig1.write_image(fig1_path)
  fig1_path
#+end_src
#+NAME: P&L_plot_meanvariance
#+CAPTION: Cumulative returns for mean-variance optimization.
#+ATTR_LATEX: :width 1\textwidth :environment longtable :caption
#+RESULTS: P&L_plot_meanvariance
[[file:img/P&L_plot_meanvariance.png]]

**** Objective: variance with constraints
By setting the portfolio variance as the optimization objective and adding as constraints a target return (that is varied) and weights to be bigger than 0, one can construct a portfolio composition map for varying volatilities. This is shown in Figures [[Weights_Composition_mkt]] and [[Weights_Composition_bl]] for the expected equilibrium returns without and with views respectively.     
#+NAME: df for variance weights composition
#+begin_src python :results none 
  # df for variance weights composition
  lmb_p = config.lambda_portfolio[2]
  x0 = 1. / num_assets * np.ones(num_assets)
  mu_targetlist = np.linspace(4,18,16) * 1e-2
  df_weightsvols_bl = build_df_weightsvol(asset_names, mu_targetlist, x0,
                                        bl_model_Sassets.mu_bl,
                                        bl_model_Sassets.Sigma)
  df_weightsvols_mkt = build_df_weightsvol(asset_names, mu_targetlist, x0,
                                        bl_model_Sassets.mu_mkt,
                                        bl_model_Sassets.Sigma)

#+end_src

#+NAME: Weights_Composition_mkt
#+begin_src python :results value file  :exports results :var name=(org-element-property :name  (org-element-context)) 
  fig1_path= img_dir / f'{name}.png'
  fig1 = px.area(df_weightsvols_mkt, x="vols", y="weights", color="assets",
                #pattern_shape_sequence=[".", "x", "+"],              
                pattern_shape="assets"
                )
  fig1.write_image(fig1_path)
  fig1_path
#+end_src
#+NAME: Weights_Composition_mkt
#+CAPTION: Allocations weights for optimal portfolio with varying volatilities. 
#+ATTR_LATEX: :width 0.75\textwidth 
#+RESULTS: Weights_Composition_mkt
[[file:img/Weights_Composition_mkt.png]]

#+NAME: Weights_Composition_bl
#+begin_src python :results value file  :exports results :var name=(org-element-property :name  (org-element-context)) 
  fig1_path= img_dir / f'{name}.png'
  fig1 = px.area(df_weightsvols_bl, x="vols", y="weights", color="assets",
                #pattern_shape_sequence=[".", "x", "+"],              
                pattern_shape="assets"
                )
  fig1.write_image(fig1_path)
  fig1_path
#+end_src
#+NAME: Weights_Composition_bl
#+CAPTION: Black-Litterman allocations weights for optimal portfolio with varying volatilities. 
#+ATTR_LATEX: :width 0.75\textwidth 
#+RESULTS: Weights_Composition_bl
[[file:img/Weights_Composition_bl.png]]

Next the P&L of the variance optimisation is calculated for a target return of 10% annually which leads to a portfolio volatility of around 20%. As shown in Fig. [[P&L_plot_variance]], the portfolio with views consistently outperforms the Naive portfolio, not so the portfolio with implied market returns.  
#+NAME: Compute variance optimization
#+begin_src python :results none 
  # Compute variance optimization
  x0 = 1. / num_assets * np.ones(num_assets)
  args = (bl_model_Sassets.mu_mkt,            
          bl_model_Sassets.Sigma,          
          0.10/12)

  cons_sett = dict(eq_rets=dict(type="eq"),
                   eq_weights1=dict(type="eq"),
                   ieq_weights0=dict(type="ineq")
                   )
  resv_mkt = optimization.scipy_minimize("variance",
                                     x0,
                                     method_name='SLSQP',
                                     args=args,
                                     cons_sett=cons_sett,
                                     options=dict(maxiter=200,
                                                  ftol=1e-12))

  print(np.dot(resv_mkt.x, bl_model_Sassets.Sigma @ resv_mkt.x)**0.5 * 12**0.5 * 100)
  print(resv_mkt.fun**0.5 * 12**0.5 * 100)
  print(sum(resv_mkt.x))
  print(resv_mkt.x)
  print('#####')
  args = (bl_model_Sassets.mu_bl,            
          bl_model_Sassets.Sigma,          
          0.10/12)
  resv_bl = optimization.scipy_minimize("variance",
                                        x0,
                                        method_name='SLSQP',
                                        args=args,
                                        cons_sett=cons_sett,
                                        options=dict(maxiter=200,
                                                     ftol=1e-12))

  print(np.dot(resv_bl.x, bl_model_Sassets.Sigma @ resv_bl.x)**0.5 * 12**0.5 * 100)
  print(resv_bl.fun**0.5 * 12**0.5 * 100)
  print(sum(resv_bl.x))
  print(resv_bl.x)

#+end_src

#+begin_src python :results none 
  weights_sol_mkt = {k: resv_mkt.x[i] for i, k in enumerate(asset_names)}
  weights_sol_bl = {k: resv_bl.x[i] for i, k in enumerate(asset_names)}
  #weights_sol = {k: w1_assets_opt[i] for i, k in enumerate(asset_names)}
  df_profits_sol_mkt = ml4qf.utils.profit_portfolio(df_assets_test, weights_sol_mkt)
  df_profits_sol_bl = ml4qf.utils.profit_portfolio(df_assets_test, weights_sol_bl)
  weights_naive = {k: x0[i] for i, k in enumerate(asset_names)}  
  df_profits_naive = ml4qf.utils.profit_portfolio(df_assets_test, weights_naive)
  weights_sp500 = {'GSPC': 1.}
  df_profits_sp500 = ml4qf.utils.profit_portfolio(df_sp500_test, weights_sp500)

  df_rf = ml4qf.utils.profit_portfolio(df_factors_test[['RF']], {'RF':(sum(res1.x)-1)})
  df_profits_variance = pd.DataFrame(np.array([
      np.hstack([1,df_profits_sol_bl.sum(axis=1).to_numpy()-df_rf.to_numpy()[:,0]]),
      np.hstack([1,df_profits_sol_mkt.sum(axis=1).to_numpy()-df_rf.to_numpy()[:,0]]),
      np.hstack([1,df_profits_naive.sum(axis=1).to_numpy()]),
      np.hstack([1,df_profits_sp500.sum(axis=1).to_numpy()])
  ]).T,
                                     columns=['Opt-BL', 'Opt-mkt', 'Naive', 'SP500'],
                                     index=df_profits_sol.index.insert(
                                         0,df_assets_train.index[-1]))
  
#+end_src

#+NAME: P&L_plot_variance
#+begin_src python :results value file  :exports results :var name=(org-element-property :name  (org-element-context)) 
  fig1_path= img_dir / f'{name}.png'
  fig1 = px.line(df_profits_variance,
                 y=['Opt-BL', 'Opt-mkt', 'Naive', 'SP500'],
                 labels={'value':'Comulative returns'},
                 markers=True)
  fig1.write_image(fig1_path)
  fig1_path
#+end_src
#+CAPTION: Cumulative returns for variance optimization.   
#+ATTR_LATEX: :width 0.75\textwidth 
#+NAME: P&L_plot_variance
#+RESULTS: P&L_plot_variance
[[file:img/P&L_plot_variance.png]]

#+NAME: df_varianceopt_weights
#+begin_src python  :results raw :exports results :tangle no
  varianceopt_weights = {}
  varianceopt_weights = {'w_naive': [1 / num_assets for i in range(num_assets)],
                         'w_opt-BL': resv_bl.x,
                         'w_opt-mkt': resv_mkt.x}
  df_varianceopt_weights = pd.DataFrame(varianceopt_weights, index=asset_names).transpose()
  tabulate(df_varianceopt_weights.round(decimals=2),
           headers=df_varianceopt_weights.columns,
           showindex=True, tablefmt='orgtbl')
#+end_src
#+NAME: df_varianceopt_weights
#+CAPTION: Resulting portfolio weights of variance optimization
#+ATTR_LATEX: :width 0.7\textwidth :environment longtable :caption
#+RESULTS: df_varianceopt_weights
|           |  JPM |  CVS | ATVI |   PH | WELL |  YUM |   KR |  ATO |  EQT |  DXC |
|-----------+------+------+------+------+------+------+------+------+------+------|
| w_naive   |  0.1 |  0.1 |  0.1 |  0.1 |  0.1 |  0.1 |  0.1 |  0.1 |  0.1 |  0.1 |
| w_opt-BL  | 0.32 | 0.06 | 0.07 | 0.36 |   -0 |    0 | 0.08 |   -0 | 0.11 | 0.01 |
| w_opt-mkt | 0.56 | 0.12 | 0.09 | 0.07 | 0.05 | 0.04 | 0.04 | 0.01 | 0.02 | 0.01 |

**** Objective: Sharpe ratio
#+NAME: Compute sharpe optimization
#+begin_src python :results none 

  x0 = 1. / num_assets * np.ones(num_assets)
  args = (bl_model_Sassets.mu_mkt,            
          bl_model_Sassets.Sigma,          
          0.)

  cons_sett = dict(eq_weights1=dict(type="eq"),
                   ieq_weights0=dict(type="ineq")
                   )

  resp_mkt = optimization.scipy_minimize("sharpe",
                                     x0,
                                     method_name='SLSQP',
                                     args=args,
                                     cons_sett=cons_sett,
                                     options=dict(maxiter=200,
                                                  ftol=1e-12))

  print(np.dot(resp_mkt.x, bl_model_Sassets.Sigma @ resp_mkt.x)**0.5 * 12**0.5 * 100)
  print(resp_mkt.fun)
  print(sum(resp_mkt.x))
  print(resp_mkt.x)
  print('#####')
  args = (bl_model_Sassets.mu_bl,            
          bl_model_Sassets.Sigma,          
          0.)
  resp_bl = optimization.scipy_minimize("sharpe",
                                       x0,
                                       method_name='SLSQP',
                                       args=args,
                                       cons_sett=cons_sett,
                                       options=dict(maxiter=200,
                                                    ftol=1e-12))

  # print(np.dot(resp_bl.x, bl_model_Sassets.Sigma @ resp_bl.x)**0.5 * 12**0.5 * 100)
  # print(resp_bl.fun**0.5 * 12**0.5 * 100)
  # print(sum(resp_bl.x))
  # print(resp_bl.x)
  print(np.dot(resp_bl.x, bl_model_Sassets.Sigma @ resp_bl.x)**0.5 * 12**0.5 * 100)
  print(resp_bl.fun)
  print(sum(resp_bl.x))
  print(resp_bl.x)

#+end_src

#+begin_src python :results none 
  weights_sol_mkt = {k: res_mkt.x[i] for i, k in enumerate(asset_names)}
  weights_sol_bl = {k: res_bl.x[i] for i, k in enumerate(asset_names)}
  #weights_sol = {k: w1_assets_opt[i] for i, k in enumerate(asset_names)}
  df_profits_sol_mkt = ml4qf.utils.profit_portfolio(df_assets_test, weights_sol_mkt)
  df_profits_sol_bl = ml4qf.utils.profit_portfolio(df_assets_test, weights_sol_bl)
  weights_naive = {k: x0[i] for i, k in enumerate(asset_names)}  
  df_profits_naive = ml4qf.utils.profit_portfolio(df_assets_test, weights_naive)
  weights_sp500 = {'GSPC': 1.}
  df_profits_sp500 = ml4qf.utils.profit_portfolio(df_sp500_test, weights_sp500)

  df_rf = ml4qf.utils.profit_portfolio(df_factors_test[['RF']], {'RF':(sum(res1.x)-1)})
  df_profits_sharpe = pd.DataFrame(np.array([
      np.hstack([1,df_profits_sol_bl.sum(axis=1).to_numpy()-df_rf.to_numpy()[:,0]]),
      np.hstack([1,df_profits_sol_mkt.sum(axis=1).to_numpy()-df_rf.to_numpy()[:,0]]),
      np.hstack([1,df_profits_naive.sum(axis=1).to_numpy()]),
      np.hstack([1,df_profits_sp500.sum(axis=1).to_numpy()])
  ]).T,
                                     columns=['Opt-BL', 'Opt-mkt', 'Naive', 'SP500'],
                                     index=df_profits_sol.index.insert(
                                         0,df_assets_train.index[-1]))
  
#+end_src

#+NAME: P&L_plot_sharpe
#+begin_src python :results value file  :exports results :var name=(org-element-property :name  (org-element-context)) 
  fig1_path= img_dir / f'{name}.png'
  fig1 = px.line(df_profits_sharpe,
                 y=['Opt-BL', 'Opt-mkt', 'Naive', 'SP500'],
                 labels={'value':'Comulative returns'},
                 markers=True)
  fig1.write_image(fig1_path)
  fig1_path
#+end_src
#+CAPTION: Cumulative returns for Sharpe optimization.   
#+ATTR_LATEX: :width 0.75\textwidth 
#+NAME: P&L_plot_sharpe
#+RESULTS: P&L_plot_sharpe
[[file:img/P&L_plot_sharpe.png]]

#+NAME: df_sharpeopt_weights
#+begin_src python  :results raw :exports results :tangle no
  sharpeopt_weights = {}
  sharpeopt_weights = {'w_naive': [1 / num_assets for i in range(num_assets)],
                         'w_opt-BL': resp_bl.x,
                         'w_opt-mkt': resp_mkt.x}
  df_sharpeopt_weights = pd.DataFrame(sharpeopt_weights, index=asset_names).transpose()
  tabulate(df_sharpeopt_weights.round(decimals=2),
           headers=df_sharpeopt_weights.columns,
           showindex=True, tablefmt='orgtbl')
#+end_src
#+NAME: df_sharpeopt_weights
#+CAPTION: Resulting portfolio weights of Sharpe optimization
#+ATTR_LATEX: :width 0.7\textwidth :environment longtable :caption
#+RESULTS: df_sharpeopt_weights
|           |  JPM |  CVS | ATVI |   PH | WELL |  YUM |   KR |  ATO |  EQT |  DXC |
|-----------+------+------+------+------+------+------+------+------+------+------|
| w_naive   |  0.1 |  0.1 |  0.1 |  0.1 |  0.1 |  0.1 |  0.1 |  0.1 |  0.1 |  0.1 |
| w_opt-BL  | 0.26 |  0.1 | 0.07 | 0.26 | 0.03 | 0.04 | 0.13 |    0 |  0.1 | 0.01 |
| w_opt-mkt | 0.54 | 0.12 | 0.09 | 0.07 | 0.05 | 0.05 | 0.04 | 0.02 | 0.02 | 0.01 |




* Conclusions and further work
A Black-Litterman portfolio optimisation with factors has been implemented. Views were generated using a statistical ARIMA process to forecast over 2 years onto the future using 20 years of past data. Mean-variance, variance and Sharpe ratio optimisations with constraints were carried out and the resulting optimised portfolio with views is shown to outperform both the S&P500 index and a naive selection of 1/N assets.

As for future work three main areas are identified where important improvements can be made: 1) the factors prediction would be much stronger with a Machine Learning model such as an LSTM; and better predictions mean more relevant views 2) A covariance treatment with denoising would make the results from the optimisation more robust. 3) using a more advance optimisation strategy such as De Prado's Hierarchical Risk Parity.
#+LaTeX: \appendix
* Code installation, execution, and structure
<<sec:appx_code>>

The codes herein have been tested in Linux (Ubuntu 22 and Centos 8) and in MacOs. To install and execute follow the next steps.

** Installing the code
For the installation it is recommended to use a Python environment manager such as Conda and with Python >=3.10. Codes reside in the folder ML4qf and it is install like a normal package: navigate to the ML4qf directory in a terminal and run 'pip install .' The package should now be installed and a good check is to run the tests as follows. 
** Testing
A range of tests have been implemented using the library pytest to validate the codes in this work.
They are located in the folder ./test and can be run by navigating to this folder and running 'pytest' in the terminal.  
** Literate programming
Both the pdf from code have been simultaneously generated from an Emacs .org file. This type of file bears resemblance with Python notebooks but it is more powerful, albeit being also much older.
tangle
export
* Factors OLS
<<sec:appx_factors>>
- JPM results:
  #+RESULTS: SummaryfactorsOLS_JPM
  #+begin_example
                              OLS Regression Results                            
  ==============================================================================
  Dep. Variable:                      y   R-squared:                       0.659
  Model:                            OLS   Adj. R-squared:                  0.650
  Method:                 Least Squares   F-statistic:                     76.91
  Date:                Mon, 21 Aug 2023   Prob (F-statistic):           5.04e-53
  Time:                        10:57:22   Log-Likelihood:                 384.56
  No. Observations:                 246   AIC:                            -755.1
  Df Residuals:                     239   BIC:                            -730.6
  Df Model:                           6                                         
  Covariance Type:            nonrobust                                         
  ==============================================================================
                   coef    std err          t      P>|t|      [0.025      0.975]
  ------------------------------------------------------------------------------
  const          0.0065      0.004      1.848      0.066      -0.000       0.013
  x1             0.9222      0.090     10.197      0.000       0.744       1.100
  x2            -0.2854      0.130     -2.191      0.029      -0.542      -0.029
  x3             1.1856      0.146      8.093      0.000       0.897       1.474
  x4            -1.0721      0.166     -6.477      0.000      -1.398      -0.746
  x5            -0.4933      0.220     -2.245      0.026      -0.926      -0.061
  x6            -0.2732      0.072     -3.806      0.000      -0.415      -0.132
  ==============================================================================
  Omnibus:                       37.395   Durbin-Watson:                   2.142
  Prob(Omnibus):                  0.000   Jarque-Bera (JB):              127.988
  Skew:                           0.575   Prob(JB):                     1.61e-28
  Kurtosis:                       6.342   Cond. No.                         75.0
  ==============================================================================

  Notes:
  [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
  #+end_example
- CVS results:
  #+RESULTS: SummaryfactorsOLS_CVS
  #+begin_example
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.250
Model:                            OLS   Adj. R-squared:                  0.232
Method:                 Least Squares   F-statistic:                     13.31
Date:                Mon, 21 Aug 2023   Prob (F-statistic):           5.26e-13
Time:                        10:53:37   Log-Likelihood:                 327.04
No. Observations:                 246   AIC:                            -640.1
Df Residuals:                     239   BIC:                            -615.5
Df Model:                           6                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const         -0.0015      0.004     -0.335      0.738      -0.010       0.007
x1             0.8682      0.114      7.598      0.000       0.643       1.093
x2            -0.2158      0.165     -1.311      0.191      -0.540       0.108
x3             0.0122      0.185      0.066      0.948      -0.352       0.377
x4             0.2534      0.209      1.212      0.227      -0.159       0.665
x5             1.0739      0.278      3.869      0.000       0.527       1.621
x6             0.0267      0.091      0.294      0.769      -0.152       0.205
==============================================================================
Omnibus:                       15.254   Durbin-Watson:                   2.057
Prob(Omnibus):                  0.000   Jarque-Bera (JB):               36.526
Skew:                          -0.197   Prob(JB):                     1.17e-08
Kurtosis:                       4.846   Cond. No.                         75.0
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
#+end_example
- ATVI results:  
  #+RESULTS: SummaryfactorsOLS_ATVI
#+begin_example
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.165
Model:                            OLS   Adj. R-squared:                  0.144
Method:                 Least Squares   F-statistic:                     7.867
Date:                Mon, 21 Aug 2023   Prob (F-statistic):           9.59e-08
Time:                        10:53:29   Log-Likelihood:                 204.59
No. Observations:                 246   AIC:                            -395.2
Df Residuals:                     239   BIC:                            -370.6
Df Model:                           6                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          0.0198      0.007      2.712      0.007       0.005       0.034
x1             0.8898      0.188      4.734      0.000       0.520       1.260
x2             0.2429      0.271      0.897      0.371      -0.291       0.776
x3            -0.2061      0.304     -0.677      0.499      -0.806       0.394
x4            -0.4707      0.344     -1.368      0.173      -1.148       0.207
x5             0.2124      0.457      0.465      0.642      -0.687       1.112
x6             0.3921      0.149      2.628      0.009       0.098       0.686
==============================================================================
Omnibus:                       42.871   Durbin-Watson:                   1.836
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              151.251
Skew:                           0.667   Prob(JB):                     1.43e-33
Kurtosis:                       6.602   Cond. No.                         75.0
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
#+end_example
- PH results:  
  #+RESULTS: SummaryfactorsOLS_PH
#+begin_example
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.607
Model:                            OLS   Adj. R-squared:                  0.597
Method:                 Least Squares   F-statistic:                     61.61
Date:                Mon, 21 Aug 2023   Prob (F-statistic):           8.36e-46
Time:                        10:53:20   Log-Likelihood:                 370.35
No. Observations:                 246   AIC:                            -726.7
Df Residuals:                     239   BIC:                            -702.2
Df Model:                           6                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const         -0.0038      0.004     -1.031      0.304      -0.011       0.003
x1             1.4929      0.096     15.580      0.000       1.304       1.682
x2             0.4461      0.138      3.232      0.001       0.174       0.718
x3            -0.1243      0.155     -0.801      0.424      -0.430       0.181
x4             1.1458      0.175      6.534      0.000       0.800       1.491
x5             0.4756      0.233      2.043      0.042       0.017       0.934
x6            -0.2364      0.076     -3.108      0.002      -0.386      -0.087
==============================================================================
Omnibus:                        2.827   Durbin-Watson:                   2.164
Prob(Omnibus):                  0.243   Jarque-Bera (JB):                3.056
Skew:                          -0.008   Prob(JB):                        0.217
Kurtosis:                       3.546   Cond. No.                         75.0
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
#+end_example
  
* ARIMA results summary
<<sec:appx_arima>>

- MKT-RF factor model parameters
#+RESULTS: ARIMA_summary_mkt
#+begin_example
                               SARIMAX Results                                
==============================================================================
Dep. Variable:                      y   No. Observations:                  246
Model:               ARIMA(15, 0, 15)   Log Likelihood                 410.138
Date:                Mon, 21 Aug 2023   AIC                           -756.277
Time:                        12:12:56   BIC                           -646.258
Sample:                             0   HQIC                          -711.898
                                - 246                                         
Covariance Type:                  opg                                         
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
const          0.0075      0.005      1.620      0.105      -0.002       0.017
ar.L1          0.2005      0.632      0.317      0.751      -1.037       1.438
ar.L2          0.2156      0.458      0.470      0.638      -0.683       1.114
ar.L3         -0.2141      0.312     -0.686      0.493      -0.826       0.398
ar.L4          0.0598      0.309      0.194      0.846      -0.545       0.665
ar.L5         -0.3863      0.229     -1.690      0.091      -0.834       0.062
ar.L6          0.2546      0.317      0.804      0.421      -0.366       0.875
ar.L7          0.0904      0.289      0.313      0.755      -0.476       0.657
ar.L8         -0.2626      0.387     -0.679      0.497      -1.021       0.495
ar.L9         -0.0458      0.243     -0.189      0.850      -0.521       0.430
ar.L10        -0.0143      0.213     -0.067      0.946      -0.431       0.402
ar.L11         0.1538      0.217      0.707      0.479      -0.272       0.580
ar.L12         0.2686      0.225      1.191      0.233      -0.173       0.710
ar.L13        -0.1839      0.270     -0.680      0.496      -0.714       0.346
ar.L14        -0.2374      0.180     -1.321      0.187      -0.590       0.115
ar.L15         0.1025      0.237      0.432      0.665      -0.362       0.567
ma.L1         -0.2907      0.737     -0.394      0.693      -1.735       1.154
ma.L2         -0.3223      0.534     -0.604      0.546      -1.369       0.724
ma.L3          0.4526      0.569      0.796      0.426      -0.662       1.567
ma.L4          0.0779      0.495      0.157      0.875      -0.892       1.047
ma.L5          0.6181      0.410      1.508      0.132      -0.185       1.422
ma.L6         -0.3270      0.506     -0.647      0.518      -1.318       0.664
ma.L7         -0.0085      0.522     -0.016      0.987      -1.031       1.014
ma.L8          0.6006      0.762      0.788      0.431      -0.894       2.095
ma.L9         -0.1827      0.536     -0.341      0.733      -1.234       0.868
ma.L10        -0.1139      0.349     -0.326      0.744      -0.799       0.571
ma.L11        -0.3142      0.305     -1.029      0.304      -0.913       0.284
ma.L12        -0.2274      0.402     -0.566      0.571      -1.015       0.560
ma.L13         0.3438      0.544      0.632      0.528      -0.723       1.410
ma.L14         0.0621      0.339      0.183      0.855      -0.603       0.727
ma.L15         0.1019      0.318      0.321      0.749      -0.521       0.725
sigma2         0.0012      0.001      1.746      0.081      -0.000       0.003
===================================================================================
Ljung-Box (L1) (Q):                   0.02   Jarque-Bera (JB):                25.25
Prob(Q):                              0.89   Prob(JB):                         0.00
Heteroskedasticity (H):               1.58   Skew:                            -0.36
Prob(H) (two-sided):                  0.05   Kurtosis:                         4.45
===================================================================================

Warnings:
[1] Covariance matrix calculated using the outer product of gradients (complex-step).
#+end_example
#+RESULTS: ARIMA_summary_cma
#+begin_example
                               SARIMAX Results                                
==============================================================================
Dep. Variable:                      y   No. Observations:                  246
Model:                ARIMA(6, 4, 12)   Log Likelihood                 571.772
Date:                Mon, 21 Aug 2023   AIC                          -1105.545
Time:                        12:12:39   BIC                          -1040.304
Sample:                             0   HQIC                         -1079.225
                                - 246                                         
Covariance Type:                  opg                                         
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
ar.L1         -2.4798      0.148    -16.777      0.000      -2.769      -2.190
ar.L2         -3.5526      0.364     -9.753      0.000      -4.267      -2.839
ar.L3         -3.4898      0.517     -6.746      0.000      -4.504      -2.476
ar.L4         -2.6013      0.481     -5.405      0.000      -3.545      -1.658
ar.L5         -1.3663      0.283     -4.824      0.000      -1.921      -0.811
ar.L6         -0.4168      0.097     -4.317      0.000      -0.606      -0.228
ma.L1         -0.8129      0.194     -4.194      0.000      -1.193      -0.433
ma.L2         -0.6925      0.173     -4.000      0.000      -1.032      -0.353
ma.L3         -0.4260      0.201     -2.116      0.034      -0.821      -0.031
ma.L4          0.5199      0.188      2.764      0.006       0.151       0.889
ma.L5          0.3480      0.193      1.801      0.072      -0.031       0.727
ma.L6          0.4938      0.218      2.261      0.024       0.066       0.922
ma.L7         -0.1631      0.130     -1.259      0.208      -0.417       0.091
ma.L8         -0.3331      0.134     -2.483      0.013      -0.596      -0.070
ma.L9          0.1170      0.155      0.756      0.450      -0.186       0.420
ma.L10         0.2265      0.136      1.662      0.097      -0.041       0.494
ma.L11        -0.4269      0.088     -4.852      0.000      -0.599      -0.254
ma.L12         0.1499      0.097      1.552      0.121      -0.039       0.339
sigma2         0.0004   5.53e-05      6.665      0.000       0.000       0.000
===================================================================================
Ljung-Box (L1) (Q):                   2.52   Jarque-Bera (JB):                16.94
Prob(Q):                              0.11   Prob(JB):                         0.00
Heteroskedasticity (H):               0.71   Skew:                            -0.37
Prob(H) (two-sided):                  0.14   Kurtosis:                         4.11
===================================================================================

Warnings:
[1] Covariance matrix calculated using the outer product of gradients (complex-step).
#+end_example

