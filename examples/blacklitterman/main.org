#+TITLE: Portfolio Construction using Black-Litterman Model and Factors
#+AUTHOR: Alvaro Cea
#+PROPERTY: header-args :tangle ./main.py :mkdirp yes
#+LATEX_HEADER: \usepackage[margin=1in]{geometry}
#+LATEX_HEADER: \usepackage{mathtools}

#+begin_comment
#+OPTIONS: toc:nil
#+LATEX_HEADER: \let\oldsection\section
#+LATEX_HEADER: \renewcommand{\section}{\clearpage\oldsection}
#+LATEX_HEADER: \let\oldsubsection\subsection
#+LATEX_HEADER: \renewcommand{\subsection}{\clearpage\oldsubsection}
#+end_comment

* House keeping :noexport:
#+begin_src elisp :results none :exports none
  (add-to-list 'org-structure-template-alist
  '("sp" . "src python :session py1"))
  (add-to-list 'org-structure-template-alist
  '("se" . "src elisp"))

  (setq org-confirm-babel-evaluate nil)
  (pyvenv-workon "ml4qf")
  (require 'org-tempo)
  (setq org-format-latex-options (plist-put org-format-latex-options :scale 2.0))
  (setq org-latex-pdf-process (list "latexmk -f -synctex=1 -pdf %f"))
  ;; (setq org-latex-pdf-process (list "latexmk -f -pdf -interaction=nonstopmode -output-directory=%o %f"))

#+end_src

#+begin_src python :session py1 :tangle yes :results none :exports none
  import pandas as pd
  import numpy as np
  import yfinance as yf
  import statsmodels.api as sm
  import getFamaFrenchFactors as gff
  import pathlib
  import datetime
  import importlib
  import ml4qf
  import ml4qf.collectors.financial_features as financial_features
  import ml4qf.collectors.financial_factors as financial_factors
  import ml4qf.collectors.financial_markets as financial_markets
  from ml4qf.predictors.model_stats import regression_OLS
  import ml4qf.predictors.model_stats as model_stats
  from tabulate import tabulate
  import plotly.express as px
  import plotly.graph_objects as go
  import matplotlib.pyplot as plt
  from pandas.plotting import autocorrelation_plot
  import config
  importlib.reload(config)
  img_dir = pathlib.Path("./img/")
  #img_dir = img_dir0.absolute()
  img_dir.mkdir(parents=True, exist_ok=True)
#+end_src

* Introduction
** Implemented computational tools and external libraries
- getFamaFrenchFactors
  gets data for fame french factors from the Kenneth French library
  https://pypi.org/project/getFamaFrenchFactors/
** Specifications:
For risk-free rate, 3M US Treasury from pandas FRED dataset/ECB website
rates for EUR/some small constant rate/zero rate – all are acceptable.
Use 2-3 year sample, which means > 500 daily returns.
** Index selection
The S&P 500 is considered a better reflection of the market’s performance across all sectors compared to the Nasdaq Composite and the Dow

#+begin_comment
#+CAPTION: Modal shape 1 
#+ATTR_LATEX: :width 0.75\textwidth
#+ATTR_ORG: :width 100
[[./img/polimi-M0.png]]
#+end_comment

* Theory
** Fama-French factors

It is one of the multi-factor models which is widely used in both academia and industry to estimate the excess return of an investment asset. It is an extension to Capital Asset Pricing Model (CAPM) by adding two additional factors apart from the market risk when estimating the excess returns of an asset. The three factors considered in this model are:

    Market factor (MKT) — Excess market return
    Size factor (SMB) — Excess return with a small market cap over those with a large market cap
    Value factor (HML) — Excess return of value stocks over growth stocks.

The Fama-French model is widely known as a stock market benchmark to evaluate investment performance.

$$
E[r_a] = \mu_a + \beta E[r_f]  + \epsilon_a
$$

$$
Var[r_a] = \mu_a + \beta r_f  + \epsilon_a
$$

$$
\Pi = w_a^{\top} r_a
$$

$$
Var(\Pi) = Var(r_a^{\top} w_a) = Var(r_a^{\top} w_a)
$$

** ARIMA model for time series
AutoRegressive Integrated Moving Average (ARIMA) statistical models are used 
AR: Autoregression. A model that uses the dependent relationship between an observation and some number of lagged observations.
I: Integrated. The use of differencing of raw observations (e.g. subtracting an observation from an observation at the previous time step) in order to make the time series stationary.
MA: Moving Average. A model that uses the dependency between an observation and a residual error from a moving average model applied to lagged observations.

Each of these components are explicitly specified in the model as a parameter. A standard notation is used of ARIMA(p,d,q) where the parameters are substituted with integer values to quickly indicate the specific ARIMA model being used.

The parameters of the ARIMA model are defined as follows:

- p: Number of lags in the observations that included in the model.
- d: Number of times differencing is applied to the observations.
- q: Size of moving average window.

** Black-Litterman model

$$
\mu_{eq} = \lambda \Sigma w_{mkt}
$$

- $\mu_{eq}$ is the Implied Excess Equilibrium Return Vector 
- $\lambda$ is the risk aversion coefficient, \lambda = \frac{E(\mu) - \mu_r}{\sigma^2}$
- $\Sigma$ is the covariance matrix of excess returns
- $w_{mkt}$ are market capitalization weights

$$
E[\mu_{bl}] = \left[(\tau \Sigma)^{-1} + P'\Omega^{-1}P\right]^{-1} \left[(\tau \Sigma)^{-1}\mu_{eq} + P'\Omega^{-1}Q\right]  
$$

- $E[\mu_{bl}]$: Posterior combined return vector ($N\times a$)
- $\tau$: error in views
- $P$: Matrix identifying assets involved in the views ($K\times N$)
- $Q$: View vector ($K\times 1$)
- $\Omega$: Diagonal covariance matrix with the errors expressed in the views ($N\times N$). 

Normal distributions:

- Prior equilibrium distribution: $N(\mu_{eq}, \tau \Sigma)$
- Views distribution: $N(Q, \Omega)$
- New combined return distribution: $N\left(E[\mu_{bl}], \left[(\tau \Sigma)^{-1} + P'\Omega^{-1}P\right]^{-1} \right)$
  
** Optimisation

- Minimise Mean variance
- Maximize Sharpe ratio
- Hierarchical Risk Parity (HRP)   

* Results
** Portfolio and Factor analysis
:PROPERTIES:
:header-args: :session py1 :tangle yes
:END:
*** Asset selection
#+begin_src python  :results none
  tickers_sp500 = ml4qf.collectors.scrap_tickers_index(config.index_weblist)
  df_tickers_sp500 = ml4qf.collectors.get_tickers_info(tickers_sp500,
                                                   config.info_sp500,
                                                   data_folder="./data",
                                                   name_family="sp500")
  df_tickers_sp500.dropna(inplace=True)
  df_tickers_filtered = ml4qf.utils.date_filter_lower(df_tickers_sp500,
                                                'first_date',
                                                date_lower=config.start_date_assets)
  df_tickers_filtered =  df_tickers_filtered.sort_values('marketCap',ascending=False)
  df_selected_tickers = ml4qf.collectors.select_assets(df_tickers_filtered,
                                                       config.ASSET_SELECTION_PCT,
                                                       config.ASSET_SELECTION_NAMES)
  # #FinancialDataContainer
  w_mkt = df_selected_tickers.marketCap / df_selected_tickers.marketCap.sum()
#+end_src

- Market cap equilibrium weights
#+begin_src python :tangle yes :results output
  portfolios_path = pathlib.Path("./data/portfolios/")
  portfolios_path.mkdir(parents=True, exist_ok=True)
  portfolios_file = portfolios_path / ("_".join(df_selected_tickers.index))
  if not portfolios_file.is_file():
      df_selected_tickers.to_csv(portfolios_file)
  print(w_mkt)
  #w_mkt = w_mkt.to_numpy()
#+end_src

#+RESULTS:
#+begin_example
GOOGL    0.824353
WFC      0.080190
EL       0.029705
F        0.025470
XYL      0.012251
FE       0.010367
VTRS     0.006752
GEN      0.006681
FRT      0.004231
Name: marketCap, dtype: float64
#+end_example

*** Assets exploratory analysis
#+begin_src python  :results none
  fdc = financial_features.FinancialDataContainer(df_selected_tickers.index,
                                                  config.start_date_assets,
                                                  config.end_date_assets,
                                                  '1mo',
                                                  './data')
  df_assets = fdc.df.dropna()
  df_assets_train, df_assets_test = ml4qf.utils.split_df_date(df_assets,
                                                   split_index=config.split_data_idx)
  asset_names = list(df_assets.columns)
#+end_src

#+NAME: df_assets
#+begin_src python :session py1 :results raw :exports results
  tabulate(df_assets.iloc[:10],
           headers=asset_names,
           showindex=True,
           tablefmt='orgtbl')
#+end_src

#+ATTR_LATEX: :width 0.7\textwidth :environment longtable :caption  
#+RESULTS: df_assets
|       |        alpha |   Mkt-RF |       SMB |       HML |        RMW |        CMA |        MOM |
|-------+--------------+----------+-----------+-----------+------------+------------+------------|
| GOOGL | -0.000952064 |  1.07236 | -0.565035 | 0.0668175 | -0.0528319 |  -0.595556 |  0.0780759 |
| WFC   |   -0.0081722 | 0.902351 | -0.191378 |   1.32462 |  -0.783329 |   0.109152 | -0.0599507 |
| EL    |    0.0039333 | 0.699059 | 0.0661193 | -0.134586 |    1.03736 |  -0.965616 |   0.127287 |
| F     |   -0.0146552 | 0.949502 |  0.289732 |  0.664054 |   0.242592 |  -0.482178 |  -0.118548 |
| XYL   |   0.00073521 | 0.996759 | -0.224459 |  0.376366 |  -0.319748 |  -0.556019 |  0.0576615 |
| FE    |  -0.00547268 |  0.10558 | -0.474695 |  0.403655 | -0.0400822 |   -1.46636 |  -0.283889 |
| VTRS  |    -0.036724 |  1.65107 |  -1.07457 | 0.0386131 |  -0.179789 | -0.0750717 |  -0.384393 |
| GEN   |   -0.0146047 |  1.71263 |  -1.88557 | -0.284629 |   -2.42119 |   0.425945 |  -0.878873 |
| FRT   |    -0.015103 | 0.472049 |    0.7014 |  0.307077 |    1.02342 |  -0.345591 |  -0.266933 |



#+NAME: basket_returns
#+begin_src python :results value file  :exports results :var name=(org-element-property :name (org-element-context))
  fig1_path= img_dir + f'{name}.png'
  fig1 = px.line(df_assets, y=df_assets.keys())
  fig1.write_image(fig1_path)
  fig1_path
#+end_src

#+CAPTION:  Asset's basket returns
#+ATTR_LATEX: :width 0.75\textwidth 
#+RESULTS: basket_returns
[[file:./img/basket_returns.png]]

#+NAME: AssetsCorrelation
#+begin_src python :results value file  :exports results :var name=(org-element-property :name (org-element-context))
  fig1_path= img_dir + f'{name}.png'
  df_corr = df_assets.corr().round(2)
  fig1 = px.imshow(np.abs(df_corr))
  fig1.layout.height = 600
  fig1.layout.width = 600
  fig1.write_image(fig1_path)
  fig1_path
#+end_src

#+CAPTION: Assets correlation
#+ATTR_LATEX: :width 0.75\textwidth 
#+RESULTS: AssetsCorrelation
[[file:./img/AssetsCorrelation.png]]

*** Factor collection
#+begin_src python :tangle yes :results none
  factor_names = financial_factors.get_factor_names(config.FACTORS)  
  df_factors0 = financial_factors.get_factors(config.FACTORS.keys(), 'm')
  df_factors =  ml4qf.utils.trim_df_date(df_factors0, start_date=config.start_date_factors,
                                         end_date=config.end_date_factors)
  df_factors_train, df_factors_test = ml4qf.utils.split_df_date(df_factors,
                                          split_index=config.split_data_idx)
#+end_src

*** Factor regression
#+begin_src python :results none
  factor_models = financial_factors.factors_regression(factor_names,
                                                      df_factors_train,
                                                      df_assets_train,
                                                      regression_kernel=regression_OLS)
  alpha, beta = financial_factors.compute_factors_coeff(factor_models)
  factor_model = financial_factors.factor_lin_generator(alpha, beta)
#+end_src

#+NAME: df_train_factors
#+begin_src python  :results raw :exports results
  albe = np.vstack([alpha, beta]).T
  df_index = asset_names
  df_columns = ['alpha'] + factor_names
  df_train_factors = pd.DataFrame(albe, columns=df_columns, index=df_index)
  tabulate(df_train_factors, headers=df_columns, showindex=True, tablefmt='orgtbl')
#+end_src

#+RESULTS: df_train_factors
|       |        alpha |   Mkt-RF |       SMB |       HML |        RMW |        CMA |        MOM |
|-------+--------------+----------+-----------+-----------+------------+------------+------------|
| GOOGL | -0.000952064 |  1.07236 | -0.565035 | 0.0668175 | -0.0528319 |  -0.595556 |  0.0780759 |
| WFC   |   -0.0081722 | 0.902351 | -0.191378 |   1.32462 |  -0.783329 |   0.109152 | -0.0599507 |
| EL    |    0.0039333 | 0.699059 | 0.0661193 | -0.134586 |    1.03736 |  -0.965616 |   0.127287 |
| F     |   -0.0146552 | 0.949502 |  0.289732 |  0.664054 |   0.242592 |  -0.482178 |  -0.118548 |
| XYL   |   0.00073521 | 0.996759 | -0.224459 |  0.376366 |  -0.319748 |  -0.556019 |  0.0576615 |
| FE    |  -0.00547268 |  0.10558 | -0.474695 |  0.403655 | -0.0400822 |   -1.46636 |  -0.283889 |
| VTRS  |    -0.036724 |  1.65107 |  -1.07457 | 0.0386131 |  -0.179789 | -0.0750717 |  -0.384393 |
| GEN   |   -0.0146047 |  1.71263 |  -1.88557 | -0.284629 |   -2.42119 |   0.425945 |  -0.878873 |
| FRT   |    -0.015103 | 0.472049 |    0.7014 |  0.307077 |    1.02342 |  -0.345591 |  -0.266933 |



#+begin_src python :results output :exports results
  print(factor_models[df_assets.keys()[1]].summary())
#+end_src

#+RESULTS:
#+begin_example
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.706
Model:                            OLS   Adj. R-squared:                  0.663
Method:                 Least Squares   F-statistic:                     16.43
Date:                Fri, 11 Aug 2023   Prob (F-statistic):           1.55e-09
Time:                        10:31:28   Log-Likelihood:                 83.115
No. Observations:                  48   AIC:                            -152.2
Df Residuals:                      41   BIC:                            -139.1
Df Model:                           6                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const         -0.0082      0.007     -1.110      0.273      -0.023       0.007
x1             0.9024      0.197      4.588      0.000       0.505       1.300
x2            -0.1914      0.346     -0.553      0.583      -0.890       0.508
x3             1.3246      0.284      4.667      0.000       0.751       1.898
x4            -0.7833      0.620     -1.263      0.214      -2.036       0.469
x5             0.1092      0.513      0.213      0.833      -0.927       1.146
x6            -0.0600      0.261     -0.230      0.819      -0.586       0.466
==============================================================================
Omnibus:                        2.682   Durbin-Watson:                   1.687
Prob(Omnibus):                  0.262   Jarque-Bera (JB):                2.381
Skew:                          -0.539   Prob(JB):                        0.304
Kurtosis:                       2.829   Cond. No.                         98.9
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
#+end_example


#+begin_src python :session py1 :results none :exports none
  import getFamaFrenchFactors as gff
  import yfinance as yf
  import statsmodels.api as sm

  ticker = 'amzn'
  start = '2016-8-31'
  end = '2021-8-31'
  start = '2014-12-31'
  end = '2019-12-31'
  stock_data = yf.download(ticker, start, end)

  ff3_monthly = gff.famaFrench3Factor(frequency='m')
  #ff3_monthly = gff.famaFrench5Factor(frequency='m')
  #momentum_monthly = gff.momentumFactor(frequency='m')

  ff3_monthly.rename(columns={"date_ff_factors": 'Date'}, inplace=True)
  ff3_monthly.set_index('Date', inplace=True)

  stock_returns = stock_data['Close'].resample('M').last().pct_change().dropna()
  stock_returns.name = "Month_Rtn"
  ff_data = ff3_monthly.merge(stock_returns,on='Date')

  X = ff_data[['Mkt-RF', 'SMB', 'HML']]
  y = ff_data['Month_Rtn'] - ff_data['RF']
  X = sm.add_constant(X)
  ff_model = sm.OLS(y, X).fit()
#+end_src
#+begin_src python :session py1 :results output :exports results
  print(ff_model.summary())
#+end_src

#+RESULTS:
#+begin_example
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.514
Model:                            OLS   Adj. R-squared:                  0.488
Method:                 Least Squares   F-statistic:                     19.73
Date:                Thu, 10 Aug 2023   Prob (F-statistic):           7.48e-09
Time:                        15:40:50   Log-Likelihood:                 86.579
No. Observations:                  60   AIC:                            -165.2
Df Residuals:                      56   BIC:                            -156.8
Df Model:                           3                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          0.0147      0.008      1.843      0.071      -0.001       0.031
Mkt-RF         1.5402      0.224      6.886      0.000       1.092       1.988
SMB           -0.6471      0.338     -1.914      0.061      -1.324       0.030
HML           -0.9529      0.297     -3.205      0.002      -1.549      -0.357
==============================================================================
Omnibus:                        2.723   Durbin-Watson:                   1.576
Prob(Omnibus):                  0.256   Jarque-Bera (JB):                2.361
Skew:                           0.485   Prob(JB):                        0.307
Kurtosis:                       2.949   Cond. No.                         46.3
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
#+end_example


#+begin_src python :tangle yes :results none
  returns_pred = factor_model(df_factors_test[factor_names].to_numpy())
  df_returns_pred = pd.DataFrame(returns_pred,
                                 columns=asset_names,
                                 index=df_assets_test.index)
#+end_src

*** Factors backtesting
#+NAME: predicted_returns
#+begin_src python :var i_asset=0 name=(org-element-property :name (org-element-context))
  i_asset = i_asset
  i_name = asset_names[i_asset]
  fig1_path= img_dir / f'{name}{i_name}.png'
  fig1 = go.Figure()
  fig1.add_trace(go.Scatter(
      x=df_assets_test.index,
      y=df_assets_test.iloc[:, i_asset] - df_factors_test.RF.to_numpy(),
      mode='lines+markers',
      name=f"{i_name} real"))
  fig1.add_trace(go.Scatter(
      x=df_assets_test.index,
      y=df_returns_pred[i_name],
      mode='lines',
      name=f"{i_name} pred."))

  #px.line(df_returns_pred['GOOGL'], y=df_returns_pred.keys()[0])

  fig1.write_image(fig1_path)
  str(fig1_path)
#+end_src

#+NAME: predicted_returns0
#+begin_src python :noweb eval :results value file  :exports results
  fig_path = "<<predicted_returns(i_asset=0, name="predicted_returns_")>>"
  fig_path
#+end_src
#+CAPTION:  Backtesting factor approximation on Google asset
#+ATTR_LATEX: :width 0.75\textwidth 
#+RESULTS: predicted_returns0
[[file:img/predicted_returns_GOOGL.png]]

#+NAME: predicted_returns1
#+begin_src python :noweb eval :results value file  :exports results
  fig_path = "<<predicted_returns(i_asset=1, name="predicted_returns_")>>"
  fig_path
#+end_src
#+CAPTION:  Backtesting factor approaximation on Google asset
#+ATTR_LATEX: :width 0.75\textwidth 
#+RESULTS: predicted_returns1
[[file:img/predicted_returns_WFC.png]]

#+NAME: predicted_returns2
#+begin_src python :noweb eval :results value file  :exports results
  fig_path = "<<predicted_returns(i_asset=2, name="predicted_returns_")>>"
  fig_path
#+end_src
#+CAPTION:  Backtesting factor approaximation on Google asset
#+ATTR_LATEX: :width 0.75\textwidth 
#+RESULTS: predicted_returns2
[[file:img/predicted_returns_EL.png]]

#+NAME: predicted_returns3
#+begin_src python :noweb eval :results value file  :exports results
  fig_path = "<<predicted_returns(i_asset=3, name="predicted_returns_")>>"
  fig_path
#+end_src
#+CAPTION:  Backtesting factor approaximation on Google asset
#+ATTR_LATEX: :width 0.75\textwidth 
#+RESULTS: predicted_returns3
[[file:img/predicted_returns_F.png]]

** Generation of asset views
:PROPERTIES:
:header-args: :session py1 :tangle yes
:END:
*** ARIMA model construction
#+NAME: arima_autocorrelation
#+begin_src python :results value file  :exports results :var name=(org-element-property :name (org-element-context))
  fig1_path= img_dir + f'{name}.png'
  fig = plt.figure()
  ax = autocorrelation_plot()
  #ax.set_title("bleh")
  #ax.set_xlabel("xlabel")
  #ax.plot(x, y, 'r--')
  fig.savefig(fig1_path)
  fig1_path
#+end_src

#+begin_src python :results none
  X = df_factors_train[factor_names].to_numpy()
  arima_train_models = model_stats.arima_fit(X, factor_names, (2,1,0))
#+end_src

#+begin_src python :results none
  arima_train_models['Mkt-RF'].summary()
#+end_src

*** ARIMA model prediction
#+begin_src python :results none
  X = df_factors_train[factor_names].to_numpy()
  arima_train_models = model_stats.arima_fit(X, factor_names, (2,1,0))
#+end_src


** Black-Litterman based portfolio
*** Prior and posterior returns construction
*** Views on
*** Covariance treatment
*** Portfolio weights optimisation
*** Analysis and discussion
*** Performance comparison
sss


#+LaTeX: \appendix
* Code execution
The codes herein have been tested in linux (Ubuntu 22 and Centos 8) and in MacOs


** Testing
