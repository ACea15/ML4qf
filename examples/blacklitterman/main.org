#+TITLE: Portfolio Construction using Black-Litterman Model and Factors
#+AUTHOR: Alvaro Cea
#+PROPERTY: header-args :tangle ./main.py :mkdirp yes
#+LATEX_HEADER: \usepackage[margin=1in]{geometry}
#+LATEX_HEADER: \usepackage{mathtools}
#+OPTIONS: broken-links:tc
#+begin_comment
#+OPTIONS: toc:nil
#+LATEX_HEADER: \let\oldsection\section
#+LATEX_HEADER: \renewcommand{\section}{\clearpage\oldsection}
#+LATEX_HEADER: \let\oldsubsection\subsection
#+LATEX_HEADER: \renewcommand{\subsection}{\clearpage\oldsubsection}
#+end_comment

* House keeping :noexport:
#+begin_src elisp :results none :exports none
  (add-to-list 'org-structure-template-alist
  '("sp" . "src python :session py1"))
  (add-to-list 'org-structure-template-alist
  '("se" . "src elisp"))

  (setq org-confirm-babel-evaluate nil)
  (pyvenv-workon "ml4qf")
  (require 'org-tempo)
  (setq org-format-latex-options (plist-put org-format-latex-options :scale 2.0))
  (setq org-latex-pdf-process (list "latexmk -f -synctex=1 -pdf %f"))
  ;; (setq org-latex-pdf-process (list "latexmk -f -pdf -interaction=nonstopmode -output-directory=%o %f"))

#+end_src

#+begin_src python :session py1 :tangle yes :results none :exports none
  import pandas as pd
  import numpy as np
  import yfinance as yf
  import statsmodels.api as sm
  import getFamaFrenchFactors as gff
  import pathlib
  import datetime
  import importlib
  import ml4qf
  import ml4qf.collectors.financial_features as financial_features
  import ml4qf.collectors.financial_factors as financial_factors
  import ml4qf.collectors.financial_markets as financial_markets
  from ml4qf.predictors.model_stats import regression_OLS
  import ml4qf.predictors.model_stats as model_stats
  import ml4qf.portfolios.blacklitterman as bl
  import ml4qf.portfolios.optimization as optimization  
  from tabulate import tabulate
  import plotly.express as px
  import plotly.graph_objects as go
  import matplotlib.pyplot as plt
  import collections
  from pandas.plotting import autocorrelation_plot
  import config
  importlib.reload(config)
  img_dir = pathlib.Path("./img/")
  #img_dir = img_dir0.absolute()
  img_dir.mkdir(parents=True, exist_ok=True)
  import warnings
  warnings.filterwarnings("ignore")
#+end_src

* Introduction
** Specifications:
For risk-free rate, 3M US Treasury from pandas FRED dataset/ECB website
rates for EUR/some small constant rate/zero rate – all are acceptable.
Use 2-3 year sample, which means > 500 daily returns.
** Index selection
The S&P 500 is considered a better reflection of the market’s performance across all sectors compared to the Nasdaq Composite and the Dow

#+begin_comment
#+CAPTION: Modal shape 1 
#+ATTR_LATEX: :width 0.75\textwidth
#+ATTR_ORG: :width 100
[[./img/polimi-M0.png]]
#+end_comment

** Implemented computational tools and external libraries
The task 
Outside the general purpose Python ecosystem (numpy, pandas, pytest, etc.), the following libraries have been employed  to complete this work:
- *getFamaFrenchFactors*: Collects data for fame french factors from the Kenneth French library
  https://pypi.org/project/getFamaFrenchFactors/
- *statsmodels*: statistical software used to deploy an ARIMA process on the asset returns and also to fit the factors to data using an Ordinary Least Squares (OLS) approach. 
  https://www.statsmodels.org/stable/index.html
- *yahoofinance*: used 
  https://python-yahoofinance.readthedocs.io/en/latest/
* Theory
** Black-Litterman model


$$
\mu_{eq} = \lambda \Sigma w_{mkt}
$$

- $\mu_{eq}$ is the Implied Excess Equilibrium Return Vector 
- $\lambda$ is the risk aversion coefficient, $\lambda = \frac{E(\mu) - \mu_r}{\sigma^2}$
- $\Sigma$ is the covariance matrix of excess returns
- $w_{mkt}$ are market capitalization weights

$$
E[\mu_{bl}] = \left[(\tau \Sigma)^{-1} + P'\Omega^{-1}P\right]^{-1} \left[(\tau \Sigma)^{-1}\mu_{eq} + P'\Omega^{-1}Q\right]  
$$

- $E[\mu_{bl}]$: Posterior combined return vector ($N\times a$)
- $\tau$: error in views
- $P$: Matrix identifying assets involved in the views ($K\times N$)
- $Q$: View vector ($K\times 1$)
- $\Omega$: Diagonal covariance matrix with the errors expressed in the views ($N\times N$). 

Normal distributions:

- Prior equilibrium distribution: $N(\mu_{eq}, \tau \Sigma)$
- Views distribution: $N(Q, \Omega)$
- New combined return distribution: $N\left(E[\mu_{bl}], \left[(\tau \Sigma)^{-1} + P'\Omega^{-1}P\right]^{-1} \right)$
  
** Fama-French factors

It is one of the multi-factor models which is widely used in both academia and industry to estimate the excess return of an investment asset. It is an extension to Capital Asset Pricing Model (CAPM) by adding two additional factors apart from the market risk when estimating the excess returns of an asset. The three factors considered in this model are:

    - Market factor (MKT): Excess market return
    - Size factor (SMB): Excess return with a small market cap over those with a large market cap
    - Value factor (HML): Excess return of value stocks over growth stocks.
    - Profitability factor (RMW): Excess returns of stocks with strong and weak profitability
    - Investment factor (CMA): Excess returns between high and low investment firms.
      
The Fama-French model is widely known as a stock market benchmark to evaluate investment performance.

$$
E[r_a] = \mu_a + \beta E[r_f]  + \epsilon_a
$$

$$
Var[r_a] = \mu_a + \beta r_f  + \epsilon_a
$$

$$
\Pi = w_a^{\top} r_a
$$

$$
Var(\Pi) = Var(r_a^{\top} w_a) = Var(r_a^{\top} w_a)
$$

** ARIMA model for time series
AutoRegressive Integrated Moving Average (ARIMA) statistical models are used 
AR: Autoregression. A model that uses the dependent relationship between an observation and some number of lagged observations.
I: Integrated. The use of differencing of raw observations (e.g. subtracting an observation from an observation at the previous time step) in order to make the time series stationary.
MA: Moving Average. A model that uses the dependency between an observation and a residual error from a moving average model applied to lagged observations.

Each of these components are explicitly specified in the model as a parameter. A standard notation is used of ARIMA(p,d,q) where the parameters are substituted with integer values to quickly indicate the specific ARIMA model being used.

The parameters of the ARIMA model are defined as follows:

- p: Number of lags in the observations that included in the model.
- d: Number of times differencing is applied to the observations.
- q: Size of moving average window.

** Optimisation

- Minimise Mean variance
- Maximize Sharpe ratio
- Hierarchical Risk Parity (HRP)   

* Results
The analysis is divided in three major sections: a portfolio selection of 10 assets from the S&P500 and the factor analysis of those assets; a statistical analysis using an ARIMA process in order to project the assets onto the future and generate the views input to the portfolio optimisation; and finally the Black-Litterman portfolio optimization with backtesting.  
** Portfolio and Factor analysis
:PROPERTIES:
:header-args: :session py1 :tangle yes :exports none
:END:
There are many approaches for picking a basket of assets and herein the adoption is a simple yet general and automatic strategy that guarantees diversification. The companies gathered are first presented together with their performance and correlations, then the factor analysis on these companies is shown.  
*** Asset selection
The selection of assets have followed a random and automatic generation of ten tickers from the S&P 500 with these constraints: no two assets could belong to the same sector; 1 company is chosen among the top 5% in terms of market cap, 2 among the next 20%, 4 among the next 50%, 2 in the following 20% and the final one picked among the 5% smallest; the correlation among assets should also be kept small. 
#+NAME: Load index SP500
#+begin_src python :results none
  # Load index SP500
  sp500 = financial_features.FinancialData("^GSPC",
                                           config.start_date_assets,
                                           config.end_date_assets,
                                           DATA_FOLDER="./data")
  df_sp500 = sp500.df[['returns']].dropna()
#+end_src

#+NAME: Load portfolio and calculate market weights
#+begin_src python :results none
  # Load portfolio and calculate market weights
  tickers_sp500 = ml4qf.collectors.scrap_tickers_index(config.index_weblist)
  df_tickers_sp500 = ml4qf.collectors.get_tickers_info(tickers_sp500,
                                                       config.info_sp500,
                                                       data_folder="./data",
                                                       name_family="sp500")
  df_tickers_sp500.dropna(inplace=True)
  df_tickers_filtered = ml4qf.utils.date_filter_lower(df_tickers_sp500,
                                                      'first_date',
                                                      date_lower=config.start_date_assets)
  df_tickers_filtered =  df_tickers_filtered.sort_values('marketCap',ascending=False)
  df_selected_tickers = ml4qf.collectors.select_assets(df_tickers_filtered,
                                                       config.ASSET_SELECTION_PCT,
                                                       config.ASSET_SELECTION_NAMES)
  # Market cap equilibrium weights
  w_mkt = df_selected_tickers.marketCap / df_selected_tickers.marketCap.sum()
  num_assets = len(df_selected_tickers)
  portfolios_path = pathlib.Path("./data/portfolios/")
  portfolios_path.mkdir(parents=True, exist_ok=True)
  portfolios_file = portfolios_path / ("_".join(df_selected_tickers.index))
  if not portfolios_file.is_file():
      df_selected_tickers.to_csv(portfolios_file)
  w_mkt = w_mkt.to_numpy()

  # Load assets returns
  fdc = financial_features.FinancialDataContainer(df_selected_tickers.index,
                                                  config.start_date_assets,
                                                  config.end_date_assets,
                                                  '1mo',
                                                  './data')
  df_assets = fdc.df.dropna()
  df_assets_train, df_assets_test = ml4qf.utils.split_df_date(
      df_assets,
      split_index=config.split_data_idx)
  asset_names = list(df_assets.columns)

#+end_src
The resulting basket is shown in Table  [[df_portfolio_summary]].
#+NAME: Compute and show Data Frame, df_portfolio_summary
#+begin_src python :results raw :exports results :tangle no
  # Compute Data Frame df_portfolio_summary
  df_portfolio_summary = df_selected_tickers.copy()
  #df_portfolio_summary = df_portfolio_summary.drop('first_date', axis=1)
  df_portfolio_summary['marketWeights'] = w_mkt
  df_portfolio_summary = df_portfolio_summary[['marketCap',
                                               'marketWeights',
                                               'sector']]
  tabulate(df_portfolio_summary,
           headers=df_portfolio_summary.columns,
           showindex=True,
           tablefmt='orgtbl')
#+end_src
#+NAME: df_portfolio_summary
#+CAPTION: Portfolio selected assets
#+ATTR_LATEX: :width 0.7\textwidth :environment longtable :caption  
#+RESULTS: Compute and show Data Frame, df_portfolio_summary
|      |   marketCap | marketWeights | sector                 |
|------+-------------+---------------+------------------------|
| JPM  | 4.48847e+11 |      0.546415 | Financial Services     |
| CVS  | 9.59856e+10 |       0.11685 | Healthcare             |
| ATVI | 7.17056e+10 |     0.0872924 | Communication Services |
| PH   | 5.34366e+10 |     0.0650523 | Industrials            |
| WELL | 4.18798e+10 |     0.0509834 | Real Estate            |
| YUM  | 3.75877e+10 |     0.0457582 | Consumer Cyclical      |
| KR   | 3.51552e+10 |      0.042797 | Consumer Defensive     |
| ATO  | 1.69714e+10 |     0.0206606 | Utilities              |
| EQT  | 1.55687e+10 |      0.018953 | Energy                 |
| DXC  | 4.30288e+09 |    0.00523821 | Technology             |

*** Assets exploratory analysis
Monthly returns are used for the analysis as a better metric for a portfolio that is not going to be rebalanced for long periods of time. A period of over 20 years is taken for both the analysis and the backtesting as to make sure a reasonable amount of data is utilised in the study. Table [[df_assets]] 
#+NAME: df_assets
#+begin_src python :session py1 :results raw :exports results :tangle no
  df_assets2show = pd.concat([df_assets.iloc[:5],df_assets.iloc[-5:]])
  df_assets2show.index = df_assets2show.index.date
  tabulate(df_assets2show,
           headers=asset_names,
           showindex=True,
           tablefmt='orgtbl')
#+end_src
#+NAME: df_assets
#+ATTR_LATEX: :width 0.7\textwidth :environment longtable :caption  
#+RESULTS: df_assets
|            |        JPM |         CVS |       ATVI |         PH |       WELL |        YUM |         KR |        ATO |        EQT |        DXC |
|------------+------------+-------------+------------+------------+------------+------------+------------+------------+------------+------------|
| 2000-03-01 |  0.0949765 |   0.0732143 | -0.0492617 |   0.139655 |  -0.100402 |   0.166667 |   0.175732 | -0.0437956 |   0.187086 | 0.00396511 |
| 2000-04-01 |   -0.17276 |     0.15807 |  -0.481865 |   0.125567 |   0.138393 |  0.0905433 |  0.0569395 | -0.0305344 |  0.0348675 |  0.0308057 |
| 2000-05-01 |  0.0355287 |           0 | -0.0099994 |  -0.103495 | 0.00784314 |  -0.134686 |  0.0707071 |   0.153543 |  0.0727763 |   0.174713 |
| 2000-06-01 | -0.0748954 |  -0.0804598 |  0.0505057 |  -0.178411 |  0.0126459 | -0.0362472 |   0.110063 | -0.0435154 | -0.0298367 |  -0.220483 |
| 2000-07-01 |  0.0814111 |     -0.0125 |   0.346154 |  0.0383212 |   0.106628 |  -0.141593 | -0.0623229 |    0.17752 |  0.0786662 |  -0.171548 |
| 2022-07-01 |  0.0244206 |   0.0325922 |   0.026843 |   0.174924 |  0.0484517 |  0.0795525 | -0.0188042 |  0.0828724 |   0.279942 |  0.0425602 |
| 2022-08-01 | -0.0141297 |   0.0258152 | -0.0182614 | -0.0833304 |  -0.112231 | -0.0922148 |  0.0322997 | -0.0659857 |  0.0856235 |  -0.215823 |
| 2022-09-01 | -0.0811572 |   -0.028324 |  -0.052873 | -0.0856227 |  -0.160861 | -0.0440489 | -0.0874009 |  -0.101693 |   -0.14749 | -0.0121066 |
| 2022-10-01 |   0.204593 | -0.00702533 | -0.0207155 |   0.199373 |  -0.050995 |   0.111999 |  0.0809143 |  0.0461463 |  0.0267485 |   0.174428 |
| 2022-11-01 |  0.0977121 |   0.0758184 |  0.0157966 |  0.0286285 |   0.163663 |  0.0880339 |  0.0401776 |   0.128109 |  0.0136233 |      0.032 |
#
Fig. [[basket_returns]] shows the returns evolution of the assets over the period of analysis.
#+NAME: basket_returns
#+begin_src python :results value file  :exports results :var name=(org-element-property  :name (org-element-context))
  # Plot basket_returns
  import plotly.graph_objs as go
  layout = go.Layout(
  margin=go.layout.Margin(
        l=0, #left margin
        r=0, #right margin
        b=0, #bottom margin
        t=0  #top margin
    )
  )
  fig1_path= img_dir / f'{name}.png'
  fig1 = px.line(df_assets, y=df_assets.keys())
  fig1.update_layout(margin_b=3, margin_t=5)
  fig1.write_image(fig1_path)
  fig1_path #
#+end_src
#+NAME: basket_returns
#+CAPTION:  Asset's basket returns
#+ATTR_LATEX: :width 0.75\textwidth 
#+RESULTS: basket_returns
[[file:img/basket_returns.png]]

The correlation between the returns is a good indication of how well diversified our portfolio is and it can be seen that a low correlation is reflected among most of the assets. It is important to keep in mind this is not the ultimate proof of diversification since it does not capture nonlinear relations between the pairs.
#+NAME: AssetsCorrelation
#+begin_src python :results value file  :exports results :var name=(org-element-property :name (org-element-context))
  # Plot AssetsCorrelation
  fig1_path= img_dir / f'{name}.png'
  df_corr = df_assets.corr().round(2)
  fig1 = px.imshow(np.abs(df_corr))
  #fig1.layout.height = 600
  #fig1.layout.width = 600
  fig1.update_layout(margin_l=0,margin_b=3, margin_t=5)
  fig1.write_image(fig1_path)
  fig1_path #
#+end_src
#+CAPTION: Assets correlation
#+ATTR_LATEX: :width 0.75\textwidth 
#+RESULTS: AssetsCorrelation
[[file:img/AssetsCorrelation.png]]

*** Factor collection
The 5 Fama-French factors are retrieved for the dates of interest together with the momentum factor and the risk-free interest rate. As with the returns, monthly  They are plotted in Fig. [[Factors_evolution]] and [[RFrate_evolution]] respectively.
#+NAME: Load Fama and French 5 factors and Momentum factor  
#+begin_src python  :results none
  # Load Fama and French 5 factors and Momentum factor
  factor_names = financial_factors.get_factor_names(config.FACTORS)  
  df_factors0 = financial_factors.get_factors(config.FACTORS.keys(), 'm')
  df_factors =  ml4qf.utils.trim_df_date(df_factors0, start_date=config.start_date_factors,
                                         end_date=config.end_date_factors)
  df_factors_train, df_factors_test = ml4qf.utils.split_df_date(df_factors,
                                          split_index=config.split_data_idx)
#+end_src

#+NAME: Factors_evolution 
#+begin_src python :results value file  :exports results :var name=(org-element-property :name (org-element-context))
  # Plot monthly Factors evolution 
  fig1_path= img_dir / f'{name}.png'
  fig1 = px.line(df_factors, y=factor_names)
  fig1.write_image(fig1_path)
  fig1_path # 
#+end_src
#+NAME: Factors_evolution 
#+CAPTION: Factors evolution
#+ATTR_LATEX: :width 0.75\textwidth 
#+RESULTS: Factors_evolution
[[file:img/Factors_evolution.png]]

#+NAME: RFrate_evolution
#+begin_src python :results value file  :exports results :var name=(org-element-property  :name (org-element-context)) 
  fig1_path= img_dir / f'{name}.png'
  fig1 = px.line(df_factors*12, y='RF')
  fig1.write_image(fig1_path)
  fig1_path #
#+end_src
#+NAME: RFrate_evolution
#+CAPTION: (Annualised) risk-free rate evolution
#+ATTR_LATEX: :width 0.75\textwidth 
#+RESULTS: RFrate_evolution
[[file:img/RFrate_evolution.png]]

*** Factor regression
With the asset returns, the factors and the risk-free rate, the $\alpha$ vector and the $\beta$ matrix are calculated using an OLS regression. The results of this regression are shown in Table [[df_train_factors]].
#+NAME: Compute regression on assets returns vs factors
#+begin_src python :results none
  # Compute regression on assets returns vs factors
  factor_models = financial_factors.factors_regression(factor_names,
                                                       df_factors_train,
                                                       df_assets_train,
                                                       regression_kernel=regression_OLS)
  alpha, beta = financial_factors.compute_factors_coeff(factor_models)
  factor_model = financial_factors.factor_lin_generator(alpha, beta)
#+end_src

#+NAME: Data Frame df_train_factors with alphas and betas
#+begin_src python  :results raw :exports results :tangle no
  albe = np.vstack([alpha, beta]).T
  df_index = asset_names
  df_columns = ['alpha'] + factor_names
  df_train_factors = pd.DataFrame(albe, columns=df_columns, index=df_index)
  tabulate(df_train_factors, headers=df_columns, showindex=True, tablefmt='orgtbl')
#+end_src
#+NAME: df_train_factors
#+CAPTION: Factor analysis alphas and betas.  
#+ATTR_LATEX: :width 0.7\textwidth :environment longtable :caption
#+RESULTS: Data Frame df_train_factors with alphas and betas
|      |       alpha |   Mkt-RF |        SMB |        HML |       RMW |       CMA |        MOM |
|------+-------------+----------+------------+------------+-----------+-----------+------------|
| JPM  |  0.00563626 | 0.908254 |   -0.22666 |    1.22866 |  -1.07388 | -0.528425 |  -0.292278 |
| CVS  | -0.00150586 | 0.934148 |  -0.207898 |  0.0581141 |  0.350351 |    1.0657 |  0.0150695 |
| ATVI |   0.0221258 | 0.954258 |   0.232224 |  -0.173592 | -0.455182 |  0.139466 |   0.416191 |
| PH   | -0.00419705 |  1.53734 |   0.415025 |  -0.222542 |   1.18971 |  0.649222 |  -0.232595 |
| WELL |  0.00133871 | 0.520551 |   0.317423 | -0.0261333 |  0.202619 |  0.395438 | -0.0621194 |
| YUM  |  0.00401247 | 0.886379 |   0.492413 |  -0.303535 |   1.09974 | 0.0829882 |  -0.169236 |
| KR   | -0.00154716 | 0.913941 |  -0.353481 |  -0.217052 |  0.468732 |   1.03135 |   0.200383 |
| ATO  |  0.00116209 | 0.437769 |   0.297623 |  -0.190557 |  0.507524 |  0.509398 | -0.0215291 |
| EQT  |    0.002473 | 0.834018 | -0.0800847 |  0.0711937 |  0.416891 |  0.257262 |  0.0740147 |
| DXC  |  0.00428523 |  1.18813 |  -0.441472 |   -0.25501 | -0.122975 |  0.301193 |  -0.212394 |

In Appendix [[sec:appx_arima]] a summary of the OLS calculation to approximate the factors is presented
#+NAME: Summary of factors OLS
#+begin_src python :results output :exports results :tangle no
  print(factor_models[asset_names[6]].summary())
#+end_src

#+NAME: Compute factor model prediction
#+begin_src python :results none
  # Compute factor model prediction
  # prediction on test data
  returns_pred = factor_model(df_factors_test[factor_names].to_numpy())
  df_returns_pred = pd.DataFrame(returns_pred,
                                 columns=asset_names,
                                 index=df_assets_test.index)
  # prediction on training data
  returns_predt = factor_model(df_factors_train[factor_names].to_numpy())
  df_returns_predt = pd.DataFrame(returns_predt,
                                 columns=asset_names,
                                 index=df_assets_train.index)

#+end_src

*** Factors backtesting
#+begin_comment
#+NAME: predicted_returns
#+begin_src python :var i_asset=0 name=(org-element-property :name (org-element-context))
  i_asset = i_asset
  i_name = asset_names[i_asset]
  fig1_path= img_dir / f'{name}{i_name}.png'
  fig1 = go.Figure()
  fig1.add_trace(go.Scatter(
      x=df_assets_test.index,
      y=df_assets_test.iloc[:, i_asset] - df_factors_test.RF.to_numpy(),
      mode='lines+markers',
      name=f"{i_name} real"))
  fig1.add_trace(go.Scatter(
      x=df_assets_test.index,
      y=df_returns_pred[i_name],
      mode='lines',
      name=f"{i_name} pred."))

  #px.line(df_returns_pred['GOOGL'], y=df_returns_pred.keys()[0])

  fig1.write_image(fig1_path)
  str(fig1_path)
#+end_src

#+NAME: predicted_returns0
#+begin_src python :noweb eval :results value file  :exports results 
  fig_path = "<<predicted_returns(i_asset=0, name="predicted_returns_")>>"
  fig_path
#+end_src
#+CAPTION:  Backtesting factor approximation on Google asset
#+ATTR_LATEX: :width 0.75\textwidth 
#+RESULTS: predicted_returns0
#+end_comment

#+NAME: Function to plot returns from factor model
#+begin_src python :results none 
  def plot_rets_fromfactors(df_assets,
                            df_factors,
                            df_returns_pred,
                            i_asset,
                            name):

      i_name = asset_names[i_asset]
      fig1_path= img_dir / f'{name}{i_name}.png'
      fig1 = go.Figure()
      fig1.add_trace(go.Scatter(
          x=df_assets.index,
          y=df_assets.iloc[:, i_asset] - df_factors.RF.to_numpy(),
          mode='lines+markers',
          name=f"{i_name} real"))
      fig1.add_trace(go.Scatter(
          x=df_assets.index,
          y=df_returns_pred[i_name],
          mode='lines',
          name=f"{i_name} pred."))

      fig1.write_image(fig1_path)
      return str(fig1_path)

#+end_src

#+NAME: predicted_factorreturns_test
#+begin_src python :noweb eval :results value file  :exports results :var name=(org-element-property :name (org-element-context)) 
  fig1_path = plot_rets_fromfactors(df_assets_test,
                                   df_factors_test,
                                   df_returns_pred,
                                   i_asset=3, name=name)
  fig1_path #
#+end_src
#+CAPTION:  Backtesting factor approaximation on Google asset
#+ATTR_LATEX: :width 0.75\textwidth 
#+RESULTS: predicted_factorreturns_test
[[file:img/predicted_factorreturns_testPH.png]]

#+NAME: predicted_factorreturns_train
#+begin_src python :noweb eval :results value file  :exports results :var name=(org-element-property :name (org-element-context))
  fig1_path = plot_rets_fromfactors(df_assets_train,
                                    df_factors_train,
                                    df_returns_predt,
                                    i_asset=3, name=name)
  fig1_path #
#+end_src
#+CAPTION:  Backtesting factor approaximation on Google asset
#+ATTR_LATEX: :width 0.75\textwidth 
#+RESULTS: predicted_factorreturns_train
[[file:img/predicted_factorreturns_trainPH.png]]

** ARIMA model for the the generation of asset views
:PROPERTIES:
:header-args: :session py1 :tangle yes :exports none
:END:
Firstly an ARIMA process is constructed with a hyperparameter search for the p, d and q parameters. 
*** ARIMA model construction
#+BEGIN_COMMENT
#+NAME: arima_autocorrelation
#+begin_src python :results value file  :exports results :var name=(org-element-property :name  (org-element-context)) 
  fig1_path= img_dir / f'{name}.png'
  fig = plt.figure()
  ax = autocorrelation_plot(df_train_factors['SMB'])
  #ax.set_title("bleh")
  #ax.set_xlabel("xlabel")
  #ax.plot(x, y, 'r--')
  fig.savefig(fig1_path)
  fig1_path
#+end_src
#+RESULTS: arima_autocorrelation
[[file:img/arima_autocorrelation.png]]
#+END_COMMENT

#+begin_src python :results none
  def pick_arimahyper(errs):
      arima_parameters = dict()
      derrors = collections.defaultdict(list)
      derrorsind = collections.defaultdict(list)
      minvalue = collections.defaultdict(list)
      index = collections.defaultdict(list)  
      for k, v in errs.items():
          conv = k.split('_')
          derrors[conv[0]].append(v)
          derrorsind[conv[0]].append(tuple(int(i) for i in conv[1:]))
      for k, v in derrors.items():
          index[k] = v.index(min(v))
          minvalue[k] = min(v)
          arima_parameters[k] = derrorsind[k][index[k]]
      return arima_parameters, derrors, derrorsind, minvalue
#+end_src

#+begin_src python :results none
  if config.compute_arima_parameters:
      errs_train, errs_test = model_stats.arima_hyperparameters(
          df_factors_train, # 
          df_factors_test,
          factor_names,
          [0, 2, 4, 6, 8, 11, 15, 19, 23, 29, 35],
          [0, 1, 2, 3, 4, 5],
          [0, 1, 3, 4, 5, 6, 15, 19, 23],
          model_stats.err_mse,
          dict(enforce_stationarity=False,
               enforce_invertibility=False)
      )
      arima_parameters, derrors, derrorsind, minvalue = pick_arimahyper(errs_test)
  else:
      arima_parameters = config.arima_parameters
  df_arima_parameters = pd.DataFrame(arima_parameters, index=['p', 'd', 'q'])
#+end_src
Table [[df_arima_parameters]] shows the parameters used in the ARIMA model after a hyperparameter search to minimise the error in the approximation. 
#+NAME: df_arima_parameters
#+begin_src python  :results raw :exports results :tangle no
  tabulate(df_arima_parameters, headers=df_arima_parameters, showindex=True, tablefmt='orgtbl')
#+end_src
#+NAME: df_arima_parameters
#+CAPTION: Hyperparameters in ARIMA process for each factor
#+ATTR_LATEX: :width 0.7\textwidth :environment longtable :caption
#+RESULTS: df_arima_parameters
|   | Mkt-RF | SMB | HML | RMW | CMA | MOM |
|---+--------+-----+-----+-----+-----+-----|
| p |      7 |  21 |  13 |  15 |  15 |  21 |
| d |      1 |   1 |   2 |   0 |   4 |   2 |
| q |     26 |   9 |   1 |   6 |   6 |  23 |

*** COMMENT ARIMA factors prediction

#+begin_src python :results none 
  Xtrain = df_factors_train[factor_names].to_numpy()
  Xtest = df_factors_test[factor_names].to_numpy()
  index_train = df_factors_train.index
  index_test = df_factors_test.index
  # arima_parameters = {'Mkt-RF': (15,0,15),
  #                     'SMB': (15,0,9),
  #                     'HML': (6,0,3),
  #                     'RMW': (15,0,6),
  #                     'CMA': (6,4,12),
  #                     'MOM': (0,1,0)
  #                     }
  #arima_parameters = config.arima_parameters
  model_sett = dict(enforce_stationarity=False,
                    enforce_invertibility=False) 
  arima_train_models = model_stats.arima_fit(Xtrain,
                                             factor_names,
                                             arima_parameters,
                                             model_sett=model_sett)
  df_arimatrain, df_arimatest = model_stats.arima_build_pred(arima_train_models,
                                                             Xtrain,
                                                             Xtest,
                                                             factor_names,
                                                             index_train,
                                                             index_test)  
#+end_src

#+begin_src python :results output  
  print(arima_train_models['Mkt-RF'].summary())
#+end_src

#+RESULTS:
#+begin_example
                               SARIMAX Results                                
==============================================================================
Dep. Variable:                      y   No. Observations:                  219
Model:                ARIMA(7, 1, 26)   Log Likelihood                 353.712
Date:                Fri, 18 Aug 2023   AIC                           -639.424
Time:                        19:26:04   BIC                           -528.847
Sample:                             0   HQIC                          -594.635
                                - 219                                         
Covariance Type:                  opg                                         
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
ar.L1         -0.3710      0.869     -0.427      0.669      -2.074       1.332
ar.L2         -0.5165      0.567     -0.912      0.362      -1.627       0.594
ar.L3         -0.4541      0.535     -0.849      0.396      -1.503       0.594
ar.L4         -0.3141      0.544     -0.577      0.564      -1.381       0.753
ar.L5         -0.1476      0.411     -0.359      0.719      -0.953       0.657
ar.L6         -0.1679      0.350     -0.480      0.631      -0.854       0.518
ar.L7          0.1141      0.276      0.413      0.679      -0.427       0.655
ma.L1         -0.4003      0.871     -0.459      0.646      -2.108       1.307
ma.L2         -0.0158      0.734     -0.022      0.983      -1.455       1.423
ma.L3          0.1300      0.654      0.199      0.842      -1.151       1.411
ma.L4         -0.1957      0.484     -0.404      0.686      -1.145       0.754
ma.L5         -0.1174      0.435     -0.270      0.787      -0.971       0.736
ma.L6         -0.1957      0.343     -0.570      0.568      -0.868       0.477
ma.L7         -0.2198      0.250     -0.881      0.379      -0.709       0.269
ma.L8          0.1011      0.311      0.325      0.745      -0.508       0.710
ma.L9         -0.0666      0.231     -0.288      0.773      -0.519       0.386
ma.L10        -0.1963      0.259     -0.758      0.448      -0.704       0.311
ma.L11         0.1612      0.262      0.614      0.539      -0.353       0.675
ma.L12        -0.1189      0.227     -0.523      0.601      -0.564       0.326
ma.L13         0.0582      0.225      0.259      0.796      -0.382       0.499
ma.L14        -0.0209      0.196     -0.106      0.915      -0.406       0.364
ma.L15         0.0430      0.145      0.297      0.767      -0.241       0.327
ma.L16         0.0485      0.177      0.275      0.783      -0.298       0.395
ma.L17        -0.0518      0.158     -0.327      0.743      -0.362       0.258
ma.L18        -0.0429      0.172     -0.249      0.803      -0.380       0.295
ma.L19         0.2715      0.161      1.689      0.091      -0.044       0.587
ma.L20        -0.2190      0.273     -0.801      0.423      -0.755       0.317
ma.L21         0.1878      0.283      0.665      0.506      -0.366       0.742
ma.L22        -0.0844      0.297     -0.284      0.776      -0.667       0.498
ma.L23         0.0535      0.238      0.225      0.822      -0.413       0.520
ma.L24         0.0151      0.205      0.074      0.941      -0.387       0.417
ma.L25         0.0078      0.182      0.043      0.966      -0.350       0.365
ma.L26        -0.1023      0.138     -0.740      0.459      -0.373       0.169
sigma2         0.0014      0.000      6.622      0.000       0.001       0.002
===================================================================================
Ljung-Box (L1) (Q):                   0.08   Jarque-Bera (JB):                30.28
Prob(Q):                              0.78   Prob(JB):                         0.00
Heteroskedasticity (H):               0.90   Skew:                             0.08
Prob(H) (two-sided):                  0.67   Kurtosis:                         4.94
===================================================================================

Warnings:
[1] Covariance matrix calculated using the outer product of gradients (complex-step).
#+end_example

#+NAME: ARIMA_Mkt-RF_train
#+begin_src python :results value file :exports results :var name=(org-element-property :name (org-element-context)) 
  fig1_path= img_dir / f'{name}.png'
  fig1 = px.line(df_arimatrain, y=['Mkt-RF','Mkt-RF_pred'])
  fig1.write_image(fig1_path)
  fig1_path #

#+end_src
#+CAPTION: d
#+ATTR_LATEX: :width 0.75\textwidth 
#+RESULTS: ARIMA_Mkt-RF_train
[[file:img/ARIMA_Mkt-RF_train.png]]

#+NAME: ARIMA_Mkt-RF_test
#+begin_src python :results value file :exports results :var name=(org-element-property :name  (org-element-context)) 
  fig1_path= img_dir / f'{name}.png'
  fig1 = px.line(df_arimatest, y=['Mkt-RF','Mkt-RF_pred'])
  fig1.write_image(fig1_path)
  fig1_path #

#+end_src
#+CAPTION: d
#+ATTR_LATEX: :width 0.75\textwidth 
#+RESULTS: ARIMA_Mkt-RF_test
[[file:img/ARIMA_Mkt-RF_test.png]]

*** Backtesting of returns

#+begin_src python :results none 
  # prediction on train data
  fnames_prediction = [k for k in df_arimatrain.columns if "_pred" in k]
  asset_names_pred = [k + '_pred' for k in asset_names]
  returns_arimapred_train = factor_model(df_arimatrain[fnames_prediction].to_numpy())
  df_arimapred_train = pd.DataFrame(returns_arimapred_train,
                                    columns=asset_names_pred,
                                    index=df_assets_train.index[:-1])
  df_arimaasset_train = df_arimapred_train.join(df_assets_train)
  # prediction on test data
  returns_arimapred_test = factor_model(df_arimatest[fnames_prediction].to_numpy())
  df_arimapred_test = pd.DataFrame(returns_arimapred_test,
                                   columns=asset_names_pred,
                                   index=df_assets_test.index[:-1])
  df_arimaasset_test = df_arimapred_test.join(df_assets_test)
  # # prediction on training data
  df_arimatest_profits = ml4qf.utils.profit_portfolio(
     df_arimaasset_test,
     {k: 1. for k in df_arimaasset_test.columns})
#+end_src


#+NAME: ARIMA_returnsbacktest
#+begin_src python :results value file :exports results :var name=(org-element-property :name  (org-element-context)) 
  fig1_path= img_dir / f'{name}.png'
  #fig1 = px.line(df_arimaasset_test, y=['JPM_pred', 'JPM','EQT','EQT_pred'])
  fig1 = px.line(df_arimaasset_test, y=['JPM', 'JPM_pred'])
  fig1.write_image(fig1_path)
  fig1_path #

#+end_src
#+CAPTION: daa
#+ATTR_LATEX: :width 0.75\textwidth 
#+RESULTS: ARIMA_returnsbacktest
[[file:img/ARIMA_returnsbacktest.png]]


#+NAME: ARIMA_backtest
#+begin_src python :results value file :exports results :var name=(org-element-property :name  (org-element-context)) 
  fig1_path= img_dir / f'{name}.png'
  fig1 = px.line(df_arimatest_profits, y=['ATO', 'ATO_pred', 'KR','KR_pred'])
  fig1.write_image(fig1_path)
  fig1_path #

#+end_src
#+CAPTION: daa
#+ATTR_LATEX: :width 0.75\textwidth 
#+RESULTS: ARIMA_backtest
[[file:img/ARIMA_backtest.png]]

*** Views from model prediction
Given the previous results on the testing data set, the following views are proposed: 
- EQT will rise 15%
- PH to outperform JPM by 11%
- KR to outperform ATO by 8%
The matrix and vector views are then:
$$
P = f
$$


** COMMENT Black-Litterman based portfolio
:PROPERTIES:
:header-args: :session py1 :tangle no :exports none
:END:
*** Covariance treatment
*** Prior and posterior returns construction

#+NAME: Calculate Covariance of excess returns
#+begin_src python :results none 
  df_Sigma_factors = df_factors[factor_names].cov()
  df_Sigma_factors_train = df_factors_train[factor_names].cov()
  df_Sigma_factors_test = df_factors_test[factor_names].cov()
  Sigma_factors = df_Sigma_factors.to_numpy()
  Sigmainv_factors = np.linalg.inv(Sigma_factors)
  Sigma_factors_train = df_Sigma_factors_train.to_numpy()
  Sigmainv_factors_train = np.linalg.inv(Sigma_factors_train)
  Sigma_factors_test = df_Sigma_factors_test.to_numpy()
  Sigmainv_factors_test = np.linalg.inv(Sigma_factors_test)
  #####
  df_Sigma_assets = df_assets.cov()
  df_Sigma_assets_train = df_assets_train.cov()
  df_Sigma_assets_test = df_assets_test.cov()
  Sigma_assets = df_Sigma_assets.to_numpy()
  Sigmainv_assets = np.linalg.inv(Sigma_assets)
  Sigma_assets_train = df_Sigma_assets_train.to_numpy()
  Sigmainv_assets_train = np.linalg.inv(Sigma_assets_train)
  Sigma_assets_test = df_Sigma_assets_test.to_numpy()
  Sigmainv_assets_test = np.linalg.inv(Sigma_assets_test)
  #####
  Sigma_4mfactors = beta.T @ Sigma_factors @ beta
  Sigmainv_4mfactors = np.linalg.inv(Sigma_4mfactors)
  Sigma_4mfactors_train = beta.T @ Sigma_factors_train @ beta
  Sigmainv_4mfactors_train = np.linalg.inv(Sigma_4mfactors_train)
  Sigma_4mfactors_test = beta.T @ Sigma_factors_test @ beta
  Sigmainv_4mfactors_test = np.linalg.inv(Sigma_4mfactors_test)

#+end_src

#+NAME: Compute equilibrium returns 
#+begin_src python :results none 
  f_mu = lambda l, S, w: l * S @ w
  mu_mkt_assets = f_mu(config.lambda_mkt, Sigma_assets_train, w_mkt)
  mu_mkt_4mfactors = f_mu(config.lambda_mkt, Sigma_4mfactors_train, w_mkt)
  w1_assets_theoretical = optimization.mean_variance_opt(mu_mkt_assets, Sigmainv_assets_train, config.lambda_portfolio[0])
  w2_assets_theoretical = optimization.mean_variance_opt(mu_mkt_assets, Sigmainv_assets_train, config.lambda_portfolio[1])
  w3_assets_theoretical = optimization.mean_variance_opt(mu_mkt_assets, Sigmainv_assets_train, config.lambda_portfolio[2])
  w1_4mfactors_theoretical = optimization.mean_variance_opt(mu_mkt_4mfactors, Sigmainv_4mfactors_train, config.lambda_portfolio[0])
  w2_4mfactors_theoretical = optimization.mean_variance_opt(mu_mkt_4mfactors, Sigmainv_4mfactors_train, config.lambda_portfolio[1])
  w3_4mfactors_theoretical = optimization.mean_variance_opt(mu_mkt_4mfactors, Sigmainv_4mfactors_train, config.lambda_portfolio[2])
#+end_src

#+NAME: Black-Litterman initialisation
#+begin_src python :results none 
  bl_model_Sassets = bl.BlackLitterman(Sigma_assets_train, w_mkt, config.lambda_mkt)
  bl_model_Sassets.set_portfolio_inputs(config.tau, config.P, config.Q)
  bl_model_Sfactors = bl.BlackLitterman(Sigma_4mfactors_train, w_mkt, config.lambda_mkt)
  bl_model_Sfactors.set_portfolio_inputs(config.tau, config.P, config.Q)

  bl_model_Sfactors.mu_mkt
  bl_model_Sassets.mu_mkt
#+end_src

*** Portfolio weights optimisation

#+NAME: Calculate Covariance of factors
#+begin_src python :results none 

  lmb_p = config.lambda_portfolio[2]
  x0 = 1. / num_assets * np.ones(num_assets)
  args = (bl_model_Sassets.mu_bl,            
          bl_model_Sassets.Sigma,          
          lmb_p)
  res = optimization.scipy_minimize("mean_variance",
                                    x0,
                                    method_name='SLSQP',
                                    args=args,
                                    options=dict(maxiter=200,
                                                 ftol=1e-12))
  w1_assets_opt = res.x
#+end_src

#+NAME: Compute porfolio weights frontier
#+begin_src python :results none 

  lmb_p = config.lambda_portfolio[2]

  x0 = 1. / num_assets * np.ones(num_assets)
  args = (mu_mkt_assets*12,
          Sigma_assets,
          0.1)
  args = (bl_model_Sassets.mu_mkt*12,            
          bl_model_Sassets.Sigma,          
          0.06)

  cons_sett = dict(eq_rets=dict(type="eq"),
                   eq_weights1=dict(type="eq"),
                   ieq_weights0=dict(type="ineq")
                   )
  res1 = optimization.scipy_minimize("variance",
                                     x0,
                                     method_name='SLSQP',
                                     args=args,
                                     cons_sett=cons_sett,
                                     options=dict(maxiter=200,
                                                  ftol=1e-12))

  print(np.dot(res1.x, Sigma_assets @ res1.x)**0.5 * 12**0.5 * 100)
  print(res1.fun**0.5 * 12**0.5 * 100)
  print(sum(res1.x))
  print(res1.x)
#+end_src

#+NAME: Function to build portfolios weights
#+begin_src python :results none 

  def build_portfolio_weights(mu_targetlist,
                              x0,
                              mu_portfolio,
                              Sigma_portfolio,
                              cons_sett,
                              annualise=12):

      res_list = list()
      for mu_i in mu_targetlist:
          args = (mu_portfolio * annualise, # annualised
                  Sigma_portfolio,
                  mu_i)
          res = optimization.scipy_minimize("variance",
                                             x0,
                                             method_name='SLSQP',
                                             args=args,
                                             cons_sett=cons_sett,
                                             options=dict(maxiter=200,
                                                          ftol=1e-12))
          res_list.append(res)
      return res_list

#+end_src

#+NAME: Function to compute weights vs volatility for target returns
#+begin_src python :results none 

  def build_df_weightsvol(assets,
                          mu_targetlist,
                          x0,
                          lmb_p,
                          mu_portfolio,
                          Sigma_portfolio,
                          annualise=12):

      # constraints: returns equal to a number given in mu_targetlist,
      # weights equal to 1, and all weights greater than 0
      cons_sett = dict(eq_rets=dict(type="eq"),
                       eq_weights1=dict(type="eq"),
                       ieq_weights0=dict(type="ineq")
                       )

      res_list = build_portfolio_weights(mu_targetlist,
                                         x0,
                                         mu_portfolio,
                                         Sigma_portfolio,
                                         cons_sett,
                                         annualise
                                         )


      weights = np.array([ri.x for ri in res_list])
      Weights = weights.flatten()
      # anualise vols
      vols=[((ri.fun) * annualise)**0.5 for ri in res_list]
      Vols = [vi for vi in vols for i in range(len(assets))]
      Assets = [k for i in range(len(vols)) for k in assets]
      df_weights_vols = pd.DataFrame(dict(weights=Weights,
                                          vols=Vols,
                                          assets=Assets
                                          ))
      return df_weights_vols
#+end_src

#+NAME: df with portfolios weights
#+begin_src python :results none 

  lmb_p = config.lambda_portfolio[2]
  x0 = 1. / num_assets * np.ones(num_assets)
  mu_targetlist = np.linspace(4,18,16) * 1e-2
  df_weights_vols = build_df_weightsvol(asset_names, mu_targetlist, x0, lmb_p,
                                        mu_mkt_assets,
                                        Sigma_assets)

#+end_src

#+NAME: Weights_Composition
#+begin_src python :results value file  :exports results :var name=(org-element-property :name  (org-element-context)) 
  fig1_path= img_dir / f'{name}.png'
  fig1 = px.area(df_weights_vols, x="vols", y="weights", color="assets",
                #pattern_shape_sequence=[".", "x", "+"],              
                pattern_shape="assets"
                )
  fig1.write_image(fig1_path)
  fig1_path
#+end_src

#+RESULTS: Weights_Composition
[[file:img/Weights_Composition.png]]


#+begin_src python :results none 
  weights_sol = {k: res1.x[i] for i, k in enumerate(asset_names)}
  df_profits_sol = ml4qf.utils.profit_portfolio(df_assets_test, weights_sol)
  weights_naive = {k: x0[i] for i, k in enumerate(asset_names)}  
  df_profits_naive = ml4qf.utils.profit_portfolio(df_assets_test, weights_naive)

  df_profits = pd.DataFrame(np.array([df_profits_sol.sum(axis=1).to_numpy(),
                             df_profits_naive.sum(axis=1).to_numpy()]).T,
                             columns=['Opt', 'Naive'], index=df_profits_sol.index)
#+end_src

#+NAME: P&L_plot
#+begin_src python :results value file  :exports results :var name=(org-element-property :name  (org-element-context)) 
  fig1_path= img_dir / f'{name}.png'
  fig1 = px.line(df_profits, y=['Opt', 'Naive'], markers=True)
  fig1.write_image(fig1_path)
  fig1_path
#+end_src

#+RESULTS: P&L_plot
[[file:img/P&L_plot.png]]

*** Analysis and discussion

- active risk (Table 6)
*** Performance comparison

#+LaTeX: \appendix
* ARIMA results summary
<<sec:appx_arima>>

Summary of factors OLS:
#+ATTR_LATEX: :width 0.7\textwidth
#+CAPTION: Summary of factors OLS
#+RESULTS: Summary of factors OLS
#+begin_example
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.229
Model:                            OLS   Adj. R-squared:                  0.207
Method:                 Least Squares   F-statistic:                     10.51
Date:                Fri, 18 Aug 2023   Prob (F-statistic):           3.32e-10
Time:                        19:20:18   Log-Likelihood:                 300.00
No. Observations:                 219   AIC:                            -586.0
Df Residuals:                     212   BIC:                            -562.3
Df Model:                           6                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const         -0.0015      0.005     -0.342      0.733      -0.010       0.007
x1             0.9139      0.126      7.242      0.000       0.665       1.163
x2            -0.3535      0.168     -2.101      0.037      -0.685      -0.022
x3            -0.2171      0.197     -1.105      0.271      -0.604       0.170
x4             0.4687      0.221      2.123      0.035       0.034       0.904
x5             1.0314      0.283      3.644      0.000       0.473       1.589
x6             0.2004      0.090      2.228      0.027       0.023       0.378
==============================================================================
Omnibus:                        1.427   Durbin-Watson:                   2.004
Prob(Omnibus):                  0.490   Jarque-Bera (JB):                1.124
Skew:                          -0.055   Prob(JB):                        0.570
Kurtosis:                       3.334   Cond. No.                         75.7
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
#+end_example

#+LaTeX: \appendix
* Code execution
The codes herein have been tested in Linux (Ubuntu 22 and Centos 8) and in MacOs. To install and execute follow the next steps.

** Installing the code
For the installation it is recommended to use a Python environment manager such as Conda and with Python >=3.10. Codes reside in the folder ML4qf and it is install like a normal package: navigate to the ML4qf directory in a terminal and run 'pip install .' The package should now be installed and a good check is to run the tests as follows. 
** Testing
A range of tests have been implemented using the library pytest to validate the codes in this work.
They are located in the folder ./test and can be run by navigating to this folder and running 'pytest' in the terminal.  
** Literate programming
Both the pdf from code have been simultaneously generated from an Emacs .org file. This type of file bears resemblance with Python notebooks but it is more powerful, albeit being also much older.
tangle
export
** 
